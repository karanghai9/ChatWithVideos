{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karanghai/Library/Python/3.9/lib/python/site-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": \"cpu\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6020"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('segments_updated.json', 'r') as file:\n",
    "    filtered_segments = json.load(file)\n",
    "len(filtered_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OK, good morning, everybody. So last week, we stopped at the question, how do we calculate the gradients for more complex loss functions and classifiers? And we saw that the thing that we need is the backpropagation algorithm. So we want to make use of the chain rule of calculus to determine the gradient of some complex compute graph. So we take one compute graph where we know the gradient for each of the nodes, and we use the chain rule to multiply those individual gradients, and thereby getting the gradient for the more complex function. And if, for example, we use a the compute graph for logistic regression, where we say, OK, we have two inputs, so two parameters for each of the input dimensions, and one bias term, our compute graph is this dot product over here, adding the bias term, getting this linear unit over here, taking the sigma. OK. And then we can use the sigma function of all of this and doing, performing, calculating our loss function. And for each of those steps, we know the gradients for each of those. For the loss function and the sigmoid, they are slightly more complicated, but we can just look up those gradients for each function and thereby get the gradient over there. And after we've looked it up, we can just add it into the compute graph, and we are happy. And if we say, OK, we put in our current values, so we're assuming this is the data point that we are looking at, and our current values for the three free parameters are those, we can do the forward calculations, so calculating the stock product, going forward, getting the result of the linear unit, calculating the sigmoid, and then getting the loss that we get. If we did a prediction that was with 37% probability, we think that the result is a positive example. So we would classify it as a 0. And actually, it is a positive example. So we somehow were off. So it means there should be some loss over here. And if we used cross entropy formula, we get a loss of close to 1. And if we do the backward calculation, we go through all of those parts over here and add in the values that we computed going forward into the formulas for the gradients. So here we need the y and the y hat that we calculated over here. We can plug this in, get this value for the gradient over here. We can plug in y hat. And we can plug in the y hat in the formula over here. So we have 0.378 times 1 minus 0.378. Get the value over here for the gradient here. And we can basically do that for all the other steps. It's kind of easy because we have linear parts over here. So the gradients are always 1. And for the last step over here, with multiplication, it's, it's always the value on the other side of the multiplication. So we have a minus 2 over here and 3 over here. And now using the chain rule, we can calculate, now that we know the individual gradients, we can just multiply up everything on the path down to each of those parameters that we are interested in. So we have three parameters. So we have three different paths that we can take through this entire graph. And if we multiply everything up, we can get the value of the gradient. And then we can calculate the gradient. We get the gradient for each of our parameters. So, and nothing changes if we make our neural network bigger. So this logistic regression, which is a one-layer neural network, works like this. We get the gradients. In this case, we could also write down the formula, easily write down the formulas for each of the gradients, for each of those terms, because it's a small network. We could just write down the formulas. But as soon as it gets much, much bigger, the formulas get pretty unwieldy. And to help us there, using back propagation and the compute graph representation, it doesn't matter how large everything from the parameter up to the loss function is. We can just mechanically do the calculation. And then we can just write down the formulas. And then we can just write down the formulas. And then we can just write down the formulas. And then we can just write down the formulas. So we can do gradient calculations by going through the compute graph and get the resulting gradients. There are a few things we can do to make calculations easier. In this case, if we combine the sigmoid and the loss function into a single node, then the gradient gets a little simpler because a lot of things cancel out over there. So the gradient for the combined loss and gradient And sigmoid, for combining cross entropy loss and the sigmoid is just y hat minus y. So like this times this one over here will in the end just be y hat minus y. So that kind of makes calculations easier. Because in a lot of cases we use those in conjunction and the gradient of those in conjunction is just this simpler number. So when implementing all of this, the hardest part is basically getting the vectorization parts right. So in almost all cases we just deal with... So at some point we have to do the... gradients for activations functions or loss functions which are slightly more complicated. But in almost all cases the gradients are that simple. So the gradient of going towards... of addition, multiplication, it's always just taking one of the numbers. We are multiplying up a lot of those gradients in the end. But we get away with basically just addition and multiplication in all of those cases. We just need to make sure that we have the right number of gradients. So we can just multiply them. So we can just multiply them. We just need to make sure that we have the right number of gradients. So we can just multiply them. We just need to make sure that we have the right number of gradients. We just need to make sure that we have the right number of gradients. So we can just multiply and add the right entries everywhere. And if we... For example, we do the gradient for the dot product. And what is the... If we do the gradient in the direction of w, it means we basically have this formula w1 times x1 plus... w1 plus w2 times x2 plus and so forth. And if we take the gradient in the direction of w1, everything here drops and this one goes away too. And we are left with x1. So the gradient in the direction of w1 would be x1. Same goes for in the direction of w2. Everything else drops. We are left with x2 and so on. So the total gradient will just be a vector x1, x2, xn. So which is just the entire vector x. So it turns out that the gradient of the dot product is kind of exactly the way that the gradient works for... Like if I just have two scalars that I multiply with each other. And so if I have a times b and I want to take the gradient in the direction of e, that would just be b. So a gradient in the direction of a would just be b. So... And for a dot product, it works exactly the same way. And somehow this also kind of carries over if we do matrix vector multiplications later. So if I have like... If we later on multiply some matrix with some vector, then we get kind of a lot of entries in here. So and for each of those... For each of those entries, we just have like dot products over here. And for each of those, again, we want to make sure that kind of the entries are correct. So it's always kind of matching entries to where they belong. And we'll do more about this later when we get to like doing proper neural networks. But it's kind of... Always remember, it will... The results will... For all the calculations that you need to do will always be kind of something pretty simple. Like in this case, it's kind of getting the gradient in the direction of w. It will be... It's x. And the complicated part is always just getting the shape of those things right and knowing which entry sits in which position of the vector or the matrix that you are dealing with. So it's... It's kind of... That's what you have to keep in mind for all of those things. So that was backpropagation. And backpropagation is kind of the basic for everything that we do when training any kind of larger machine learning system. Yeah? There's no clear-cut rule for this. So as an activation... So... There is certain cases where you always use a sigmoid. So for example, if you have... So if... So if you think of a larger neural network where you have multiple layers going through those, in between those, and we'll talk about this more in a minute, you might use any kind of activation. And it's kind of an engineering fine-tuning thing to determine which one works best. The final output. Here you are more constrained because it depends on your use case. So if you, for example, want to predict one class, so it's either 0 or 1, then you almost always want to use a sigmoid function there because that one gives you something that you can interpret as a probability of, okay, how likely is it that I'm in this class over here? So basically, you want to use a sigmoid function. It's a number between 0 and 1, and this is kind of scaled in a way that it works as if it was a probability in some way. So if you want to predict a binary class, you will always use a sigmoid. If you have multiple classes from which you want to predict one, you will always use the softmax function. If you want to predict just any kind of number, so like the price of a house or whatever, you will always use... like the identity function. So don't do anything with your output. Just use the number that comes out of it. And yeah, that's basically it. So if for those... So it's basically just have those three cases for the outputs that you have. Like you want to have one number, have a binary output, or have like a lot of discrete outputs, so 1, 0, 1, 2, 3, 4, 5. So and for... In each of those cases, your activation function is kind of determined because there's like one activation function that fits to that output. In all the other cases, so for these intermediate layers, you can use any activation function you like. We'll talk a little bit more about other activations functions that are commonly used in practice. But which one works best, there is no proper way to know that beforehand. So there's like a little bit of... experience for certain use cases that you know in certain use cases it might be this way. And the general advice is just use the ReLU activation function in between because it usually works well and it's fast to compute. So that's kind of the general advice. But yeah. Otherwise, there's no way to know what works best. So if we... Logistic regression is a one-layer neural network. And here we use the sigmoid as the last activation function because we have a binary output. And the logistic regression does a direct mapping from our input to one output using one linear layer. So... The next... The next step is... We want to add... Another layer here. So I... And... Just do the same thing that we did in our... In the layer that we did before. So if we have... If we say this is logistic regression over here, it takes some inputs, does some linear operation, applies the sigmoid and gets like the properly scaled output which we interpret as... A probability. And... This layer over here, it basically doesn't really care where its inputs come from. So if those inputs were x1, x2, x3, we would call it a logistic regression. If those inputs were the outputs of another neural network layer, we would call it a neural network. So it's... But this layer over here doesn't... It doesn't know anything about what happens over here. It just gets three inputs and does a logistic regression to determine its output. If we... If we build a neural network, we just add additional steps over here and each of those does a small little logistic regression or some other linear transformation... Almost linear transformation to modify its inputs. And produce some more refined output that this last layer over here can use to make a better prediction over here. So... And... The terminology here is we talk about our input which are the original features that we get from in our data. We have the output layer which is the last layer that... Which output... Where we do observe the output and where we actually... have some proper interpretation for the output where we know what this output should be. And everything else we call a hidden layer. And we can have a lot of those. If we put all of this into formula then... We say... Okay, we have our inputs and we have like a little dot product over here for this first neuron over here where we say... Okay, we do the dot product of some vector w1, 1. And we have a bias term for this neuron over here. So this is... The result of the linear operation that we calculate in this neuron over here. And we call this output z1. And we apply some activation function over here. For example, the sigmoid. And get something that we call the activations. And we have a number of the first layer. So we get like one number as an output over here. And this number would be a1 superscript 1. And we do the same thing for each of those neurons. So we have like three neurons over here. So this one would be neuron 1, 1. This is neuron 1, 2. Neuron 1, 3. Neuron 2, 1. And because we will have like a lot of layers and different neurons and the different indices within them. So... Indices within the neurons. We have to fight a little with sub and superscripts to get the mathematical notation somehow consistent. And probably I'll also have still some errors in the slides at some points where I mix up the indices. So I'll use this superscript in square brackets to determine the layer. So this is like layer 1. This is layer 1. This is layer 1. This one is in layer 2. And I use the subscript over here to determine the individual neuron within the layer. So I have like this is the output of the first neuron in the first layer, the second neuron in the first layer, the third neuron in the first layer. And the output layer has just one neuron. So I could make a subscript over here. But I'll just leave it out because I just have a single output over here. And we'll have to add more indices later on when dealing with more dimensions and making things more complicated at some point. But for now, remember the square brackets indicates the layer. And the subscript indicates kind of the neuron which we are using. So if we think about this hidden layer over here, what we are doing is we have like this. Like a dot product over here, a dot product over here, a dot product over here. So we do three dot products producing three activations or producing three so-called logit values. So this one would be z1, z2, z3 each of the first layer. And those get mapped using the sigmoid or some activation function into the activations. And those are the input for the next layer. And what we want to do is batch together all those operations to vectorize more of the things. So what we can do is we can batch together all those individual dot products over here into one matrix vector operation. So if we write those individual vectors W as rows of one large matrix W1, so which in this case will be a three by three matrix. So we have like three different neurons in the layer. And each of those neurons has three different inputs. So this would be number of neurons. This is the number of inputs those neurons have. We can add it. All of this. We can add it. It gets turned into a matrix vector operation where this is kind of the matrix with those values. This is a vector with all the bias terms. And then as an output, we get a vector with all those z values. So this would be the vector with that one, one, z2, one, z3, one. So nice thing about writing it like this. It's very easy. We kind of can get rid of one of the subscript index because we only need to keep the number, the index that tells us for which layer this matrix is relevant because we kind of already grouped together all the neurons into one large matrix with parameters. And the sigmoid function, again, in the way that NumPy does it, and we need to do it over here. It's applied for each of the elements over here. So it's element wise applied to each of the entries in the z vector over here. So if we put all this together, so if we put all the calculations from start to finish into one formula, we get that our predicted value is the sigmoid of vector w2, dot product. And then we have the z vector over here. So we have a z vector over here. We have a product with the sigmoid of matrix product of w1 times x plus bias term 1. And, again, this whole thing will be added with bias term b. So there is always still some errors with the indices. So it's b2 over here. And what is inside here, we call the logit, of the first layer, Z1. After mapping it through the sigmoid, we call it the activations of the first layer. Then the next linear operation gives us the logit of the second layer. And the sigmoid over here gives us the activations of the second layer. And because that was the last layer, the activations of the last layer is also the final output. So if we put all this, if we try to write down all this as a compute graph, basically we get this one over here. We have this matrix product over here. Adding a bias term, mapping it through a sigmoid, multiplying it with a matrix, adding another bias term, and doing another sigmoid over here. And of course, if we would add the loss function over here, we also would get a node for the loss over here. Which also gets a Y as an input over here. So in some way, if we think... What we often think about is having this idea of a single neuron, where we have one value as an output, which is kind of what we do with logistic regression. But when building neural networks, we basically always think in entire layers. We don't really care about the individual neurons of one layer, because we always batch them together. We usually think in terms of an entire layer for a neural network. And the layer is... By using this matrix vector operation, we kind of batch together all those individual operations for each of the neurons. So we're having like one matrix operation over here. And getting the activations for each of those neurons in here. And we usually never really think in terms of individual neurons, because we kind of always combine operations as much as possible. So a tricky part, I told you this again and again, is getting dimensions right. So... Let's think about a little bit about the dimensions that we are dealing with here. So we have the size of the input vector. So and we call this n superscript 0. So this would be the dimension of the input vector. After the first layer, we are dealing with activations of size n superscript 1. So this would be the size of the activations after the first layer. And if we have those sizes over here, that means our matrix here in the first layer, so w1, would be a matrix n1 times n0. Because that's the size of the input, and that's the size of the output of this operation here. If this is the size of the output of this vector matrix multiplication, we know we need bias terms for each of the outputs. So the bias vector will have dimension n1. And because that's the size of the output, the output will have size n1 again. Somehow, again, to kind of illustrate how the matrix vector multiplication works, so if we have like a matrix A and a matrix B and multiply them together, we multiply this element and this one, add this one and this one, this one and this one, we add all those together to get the result over here. So and we keep doing that for each of the outputs. And the thing to remember is, if A is an L by M matrix, and B is an M by N matrix, then the M's in between have to be the same and will cancel out and give us a result that is an L by N matrix. And if we have like, if B turns out to be a vector, then the N will be a 1, so the result will also be a vector again with dimension this way. And that's kind of the image that you need to keep in mind to remember what you're doing. And what your inputs and outputs have to be, and so you can kind of remember this and get back to, and if you have like errors and things do not match and NumPy throws weird errors for you. We can use broadcasting to do these calculations over here, not just for a single example, but we want to probably do all those operations for a lot of examples. So we don't want to just calculate the activations for one input example, but we want to do that for a lot of examples. And so I'll just give you kind of a brief example over here with NumPy. If we, so if we have like a single example over here, a single input here, then our z values, our z values will again be a vector of entries, and the same would be if I just remove this one, then, and things do not work because I do a matrix multiplication over here. So if I, in the basic formulation, we would do something like, and let's call this a small x equals and p random point rand, and make this length four. So let's leave this one out, and small z would be w times x plus b. And so, where do I go wrong over here? So this one, so and this, and this one would also be just a regular vector, and I get the result that I expect over here. So I get like, I have like x is some vector, w is a matrix, b is another vector that has like the size of the output dimension, and if I do the matrix multiplication w and x, I get something that has length, three out of the input that had length four. I add the bias term with length three and get my, my logits, and later on, after that, I would push those through a sigmoid function to get the activations. Now if I want to vectorize stuff, I'll need to make sure that I put in a lot of x, and I'll need to put in a lot of x. So I'll just assume that I have like five different examples, so each example has size four, and I use five of those examples. So I don't deal with a vector x anymore, I have a matrix x where each of those columns is one of the input, of the input data I deal with. So and everything else should kind of stay the same, and if I do this, things do not work out as nicely, work out anymore, because that turns out now to be again a matrix with one output, so it's three by five, so I get, again I have three outputs, but I have one of those vectors for each of my data points. So I get, I had five original data points as input, and now I still have five data points as output, and I have the results of this linear operation for each of my data points. And to make sure that I now add the bias term individually to each of those data points, I must make sure that I tell numpy which is the dimension over which to broadcast, and the so-called batch dimension, which is where I put in the, over which I index my different examples, which in this case is the last dimension over here, and it kind of often is the last dimension. I'll have to make sure that numpy knows that it has to broadcast over this operation, over this batch dimension. So I'll put, when I define my bias term over here, I don't define an, a vector of length three, but I make it a matrix of length, of size three by one, so that this operation here works out in the way that I expect it to do. And that's kind of the motivation over here to, why each of those is kind of written as a two-dimensional, or two, a numpy object with two shapes, so that the broadcasting works over this batch dimension when I'm using, I'm calling, doing this addition over here. So, but the nice thing is everything works even if I'm putting in a lot of input examples, and this way I can kind of batch together a lot of operations into a, into like single numpy operations, and make them being computed pretty, pretty efficiently. So, these things over here are called the activations, and it's the output of like mapping each individual entry through the activation function. And there's probably one question, why do we actually need the activation function anyway? So what is, what is the activation function good for? So, and the easiest way to find out is by trying to just leave it out. So what happens if we leave out the activation function? So if we have our formula for our two layer neural network, so we have like two layers, so and two operations, W2 and times W1 times X plus B1 plus B2. And in the original version, we would have had, a sigmoid around here and a sigmoid around, in front here. But now if we, or any, any kind of activation function, but if we leave it out, we can do some arithmetic to change this formula over here. So we could just say, okay, this thing is the same as W2 times this plus W2 times this one. So, got this one wrong. So there should be a W2 in front of the B1 over here. So W2 times B1. And, what we then can do is group those together. We can say, okay, I'll just multiply the parameters that I had in here with the parameters I had here. Get a new vector. In this case, this will be, this matrix W prime will be a vector. Because I have a single output over here. So get, we get like a vector with entries over here. And if I multiply this matrix with this one and at this vector over here, I get another vector B prime over here. And, all together, this entire formula collapses to this simple linear operation over here. So, having, and this kind of means having all the operation, all those parameters, all those parameters over here. So I had, I have like a big matrix of parameters over here. And to make lots and lots of calculations. But I could get the same result, the very same prediction over here, using just a simple dot product down here. So the result that I calculate over here, is the very same, as if I just used much, much fewer parameters and just did a dot, dot product down here. And that is the reason why, we need the activation function to make sure that those parameters cannot just, cannot just be grouped together and do the very same thing. We actually want, just throwing in a lot of parameters, doesn't help us if we cannot calculate something, that we could just have calculated with a simple linear operation. Because that means, this more complicated formula, has the same predictive power, as the simple formula down here. It calculates the very same thing. So we could, and if we had up here, probably W2 had, if W2, was some vector of, like 10 parameters, and W1, was a matrix that had, that had 10 outputs and 3 inputs. So we had like 30 parameters in here, and 10 parameters in here. So in total we have like 40 parameters, just in the W's alone. And down here, W prime would just be a vector with like 3 entries, with like, with just 3 entries, so the inputs over here. And would it, and everything that can be calculated, with those 40 parameters over here, can also be calculated, just with 3 parameters here, over here. I just have to choose those parameters more carefully, and different. But there's nothing that those, 40 parameters over here can, can calculate that, those 3 parameters down here cannot calculate. And that means, whatever I want to predict, I could do that with this linear function, so this thing also just can, calculates a linear function. And that, that's kind of, kind of what, what we, where we need to make sure that this doesn't happen. So, we need this activation function, so that those, that, that we actually get something that is more powerful, than just a linear operation. And we, but we, we are actually pretty free in, what we want to choose as an activation function. So, so far we have usually chosen the sigmoid, function over here, so it maps everything between 0 and 1. But we can choose other activation functions. So, this is kind of the natural, natural choice for the output layer of a binary classification, as I already told, said. So, there, if for binary outputs, that's always the last, the choice for the last layer. But, as the intermediate, for the intermediate layers, we might want to choose different activations functions. So, we also can kind of make an index for the activation functions, because we want, probably want to have different activations in different layers. And, another activation function that is pretty popular is the hyperbolic tang, tangent. And, that is something like a scaled version of the sigmoid. So, it's, the, the difference being that it maps not from 0 to 1, but from minus 1 to 0, to 1. And, there, it, this has, a nice property for, for, for the, for the hidden layers. Because, the mean, of what this thing maps to is closer to 0. And, that is usually some, that, that, that is usually some property that is, that is pretty nice, because, that we, it, it, it, it usually, usually the most interesting parts happen, when switching from positive to negative. So, switching from positive to negative numbers, because that is kind of the difference between, I, I want to, it's a positive example, it's a good example or a bad example. It's a 0 or a 1. And, the most interesting parts usually happen over, over here. And, having like this property that it's, that, that, the, it's, it's getting scaled close, it, it's to a mean that is closer to 0, 0 usually works a little bit better, than the one where the mean is scaled to 0.5. So, or, or, where the interesting parts happen at 0.5. So, it's, that, that's, that's, so, this Tangent Superbolicus work, usually works a little bit better for the intermediate layers, than the Sigmoid. But, it also has some disadvantages. And, one thing is, so, if we look at the gradient, for example, over here, the slope of this function, is not very large over here. So, it's, it's, it's, it's, it's, it's very large over here. So, the gradient here is pretty close to 0. And, the further, the larger we get over here, or the smaller we get over here, the smaller the gradient gets. The, there's only a small area, where we actually have a pretty steep gradient, and things work almost linearly over here, and then the gradient starts to, to, to go down when we get larger values. And, that is, that can be quite a problem, because, if you have very, very small gradients, then gradient descent doesn't make a lot of progress, because, if you, if you remember, we always subtract learning rate, times the gradient from the values, and, which means, if the gradient is incredibly small, we will make a step in the right direction, but it will be, also be incredibly small, and that means our algorithm might run for a long, long time. What is the vanishing gradient problem? That, that, so, it's, not, when talking about vanishing gradients, we'll, we'll also talk about those, it's usually a different, problem, that's, the vanishing gradients usually happen due to, if you remember, in back propagation, we multiply everything up. If you multiply a lot of numbers that are smaller than zero, one, you get also something that is close to zero, so that usually vanishing gradients happen due to, I multiply up a lot of numbers that are smaller than one, so that, this way they vanish. In this case, they all, also vanish due to the, the, the, the, a single step in the, in the neural network, so, the vanishing gradient problem is usually something else that we, that, if you talk about this, but it's also, it's, in general, it's also one of the possible reasons why your gradients might get close to zero, and, so, it's kind of, there's a lot of reasons why zero, gradients could get close to zero, and, when using this term vanishing gradients, one usually talks about a different, issue, reason for that problem, but, there's, the problem that they might go close to zero, can come from a lot of sources, yeah. So, and, but, and, and, yeah, okay, having, having, having gradients that get, go close to zero, is, is kind of one of the main reasons why training doesn't work well, so, that, our algorithm will, at some point, get into the proper region over here, if we just train it long enough, so it will, it will work at some point, but, we might, it might take ages, and, we don't, might not have this time, and, solving this, for example, the ReLU, activation function, which is kind of the maximum of, some value and zero, so it's, this value, I should not mix those things up over here, so, if the, this input value Z, is greater than zero, I'll just take the value Z, and otherwise I take zero, this is kind of, if, if you think about it, it's this, the, it's, it's in some way the stupidest, activation function that you can use, which fulfills, that it's not a linear function, so it has to be something that is not a linear function, for, to, to, to make sure that we don't, our, our, our weights do not, cannot be, so we, basically just say, okay, we just, take one point, which is not linear, everywhere else, our function is a linear function, it's a linear function over here, it's a linear function over here, it just isn't linear at this, one single point, and, that you, in a lot of cases, that already does the trick, so, by, by, by, by, by, by, by, that the entire neural network will learn something useful, and, because it can now make a distinction over here, between very, values that are larger than zero, smaller than, than zero, and for those that are smaller, than zero, it kind of, can, can make sure, that it doesn't matter how small it is, it always gets mapped to the same value, and that can, can, can, it then, look, it, it basically can use that, and the classifier is supposed to distinguish between positive and negative examples. And a linear classifier can only predict some straight line over here. But using this small non-linearity in the layers, our neural network basically can make something that works like this so that it can also use this non-linearity to split up the decision boundary into smaller linear parts. So if you use the regular activation function, then you give your classifier the ability to add those kind of kinks into the decision boundary, which is what we need to get better predictions in some cases. So how about the gradient of this thing? So the gradient of this is 1. If we have values that are greater than 0, and it's 0 over here. And no matter how large the value is that we have over here, the gradient will always be 1. So it's not going to 0 as long as if we have an incredibly large value over here, we always have a proper gradient to go down here. Only thing is, if we are negative, the gradient gets 0, and the algorithm has to be 0. So there's no way to figure out that it has to travel into some direction over here. And there is a fix for this, and this is the so-called Leaky ReLU, which is after 0, I'm not mapping exactly to 0, but to some small value over here. So I'll basically take the maximum of x and 0.01x or something like this. So if x is greater than 0, I'll just take x. And if it's small, then I'll just map it to a very small, very small value. So I'll just take a variant of this. This means that the gradient here will be 1, and here it would be 0, 0, 1. So I still have a gradient that helps me traveling out of this region over here and getting over the nonlinear part over here. This often works slightly better than the basic ReLU function. It's possible to choose another value over here. So this is something that you can freely choose. In this picture, I chose 0.1 over here, because otherwise, if it's 0.01, you don't even see the difference between the ReLU function in the picture, just so you know that I made this more extreme so you can actually see something here. In practice, Leaky ReLU is not used that often, which is mainly because it just works slightly better than ReLU in most cases. But it's kind of interesting. It's interesting that the fact that the gradient over here goes to 0 doesn't seem to matter that much in practice, so that fixing this problem is not that relevant. Another activation function that we can use is the identity function. And obviously, this is a bad choice for all hidden layers because of all the things that we already talked about. So there's no reason to use this in any hidden layer. But for the final layer, we might want to use that. So as an activation function, we might want to use that. Because if we have some kind of regression problem where we want to predict any kind of number, we want to make sure that we can predict any kind of number. So it might be a good choice or the proper choice for the last layer of a neural network. So those are the most common choices for an activation function. You can think of others and basically any kind of nonlinear function where you have a good derivative and can calculate it easily can work as an activation function. I have a question for the value of functions earlier. I don't really understand why this is... I mean, if we take the 0.01, can we just take any number? So it's like the ReLU function, but it still has a gradient that kind of makes sure that if you're over here, you have a way to know that the gradient descent has a way to know that if it wants to predict something larger, it should have traveled this way. So you want to add something that is small so that it's close to 0, but big enough that you still make some progress into that direction if needed. So that's kind of... That's kind of... That's kind of... So it usually will be something like 0.01, but it could be any number. So you could even put a 10 over in here and then it's not the maximum or something like this, but you could also make something that works the other way around and just make it very weird. It probably won't work in practice, but it would give you the properties that you need. So basically, as I say over here, it's kind of... You can take almost any function as long as it has some non-linearity in there and you kind of have a derivative almost everywhere. So in this case, you kind of... This is an... The whole function is not differentiable because there is like this point where there is no derivative in mathematical terms, but as practical engineers, we don't care about this because we can say it's just one point and actually you will never be exactly at this point. So we just say, okay, the derivative at this point is... Let's say in this case, it could be 0.01 or it could be 1 and we just choose one of those if we are at exactly 0. So over here, it would be 0.01. Over here, it would be 1 and exactly at this point, we say it's 1. So we just choose one of those possible derivatives and that's for practical reasons, for practice that works sufficiently good. So even though it's not completely mathematically sound, but having like any function where we have like a derivative almost everywhere, that works in practice. So and you can choose any kind of activation function, function you want to, but not any activation function will work in practice. So it's kind of value turned out to be a function that is easy to compute and works pretty sufficiently well. So it's kind of the go-to function. You can, but you could use any function, but you will need to make experiments and see, okay, does it actually work better? And do I make... So if it's a very complicated function and I need more calculations and it's not going to work, then I can use the go-to function. And if it's a very complicated function, then I can use the go-to function. And if it's a very complicated function, and it takes longer to calculate it, then probably just having another layer with a ReLU function would do the same trick and I can put in another... Because I have more... I have less... It takes less calculation to calculate ReLU. I can have more time that I could use to add another layer in the neural network. And then probably ReLU with another layer is better than my fancy function with layer less. And then I could have also just used ReLU and done nothing. So it's... It's kind of... Finding something that actually works better than the simple ones is not that easy. So now we have basically introduced all the things that we need for one of our neural network neurons. And each of the neurons has basically two jobs that it has to do. One is I allow my inputs to interact. And that is part... That is what the linear part of my operation does. So I... I have like some... Allow that I mix up all those different inputs weighted by something in some way. But I have... This way I allow those inputs to interact in some way. And then I introduce some non-linearity to make sure that I just don't have linear operation all the way through the entire network. And... And basically these are also the... The kind of two minimal requirements that I need that a neural network with some depth makes sense. So multilayer neural networks make only sense if I have basically those two properties. And... The neurons that we use only kind of fulfill those minimal properties. So we could try to make more fancy neurons. But like the neurons that we use only fulfill the minimal amount of work that is needed to... Needed to... To... To... To... Be meaningful for having a multilayer neural network. So there is no talking about multilayer neural networks without talking about the XOR problem. So that's kind of the... The simplest problem where a single layer... Doesn't work... Single layer classifier doesn't work. So if I have two inputs, X1, X2, and they can be either 0 or 1, and I have the output that I want to have is also either... Either 0 or 1. And so if both are 0 or both are 1, I want to predict 0 and 1 otherwise. So which is kind of the logical XOR gate. If I draw this in this way, it's kind of easy to see that there is no linear decision boundary that I can put in there that would perfectly separate the Xs and the circles. So there is no way I could draw a linear decision boundary that would separate them. So... There is no linear classifier that can properly separate them. But if I have like two layers, I could do that. So one way would be to define my weights in this way that I have like the activations A1 would be ReLU of X1 minus X2. A2 would be minus X1 plus X2. And then I... For the output, I just add up those activations. And so if I go through the calculations, A1... would be... 0 minus 0 is 0. 0 minus 1 is minus 1. Gets mapped to 0. 1 minus 0 is 1. 1 minus 1 is again 0. And if I do the same thing for A2, I get 0 here. 0 and 0 is 0. And 0 and 1 is 1. And minus 1 plus 0 is minus 1. Gets mapped to 0. And minus 1 plus 1 is 0 again. So we get those outputs. And if I just add them up, we get kind of 1 over here and 1 over here. And this is kind of also shows you why this ReLU over here is actually important. Because otherwise, if I didn't have the ReLU over here, this 0 over here would be a minus 1. And this 0 over here would be a minus 1. And if I add them up over here, I get 0s over here. So the trick that this neural network does is by making sure that once I get smaller than 0, I just map everything to 0. And this way, I can get this kind of kink into the decision boundary. And that's kind of what... And it also shows that even the simple ReLU function is already... It is sufficient in providing enough nonlinearity that kind of this logic gate now works. So kind of if I plot those values A1 and A2 after the first layer, then after the first layer, I kind of get it mapped to those points over here. And I have like two circles over here which just are above each other. So these are the first activations. And now the second layer can just make a linear decision boundary between those. The mapped values A1 and A2. And if I don't have a ReLU function, then kind of those points would be all on the same line. And again, I cannot draw... Even after the first layer, I still cannot put in any linear decision boundary here. So it kind of visualizes and gives us the intuition why the activation function is actually important to make sure that we can make some nonlinear prediction over here. So if we have like these activation functions, we kind of need... For each activation function that we want to use, we need to know the derivatives of those to do the backpropagation calculations. So for the sigmoid, we already had that one. So if this is kind of the... If our activation function is the sigmoid, then the derivative of the sigmoid is the sigmoid times one minus the sigmoid. We already had that when talking about backpropagation. For the hyperbolic tangent, the derivative is... Again, something that is a little bit more complicated, but we can just look that up and see, okay, the sigmoid of the hyperbolic tangent is one minus the hyperbolic tangent squared. So after looking it up, we can just use the formula. We can use the formula for that. For rectified linear units, the gradient is incredibly simple. We talked already about this. So it's one for every number that is bigger than zero and zero otherwise. And this is kind of pretty neat that we have like... We don't even need to... In this case, we need to do a few calculations to get the derivative. And so the number of calculations we need to do when doing... Neural networks add up and can take quite a lot of time. And like... Having very, very simple calculations is pretty nice when it comes to... When you think about, okay, what we have to do and if we can save time, training works faster and everything works faster. For the Leaky ReLU, it's similarly simple. We just don't have a zero over here, but like the slope that we had on... That we have in the leaky part over here. So now we have like... We have the different building blocks for how to do the calculations for the relevant calculations for a neural network. Now if we want to do gradient descent with multiple layers, we need... What we have is a lot more parameters. And for each of the parameters, we need to calculate the gradient. So we have like... For each layer, we have a matrix with... With the weights for each of the inputs. And we have the bias terms and we have that for each of the layers. And like the dimensions here, we have the formula... We remember this is like the input for the first layer, output of the first layer, input for the second layer, which is the same as the output of the layer before and output of the second layer. And if we think about it, okay, when we do gradient descent, we want to optimize some cost function. And we usually call that the function J, which is the cost function over the entire data set or the entire training batch. And the cost function is dependent on all the parameters that our neural network has. So we have like a lot of different parameters that we can use to calculate the cost function. And we can also use the cost function to calculate the cost function. So we have a lot of different parameters. We have each of the matrices and each matrix has kind of potentially a lot of parameters. And our cost function then is the mean of the training examples that we looked at. And probably the... And if we do classification, this will be the cross entropy loss for each of those data points. And if we do gradient descent, the update step will be going into the direction of the gradient for each... Or into the opposite direction of the gradient for each of the parameters that we had scaled by the learning rate. And that's the same thing for each parameter. So it does... I wrote down a lot of stuff over here, but it says the same thing for each of the... In each of the cases over here. And having more layers and more parameters doesn't change anything here. So if we get more parameters, more layers in the neural network, the formulas will stay the same. We need to calculate the gradient into the direction of the respective parameters and do a little step into the opposite direction of those scaled by the learning rate. So you can guess how this would look if we have more layers. So if we do the backpropagation algorithm, we have two steps that we have to take. The forward propagation step, where we calculate the value that we predict. So that is calculating z1, which is w times x plus b. Then calculating the activations by using the first activation function on those values z over here. So this should be probably a smaller z over here. Then doing the linear operation on the output first activations, getting the first logits over here, applying the next activation function on those logits over here. Again, this would be small z over here. And getting the second activations, which turn out to be our predictions for this two-layer neural network. And the activation function over here turns out to be a sigmoid because for the output, we definitely want to use the sigmoid over here. One thing to note, and this is why I mixed up small and big letters over here, we probably don't want to do this for a single vector x over here, but for an entire matrix of x, which is of dimension n0 times m, where m is the number of examples that we are looking at. So this is kind of... And we probably want to do everything batched for the entire set of data, for all the data points that we have or that we are looking at at the moment. And then we also get like a matrix, a big z over here and a matrix big A over here because we get the outputs for each of the data points in the same operation. So if we... I try to draw this as a compute graph, I'm starting to combine more and more operations into the individual nodes of the compute graph so that it doesn't get too large. So in this case, I have like a linear layer and the linear layer has weight matrix as inputs, a bias term as inputs, and like the original input features as an input. And as an output, I get the values z, from the linear layer, then I map those through the activation functions, get my activations A and get another linear layer up till the final outputs y hat, which I then put into the loss function and get the final loss number which I want to optimize. So when doing backpropagation, when we do backpropagation, when doing the forward propagation, we kind of calculate through this compute graph to get the final loss. Doing backpropagation, we need to calculate the individual gradients. And so I'll write it down in kind of mathy terms over here. In the exercises, we will kind of try to program all of those things in NumPy. And it's even... which can still be a bit challenging, even though you know all the formulas for each part here. So if the gradient for the loss function, we already saw that we can kind of batch together the sigmoid and the last loss function, so we can kind of batch these two operations together when calculating the gradient because it's an easier gradient that we get at the end. So we get like the activations, the second activations minus... which is just y hat minus y as the gradient into the direction of z2. So we kind of already have the gradient of the loss into the direction of z2. So we get the gradient up to the point down here. So if we now want to take the gradient into the direction of the parameters w2 over here, we need to... multiply the gradient that we just had before times... so... and times the gradient from here till here. So that... and the gradient from here till here will just be the inputs that we put in here. So it will just be the a1 over here. And... so if we want to multiply... and we need to multiply that with the a1, we need to multiply that with the a2 over here. So we multiply that with the a2 over here. And we get the gradient that we got from over here. So the gradient into the direction of w2 will be the gradient that we got up here times the inputs from over here. So... and... again... all these are vector and matrix operations over here. So... and written down in the way that the proper derivative ends up in the proper position for this matrix over here. So... remember that if I do a matrix vector multiplication, so I multiply w and a... so... this thing over here is affected by... so... this value over... so this one over here times this value over here gives you the value over here. And... if I have like multiple inputs a, so if I have like another input here, then this value over here times this one over here results in this value over here. So... what we will need to do is... we already have the gradients for each of those values, which are the ones that are in here. So we have the gradients for each of the values over here. And each of those need to be multiplied with the corresponding value over here. And we will need to add them up to get the gradient for this value over here. And this is kind of what happens if we do this operation over here to make sure that the gradient for each of the... that the correct gradient ends up at the position in this matrix over here that we need for... that... which kind of... is each of the output values that are affected by this one and each of the input values which are multiplied up with this value over here. So if we... go one step... go to the other direction, if we go to the bias term over here, it's just a linear operation. So we multiply the gradient over here with one, because the gradient from here to here is just one, so it's just the gradient that we already calculated over here. Now we go down one step further. We calculate the gradient into the direction of the activations A1. So we take the gradient... so we... so we have had like the gradient this way, we had the gradient this way, now we need to calculate the gradient this way. And now it's the gradient that we have over here, but we need to multiply it with the weights from this way... from over here, because we are kind of on the other side of the product. So it's the... this matrix over here, multiplied with this gradient over here, and to make sure that everything ends up at the right position, we need to transpose the matrix, so that everything works out, and probably a good exercise for... if we want to... for the next exercise sheet, if you want to understand this more properly, it's kind of a good thing to write out this matrix multiplication in full, to see why we need to transpose it, so that the gradients end up at the right positions. Next, we... over here, we need to make the step down here for the gradient, so we need to multiply the gradient that we just had with the gradient of this... activation function that we have over here. And in this case, this thing here is the element-wise multiplication, because there's no proper mathematical symbol for doing this operation that is kind of the normal NumPy multiplication, but... I can do this between two vectors, where I just want to have another vector where I multiply this by this, and put the result over here, and if I want... and so on, and kind of this element-wise multiplication, I'll introduce this symbol over here to... to... for that one, which is kind of the standard NumPy multiplication if I multiply two vectors, because otherwise in math, that's usually... you usually always take the dot product between vectors, which is something different. So, we take the gradient of... that we had for each of our activations and multiply it with the gradient of the activation function at each of the... of the... the individual intra-entries, and this is kind of... over here and over here, it's kind of hard to get those... those transpositions and matrix operations right. Over here, it's actually pretty simple, because this... usually we can have just the formula over here, and we just do element-wise stuff, so it's kind of pretty easy to go down this... this way, so we end up over here. Next step is, we kind of need to go down from here to... the parameters W1, and this will be the same operation that we did over here already, so we take the gradient into... that we have accumulated over here and multiply it with the parameters W1, the parameters that we have over here, so that's kind of the same thing that we did over here, and if we have more layers, it will stay the same, even if we do more and more calculations, so going down even further will just be the same operations over and over again, the same thing for... if we go over... into this direction, we again multiply this one by 1, so it's just the gradient that we already calculated going down over here. So, now we have like the gradients for... each of the different parameters over here, we don't need to go further down this way, because we don't need the gradient for the input parameters, because we cannot change them. Again, in the same way that we batched everything... and vectorized everything when we went up this chain, we also want to make sure that everything going down the chain is vectorized properly, and here are some examples for this, so if we... when going down... doing the gradient computations, we also want to make everything batched over all the training data that we are looking at, and vectorize everything, and... in some way, everything... everything works exactly the same way that we write down things over here, we just need to make sure that at some points we need to average over the entire training batch, so, which is because... if we basically take 1 over m times the individual loss functions, so if we... we kind of have this 1 divided by m, which needs to be part of the... of certain of the calculations, and... we kind of need to remember to put that in, so it's... it's one thing that needs to be done to make sure that everything still works in the batched way, so this way, kind of, we can get... but otherwise, all the calculations over here are kind of just numpy... the translations of those things into numpy. So, having all this, and you'll be in the... in the exercise sheet for next week, you'll be able to kind of put all this together to also... to train your own numpy-based neural network and make... put all this together so that we can kind of make a proper classifier with this, and we'll use the same data as for the logistic regression example, so we can see that, okay, adding more layers actually gives us some advantage and makes the results actually better. So... when we train logistic regression, we can just start by setting this entire... this vector w and this vector b to zero, this value b to zero. So, can... a question is if we can... can we do this now that we are dealing with a neural network? And... the... short answer is no, but let's see why. And... if... one thing... one thing is if we have multiple layers, setting everything to zero will mean that all the gradients turn out to be zero at the end, because you start multiplying zeros in at some point, and then... for logistic regression, this doesn't happen, because you kind of... if I take the gradient into the direction of w, I'll take the gradient x, which is not zero, so everything is fine, but if I have activations, which are already zero, then the gradient into the direction of w will again be zero, and I don't make any progress, because I multiply up zero gradients. So, this is an issue. The other issue is symmetry. So, let's assume our parameters for each of the neurons are not zero, but they are the same for each of the neurons. So, I... basically, my matrix w just consists of, like, a, b, c, d, a, b, c, d, a, b, c, d, so if I have, like, the same values over... c, d... so if I have the same values for each of the neurons, so each neuron has some values which are different, but each of the neurons is the same. So, if I think about some of my inputs, and I have, like, three, four inputs and three neurons, and each of the neurons calculates the very same number. So, if I have my inputs over here, this one will say my output is five, and this one will say it's five, and this one will say it's five. Then, if I do my back propagation, if everything over here was the very same thing for each of those neurons, then the gradient for each of the neurons will also be the very same. So, the gradient turns out to be the same for every neuron, and if I now say that my matrix W is equal to W minus some learning rate times the gradient of W... into the direction of W, then I'll just subtract the same thing in each row, and each row was the same beforehand already, so it's different, but it's still the same. So, each row is still the same, so every neuron will have now a different output, but still the same output. So, the neurons never differentiate between each other. Every neuron will still produce the same output every time, and that is pretty bad, because no matter how long I train my network, every neuron will still give me the same output, and if it gives me the same output, I could have just left them out. I don't need ten neurons which all say the same thing. I can just use one and multiply its result by ten and still have the same effect. So, we need to make sure that we can break this symmetry, that we don't end up in some symmetrical solution. And the easiest way to do that is just start with random values. So, and the nice thing is I don't even need random values for everything, not just for the values in my weight matrices. So, the bias terms can usually be initialized with zeros, but for the weight matrices, I still need to put in random values and make sure that, I need to put in random values and make sure that they are all different in some way. . Something like this might happen. So, it's unlikely that they align perfectly. So, it's... Thankfully, thanks to numerics and everything, it's usually that there will still be some difference even if they get very close. It's incredibly unlikely that you turn out that, it turns out the two neurons get the exact same values at some point. But it's not impossible. It's perfectly possible that you end up in some degenerate solution where you get, where two neurons are, due to some circumstances, are forced to get the same values and then, or, and after that are kind of useless in some way. So, that's... But in practice, that doesn't seem to be an issue. It's... So, if you take random initialization, they usually tend to... They might still... It might still turn out that two neurons are too similar that they don't make... That it's not very useful what they are doing, but... Or not usefully differentiated, but it's... They... You usually break up the symmetry this way. Something you usually do is that you multiply those random values with something small. So, you want to make sure that those values are close to zero but not exactly zero. You just multiply and you just want to make sure that you break up the symmetry. It's... And have some direction to start with. So... And one of the reasons for this is also that if you... So, we want to have values that are close to zero. And if you think about all our activation functions, the interesting stuff always happens close to zero. So, you want to make sure that whatever your neural network predicts, it should be somewhere close by this interesting point because that's where you differentiate between, like, the... Like, the good and the bad examples. So, we want to, like, put bad apples on one side, the good apples on the other side, and the... Like, having the non-linearity here should be the point that kind of differentiates between them. And... Because that is usually close to zero, you want to make sure that your values are also close to that point. But... If you make this too small, then you are kind of getting problems again. But, like, something random close to zero usually does quite fine. Also, when doing logistic regression, you always have, like, a clear-cut global minimum. So, if you do gradient descent, you will end up at the same parameters every time, no matter where you started. When training your neural networks, you have a lot... You have a much more complex surface over here, so... And there is... There can be local minima, so... And there can be a lot of issues when training neural networks. And if you, like, start again... Start your training run again with different random parameters, you might end up at a completely different solution, and it might even be better or worse than another one. So... The... The... Like... This randomness over here also might give you completely different solutions when it comes to the final neural network that you have. So, in general, we'll also talk a little bit more about this. It will not be such a big issue where you start exactly, but it's important to keep in mind we start, we randomize the starting position, and, depending on where we start, we may end up at different local minima of the... of the loss function and have... get different classifiers, because it's not... not a simple surface where we are guaranteed to always end up at the same point, like in logistic regression. So... That's it for today. Do you have more questions? I know that I threw a lot of math at you today, and a lot of the... and a lot of the... those... those scary-looking gradient parts. It's... When you start implementing everything... all those things, at some point, it might click for you, and then you realize it's not... it's easier than... than the scary things over here look like, because, again, as I already told you, it's all... it's just multiplications and additions in some way, and put... blocked into fancy operations, which are matrix-vector multiplications, but it's all addition and... and multiplication at the... at the lowest level, and... at the lowest level, so... do... do the exercises and try to figure out what... what... what actually happens deep down in there, and then usually the... the scary part goes away after... at... when... when it clicks at some point. And see you on Friday. It is a leaven, then, says he, and it's in Norhing. Is its mile where, may send a little. Nay, and that she knew with her all men. I had dealt a frail. So welcome to artificial intelligence. Some organizational things first. So there is an OLAT course for this. So I hope this link is the right one. Yeah, so and the OLAT course has some kind of the links to the relevant links. And I will upload exercises here and the solutions for exercises. Exercises are in some way completely optional. You can do them, you can not do them. I prefer if you do them or at least you should prefer that because if you don't that's your loss. The OLAT course has links to the slides I'm using and the slides are in the description. So if you want to download the links, I can see them here. So I have some links here, so you can download them. So if you want to download the links, I can see them here. So I have some links here, so you can download them. So if you want to download the links, I can see them here. So I have some links here, so you can download them. So if you want to download the links, I can see them here. basically just web pages with fancy web pages if you want to. So it has like the slides itself and version is the print version. So the print version is basically also just a web page, but one which is kind of better suited for directly printing slides if you want to do that. Printing in a lot of cases the print looks the way it should look, but in some cases the slides, if the slides have interactive elements, then printing doesn't really make sense and then printing also doesn't work that well. I will, I have created a Teams channel for this course so you don't have to, but probably it's a good idea to register there and for asking questions. So I have created a Teams channel for this course so you don't have to, but probably it's a good idea to register there and for asking questions. So if you want to, you can do that. and getting answers either from me or from somebody else in the course. So it's good idea for communication and in case there should be the next pandemic or don't know what we'll and we have to switch to making remote classes, I will also do that via Teams. So who knows what the future holds? I will record all the all the lectures and upload them into the this Panopto folder. So obviously, so you can log in there with your THBing credentials and once there was a first lecture there you will see the videos there and can for example when doing exercises, it might be a nice thing to go through some parts of the course again or when preparing for the exam at the end. There I have created a Jupyter hub for the exercises and I will talk more about this later, but if you want to solve some of the exercises you can do that on using this Jupyter instance that I put on one of the THBing servers. I'll talk about that later, more about this later. So yeah, so we can do the exercises there, lecture recordings and at the end of the course there will be a final exam and we have termining, grade and and so on. The exercises will be posted in the form of Jupyter notebooks. So again more more on that later and I will post them into this OLAT course over here. Okay, so basically basically, I'm going to do a little bit of a demo here. So basically the Jupyter notebook is some kind of remote Python environment. I probably a lot of you have already some Python experience, but in case not I'll use the exercise on Friday to make a brief Python introduction. So if you already are pretty experienced with it with Python and with numpy the Python one of the main Python libraries we will be using for Python. So if you are interested in learning in this course you can basically skip Friday, but otherwise I'll give kind of an introduction into those things on Friday to bring you up to speed. So everything will be in form of of Jupyter notebooks. So maybe as a very very brief introduction just now, Jupyter notebooks are kind of a remote Python library. They live, all the notebooks live on kind of a remote server and what you see is a small web front end to edit them and all the notebooks are comprised of small cells where you can write some small Python code and which you can use to do some kind of Python and execute individually and see the results and which gives quite a for at least for those small examples and small projects we will be doing here is it's kind of a pretty pretty nice developing experience. Also something nice about this is that the server has actually a lot more horsepower than probably your notebook has because it has two GPUs installed and quite a bit of hard disk. So you can see that the server has actually a lot more horsepower than probably your notebook has because it has two GPUs installed and quite a bit of hard disk. So you can see that the server has actually a lot more horsepower than probably your notebook has because it has two GPUs installed and quite a bit of hard disk. And memory so that we can even do some more demanding tasks there. memory but for example, doing larger image classification tasks does not necessarily require a GPU but it makes the a GPU, but it makes the different between printable and printable. been developing something in a few hours or waiting weeks for something to finish. And that can be a nice thing to have those. You can use this Jupyter instance for other projects. So if you, for example, do your master thesis and need some compute power for that, you can also use the Jupyter server for something there. So it's not restricted to this course. Just don't abuse it. So if I see anybody mining cryptocurrencies there, I will press charges for that. So references for this course. There's kind of one really good book about deep learning by Ian Goodfellow and Joshua Bengio and Aaron Colville, which is recommended but not necessary. So necessary. None of those references are necessary. And Andrew Eng has put up a lot of good learning resources as well and learning videos. And I'm following along a lot of his course material. So that's kind of also quite a good resource. So in total, this course will... Artificial intelligence in general is a very, very, very huge bucket of different things to do. And the main focus of this course will be... On deep learning techniques. So that's not all there is to artificial intelligence, even though at the moment it sounds a little bit like this. If you follow the media, everything that's... All the big AI breakthroughs are deep learning based at the moment. And there's a lot of gold rush fever around deep learning topics. But it's not everything. It's not the entirety of what artificial intelligence is. And there's a lot of other techniques and algorithms that are also incredibly useful and widely used in a lot of industry contexts. But this course will be all about deep learning and how to build neural networks, how to build deep learning based systems to solve a variety of tasks. So going a little... A little bit into the history of artificial intelligence. So if... In the 50s, many, many, many of the techniques here are old. Even some are much... Even older than this. So kind of machine learning. And it's first mentioned in this book... In a paper about the perceptron. And... And... Where the... They first built basically a machine to do machine learning. So it's... That was the time where computers were still room-sized things. And the perceptron was a machine that could do classification tasks by learning from data. And the first instance of this was basically a machine for this thing. And now that machines can write number of41 doesn't have this specific purpose. Nevertheless, just as sort of a parent system, a computer line is really similar to cars that can write kans using the 60sAND and even more, it can create a macomb Tableau algorithm that's specific to simple course. However, one might be confused about why this mechanism works. It's simply a dual. It's able that all standing things can play and it must be balanced according to different and in the other matter, it has to it's kind of funny that there hasn't been any changes in the fundamentals there. So there's like little tweaks and some little engineering ideas to make it work better. But the core idea is still the same for the last like 60 years. Back then, they discovered a few fundamental limits for neural networks. It's funny that back then, there was a big report about how a simple linear model, for example, a linear perceptron can compute and what it cannot compute. And this report led to a lot of funding for AI research being frozen and a lot of research being discontinued. Even though those fundamentals... The fundamental limits are kind of... It's a pretty weak... It's not like they said a neural network cannot compute a lot of... Cannot, for example, ever do image classification. It basically said that, for example, one layer neural network can never solve the XOR function. And yeah, which doesn't say anything about two layer neural networks. And so kind of... This report was... Misread by a lot of legislators back at the time. So in the 80s, people rediscovered backpropagation. And back then, we got the first proper industrial uses of neural networks. They had the first convolutional neural networks for identifying digits on letters. So the U.S. Postal Service was using a machine learning algorithm that used a convolutional... Neural networks. More on that later in the course. Which classified the individual letters for the zip code on the envelopes. And thereby kind of read in the zip code automatically. Back then, a lot of things that we kind of rediscovered later on... Were already invented. So kind of reinforcement learning, support vector machines, recurrent neural networks, convolutional neural networks, as I just said. But back then, people were lacking mainly two things. And that was sufficient data for training all those algorithms. And the compute power to really run big neural networks. And so interest all died down again. And it... It took some time until all those things were again rediscovered. So in the 2000s, there were like two things which started to get everything going. There was one thing, the Netflix price. Netflix put out a price money of $1 million for somebody who could improve their movie recommender algorithm... By... 10%. Or more. And it turned out that... And they put out their movie recommendation data set for everybody to use and to fine tune their algorithms. And that was a pretty big thing. Because that was kind of the first time where a big proprietary data set was kind of free for the taking for everybody out there. And to do research on it. And probably the price money was net. And the price itself was also a nice thing. But... But the... The fact that they put out a really, really huge data set for everybody to work on was kind of a novel thing. Up until then, most data sets existed in walled gardens. And... This was one of the first times... That was one of the first things that needed to be solved. The access to large amounts of data. And... Later, other people started ImageNet. Which was a kind of library of publicly available images. With... Tagged with classifications of what you can see on the image. And... They... That was also publicly available. That then also became publicly available. So... In the 2000s, it started that we solved some of these data access issues. And in the 2010s... In the 2010s... And... We... A neural network called AlexNet. Was... It solved image classification on this ImageNet data set. On a level that was on par with human people. With humans. So it was almost as good as somebody... As humans could classify those images. And that basically led to the deep learning boom that lasts till today. So we... since then people discovered now we have access to enough data to train those really large models and we have enough compute power to train those models and it seems that we can now do really useful things with all those techniques that were discovered pretty far back then which led to people doing more and more applications for this and finding more techniques to use these on different data sets on different types of data and to find new applications some of those applications so we can nowadays basically do machine translation on a level where so like 10 years ago machine translation was still something you could ask Google Translate and sometimes get pretty funny results and sometimes translations back in the day used to be still pretty shitty in some cases nowadays machine translated texts are basically as good as a human could translate them so it's very rare that you see cases where a good machine translation software doesn't do a proper translation or does something where it results in anything funny we can do object recognition we can try we can do object recognition we can identify objects and images classify them this image also kind of is kind of leading up to another application which is kind of self-driving cars so nowadays image recognition is good enough that we can build reliable systems that can identify okay where is the lane on the street and where are other participants in the street so I can steal and I can reliably steer a car in this environment up to some limits so it's not at the point where we can have fully autonomous self-driving cars but the tech is getting better and better and I think it's just a matter of like next 10 years I guess we will have fully autonomous self-driving cars as more and more issues get resolved in the coming years so we could see if that happens it's the expression it's going to affect the action and we want to build things for the future so it's a change from level two in technology in which order are the different molecules within the protein. But that just gives you a long string of molecules. What is interesting and what determines the way that the protein works is the shape that the protein will take on in the real world. So if you take this long string of molecules, they will fold into a certain shape. And this shape determines the properties that the protein has. And it's not trivial to know, if you just have the string of molecules, how they will behave in the real world. The physics is kind of well understood. They will fold into the configuration of least energy. But it's non-trivial to know what this configuration of least energy will be in the end. And a team from... Google created this algorithm AlphaFold, which is a deep learning based system that trains on proteins where we already know this so-called tertiary structure and from the two-dimensional structure or from the long one-dimensional structure and kind of learns how proteins... different molecules tend to interact and does a pretty good job at predicting this three-dimensional structure. Which... So it's still pretty young, but there have been a lot of medical breakthroughs thanks to this now that they can do things like we need a protein that kind of binds to certain other protein parts and they can now do things like, okay, we test a lot of proteins, we test a lot of different protein configurations now and check, okay, what will be the tertiary structure of that and then from that... they basically get the idea, okay, what is the protein that they need to synthesize and derive medications from there. So there has been some breakthroughs for artificial intelligence in games. We had certain games where humans were... always kind of better than the machine... than machines. So chess was pretty... was solved in the 90s with Deep Blue. But like more complicated games where there is social interact... there's interaction between people and... complicated environments which are hard to parse. That took more... much, much more than chess did back then. So you need to parse much more than... much more information in a visual image here and it's much harder to... probably build a bot that can solve those environments. But deep learning is no magic pixie dust. It's not like you can have any kind of problem and you can just say, okay, let's just throw deep learning at this and... it will magically solve the problem that we have. There's a lot of limitations of when can we use a deep learning algorithm for solving something. For example, anything machine learning based will need, as the name suggests, something to learn from. And if we don't have the data to learn from and if the data is not good enough to learn from, we have no chance to build a learning algorithm for the problem that we have. So there's still a lot of tasks where we don't have the proper learning data, the proper way to solve that. So it's not... We still need to think ourselves to how to approach those problems. As I said in the beginning, artificial intelligence is a pretty big field. So there's... What we will cover in this course mainly is... will be the deep learning part, which itself is a subfield of machine learning, which covers much more than just neural networks, which itself is a subfield of artificial intelligence, which also covers other things. So artificial intelligence also covers things like planning algorithm, shortest path problem, for example, is also an artificial intelligence problem that we won't cover here, or how to solve, for example, if you want to solve a time problem, like making this timetable for a university like here. That's also an artificial intelligence problem, but one where, for example, a deep learning algorithm is not the ideal choice for solving that. So a question that we... I already started to answer a little bit. Why do we have the deep learning boom right now? So why is deep learning something that took off like six years ago and is kind of created... creating so much fuss right now? Why didn't it in the 80s, when a lot of those algorithms were already known? So in some way, more information now is digital. So back then, almost no information was digital. So the Internet was basically some kind of... something that was used for universities to change a little bit of text data and communicate with each other, but not something every university would do. Everybody had it used. So digital data was almost non-existent back then. Nowadays, all the information is digital. So we have images, texts, shopping transactions, and whatnot. And it's all already available in digital form because the information is basically directly created digital. If we think of... So this image is scaled a little badly, so the axis here would be data. So kind of on a log scale. So amount of data. If we take very, very simple linear models, they kind of... They perform well with little data, but it doesn't matter how much data you throw at a very simple linear model. And so as long as you have only little data available, you don't realize that your model has kind of fundamental limits in what it can compute. But the bigger you build your model, the more powerful your model becomes, the more it can benefit from having large amounts of data available. So if you have, like, a very small model, a very, very large neural network, you will basically have the effect that as long as you have only a little data available, it will underperform a linear model or a smaller neural network. But if you have a huge amount of data available, then it will start to give you more performance. And that is basically... As we were still in an age where there was not that much data available, there was no use in producing bigger models or training big neural networks. There was just not enough data to train them properly. So if you look at something like ChatGPT nowadays, that is something that is trained on a huge amount of crawled internet data. So we don't know how much data they exactly use, but there is kind of... The open competitors to ChatGPT they use certain crawled data sets which have a few terabytes of data available. Some of that... On the Jupyter server, I have a copy of one of those dumps that can be used. It's like two terabytes of text data. And two terabytes is an enormous amount of text data. That is more than... So if you take, like, your average library and would digitize all the books in there, that's a few gigabytes at the most. The terabytes of text data is incredible amounts of information. So if you think about... All this, we don't do... Our plan is not to use neural networks because neural networks are incredibly cool. They are. But the goal is we want to solve problems. We want to build products that can be used by somebody and that do something useful. And to do that, we need kind of deep learning... Groups of people who can create those products. So the question is, what makes a successful deep learning team? So what is a team that can build a successful deep learning product? So there are several factors that make teams that can build successful deep learning products. So one thing is they are really, really good at acquiring data. So data is kind of the most important resource that you have when it comes to machine learning algorithms. So everything else, if you start with a shitty model, it's okay. You can improve on that later. If you have shitty data, you will never get better. So it's kind of the... Having good and enough data is kind of the most important thing to start with. And if you have no way to acquire that, your product will definitely fail. So that is kind of the... If you're good at this point, you have kind of got the most important issue out of the way. The second thing is good deep learning teams use every opportunity for automation. So if you... If you think about it, artificial intelligence is all about automating things. So the idea is we want to build algorithms that can solve something, that autonomously do something for us. And basically, a good deep learning team kind of tries to do the same on the inside. So you want to try to automate all the things that you do, even inside the team, to scale up the resources that you have. And if you think about managing two terabytes of text data from the internet, you will not be able to kind of manually do anything in there. It has to be automated pipelines that process the data and do all the things in between... in there. In a similar way, almost all companies have data that is stored away in different silos. So you have like different parts of the company and they rarely talk to each other. And an important part is that you kind of... that you are incredibly good at data warehousing. So taking in the data, different data streams from different sources and joining them together is also something that... makes really, really good deep learning teams. And if you think about it, if you have like... which companies are incredibly good in artificial intelligence nowadays? You have mainly companies for which kind of those... for example, where this first part was incredibly easy. For example, Google. Aggressive data acquisition is incredibly easy for them because they are already completely digital. So all the... people come there, enter search terms into their... into their web search engine and produce already... immediately digital data that they can use for... later on to improve their search algorithms and so on. And they also basically started at this point. They are not like an old... chemical industries company like BISF in... where they started without any kind of... where the internet... they started where the internet didn't even exist. So joining all the data sources is kind of... they basically could start with... when they started, they were able to make sure that... all the teams have access to all the data that is necessary for them. And they didn't even... they had... were able to not even build up the silos in the beginning. And you have the same with all the big internet companies like Amazon and Microsoft and so on, which now are kind of the dominant players as well for... when it comes to artificial intelligence. So this... because they had it easy to... to get the data in the first place and to not have siloed data sources. And a lot of the big industry companies nowadays, they... which try to also get good at this. Think of, for example, the big car makers. They have a much harder job to even get good at data acquisition. So Tesla kind of already built data acquisition into their product from the get-go. So if you drive a Tesla, they will... they will gather driving data from your car all the time with every mile you drive. Your Volkswagen is not doing that. So... especially if you have a... older Volkswagen model. So... having this ability to immediately build in data... built-in data acquisition into your product is kind of a pretty important thing. So... this kind of gives you an idea of data is incredibly important and get... all... everything you do in this course will only be as good as the data that we use to feed those algorithms. So... and... within the course, I will focus a lot about those algorithms. So you get... will learn how to build... deep learning models. But a lot of those practical things... how to build data warehousing, how to build products in a way that you can immediately acquire data from the user will not be something I can teach you in this course. It's kind of... it's... would be out of scope. But it's also... but it is also something that is incredibly dependent on the industry. So... if you... for example... how to build a webshop in the way that... you can always collect information what the user actually wants to see and what not is kind of a very, very complicated thing user interface wise because you need to do something to build the interface in a way that in a non-obstructive way the user can give this feedback or automatically generates this feedback without... giving you a shitty user experience and that is... is... kind of a very, very complex and... skillful thing to do in the first place. So... within this goal I want to teach you the relevant deep learning models and where to apply them. So we will cover several model architectures. I will teach you how... deep learning works... from... from the mathematical side how to implement deep learning models we will implement a lot of those completely from scratch and then... slowly work ourselves up using frameworks that take away some of this... the necessary work and so we can build bigger and bigger models and more powerful applications. We... will implement and train several deep learning models and we... I will try to... help you... getting the... the know-how how to... debug those. So it's... if you think about how does any kind of software project work in practice it's usually you try to do something and it doesn't work and then you start debugging. And... so... I will try to also teach you some... some ways of how to... to figure out why something you are doing is not working and because that's usually the... the... default status for every kind of software and at least in the beginning. And... and yeah... make you able to fix those problems. So I will not... cover other... machine learning techniques like for example support vector machines or k-nearest neighbors or a lot of other things that are... for machine learning or other... artificial intelligence... techniques. So... these will not be part of the... at least of this course. So in the exercises we will implement a lot from scratch and to see how the details work. So we will... while there is a lot of deep learning frameworks where you can just say okay... I want to have a neural network with... three layers, this many neurons and this is the data. Go train it. We will start with... implementing everything from scratch. Say okay... the... this is the data. This way we turn it into... into vectors. These are the matrices that are... define our neural network. This will be the gradients of those matrices. This will be the updating rules. How the neural network would update in each step. And... so that you get a better understanding how all those things work under the hood. Because that is kind of the... the... the thing that will be incredibly important to be able to fix problems. Because if you just blindly use a framework if it doesn't work you have no clue why it doesn't work. Because you... you don't know... what is the... the... the arrow mode. What... what went wrong. And so... my goal is to demystify those inner workings. Because kind of neural networks kind of tend... tend to scare people away. They... treat it as black boxes where nobody knows how they work in the... on the... on the inside. And... my goal is that when... at the end of this course you will know how they work on the inside. That is... that this mystery will be lifted at least for you. And so... and... and that you know how the details on the inside work. So... I showed you... at the beginning I showed you this Jupyter server where the... you find the link on... on... in... in OLAT. If you don't want to use that or if you... for example want to work offline because... because you don't... the internet is bad which at the... at the university can often be the case. So the Wi-Fi here has... is kind of flaky a lot... in a lot of cases. You can install your own Jupyter environment. So... one way to do that is... for example the Anaconda distribution which exists for most... relevant... operating systems and... and which is kind of one of the easiest ways to install kind of a Jupyter distribution... a Python distribution alongside with Jupyter and everything that... you might want to... want to use. But you can... you... if... for doing the exercise you can do that any way you want to. So it's not... there's no required way and... especially if you have... if you are more proficient with your laptop setup then probably you'll prefer some other way. But in that case you... I also... you also probably don't need my help to do that anyway. So... if you want to... make a local Python and Jupyter installation this Anaconda distribution is kind of the... the way I would recommend it for you. So... are there any questions... regarding course logistics... the topics of the course? Yeah? I don't find the OLAT course for... for that. You don't find the OLAT course? Anybody else with that problem? No, I could find the... You could... Okay. So... it should... so... I would... I would say I'll send you the link via Teams but probably that's... that's the chicken and the egg problem. So... We can find it over the... the Masters course. So... True, there should be... there should be a general Masters course. So... are you in... registered in the... Computer Science Masters course? There is... there should be one where there's a link. Otherwise... So, if I'm... catalog... So otherwise... if you are going to the... OLAT catalog... and go to TH Bingen... and go to FB2... and then search for me, which is... this nice looking guy here... then it should... then it should be... then I was too stupid to make sure that the course is... in there as well, so... this one... and... yeah. And... now probably the internet broke down because... it should... So... and... Here we go. So, okay, now... now you can find it in my... my course list here. So... Otherwise... you can also... if you send me an email... I'll try to send you the link as well. So if you... That goes for the entire... entirety of the course. So if you have... run into troubles or issues... at any point... feel free... ideally... write something in the Teams channel... because that means... maybe somebody... so somebody else might be even able to help you before I do. So... and other people can also see... the solution for the problem as well. So if you have issues... just write in the... in the common Teams channel... ideally. So... and then... then... then we'll try to... to resolve the problems. So... on Friday... as I said... I'll do kind of a... a small Python introduction. Now we'll start with... the... the... the... the... the main part of the course. So we... we'll start with... the question... what actually is a neural network? So... again... I have some... formatting issues with those images here. So... assuming I have some data... so I have... the size of... the... information about... the... the... the... the... the... the... the... the... the... the... the... the... the size of a house... and its price. So I have like the... number of square... square... square meters... and I have the price... on one axis. So I have like basically... two dimensions of data... and I have... one two three four five six houses... and... what I want to do is... I want to have a way... to predict... if I've given... any kind of... any kind of size of one house, I want to predict the price for it. And one of the easiest ways to do that is do linear regression. So we can say, okay, I'll plot a line in here, which is a linear function of the size of the house. And which has two free parameters, W0 and W1. And once I know those two parameters, I can calculate for any kind of size a price for that house. It doesn't need to be the correct price. It's just the way I'm modeling the world. I'm saying, I'm assuming the price is roughly less, but I'm trying to learn the parameters of that function. And those are the two parameters that I want to learn. And given I have them, I have my entire model H. And that model will give me a price for the house. And what we want to do is, we want to calculate the cost of the house. And this kind of linear model is kind of one of the earliest things for machine learning in general. So that was basically invented back by Gauss in the 1800 something, how to calculate a regression line through several data points. And what we would want to do is calculate those parameters, such that the distance, and what the distance is, we'll see later, but that the distance between the actual prices of the data points that we have and the price that we predict gets minimal. But if you think about what this model that we have here does, then there is one... issue that kind of would be more obvious if the cropping wouldn't be so bad. But if we have a price, or if the size of our house gets pretty small, the price gets negative. And that is kind of very, very obviously wrong. So we can try to fix this model and say, okay, we make a new model. And that model is, take the maximum of the... the linear function, and zero. So if the linear function that we just had is bigger than zero, we'll just take that one. And if it's below zero, we just take zero. So we kind of cut our function off at zero and make sure it doesn't get below that. And this is probably a better predictor than the one we had before. So because we kind of have fixed one of the small issues that we have with this model, we never get a negative price. And that makes things at least a little better than it was before. Still having a zero-priced house is probably pretty unrealistic, but it's at least not as wrong as it was before. And what we basically did here was we created a very, very small neural network. We have... a very small neural network. We have... some input, which is the size of the house. We have our neuron, which is... this little function here, which... it takes... has a linear predictor, and this maximum of the linear predictor and zero part, so it has kind of something additional to this linear part, and outputs some estimated price. So it's... and... the neuron here is basically this function. So that is mainly what one neuron in an artificial neural network is. Doesn't need to be those functions. It doesn't need to be... look exactly like this, but it's one way a neuron could look like. When you hear people talking about neural networks, then the neural networks are often compared to the brain. And this is... it's somehow... the comparison doesn't always hold very well, especially in this case. Human neuron is way more complicated than what this neuron does. So... like the information processing that happens within one human neuron is way more sophisticated and does way more than like this simple linear plus a little bit on top operation over here. So... the comparison between like a human neuron and this artificial neuron that we created here is weak at best. Um... If... the neuron that we have here, as I said, consists of a linear part and something on top of this. And this something on top of... is called the activation function. Which is... it's a one-to-one function. So it's a function that takes one input and produces one output. And in this case we use the maximum of... the input and zero. And say, okay, this is... the... that's what this function G should output. And... um... this particular activation function has a name of its own. It's called the rectified linear unit. So... it's... the name... mainly derives from its linear up to some point where it's getting rectified. And... so... we make sure that... it never drops below zero. And this... we'll... come back to that later on. But it's kind of one of the most used activation functions for neural networks. So... this simple... this very, very simple predictor... might do some okay-ish job for predicting the house price. But we actually probably want to do better. And to do better... we need to take in more information. So that we... the size of the house is one particular... piece of information that we can use. But we probably want to use more information. And... um... what we want to do is, for example... um... use... more inputs... like the size of the house... number of bedrooms... the location where it is... the distance to the next public transport... and so on. So we have more information for each of our data points. And... we don't want to just use one... stack of neurons. But... do something like... have one neuron... predict some... intermediate feature. So, for example... this neuron... should... predict... not the price... but... the possible family size that the house could accommodate. And it could predict that from the size and the number of bedrooms. And this neuron... should predict the... school quality of the surrounding schools. It might be able to predict from the location or the zip code. And it might be... this neuron should predict... the commute time for the inhabitants. Which it might be able to predict from the zip code... and the distance to the nearest public transport. This way, those neurons... derive some... create some derived features. They calculate something that is... not directly in the input... but can be computed from the input. So we get more refined features. And the next neuron... will take those more refined features... and predict the price of the house from it. And that is basically what... a real neural network is. We have several layers of neurons. Each layer... computes some more refined features... from its inputs. And gives those... to the next layer of neurons... which can use the more refined features... to make either... the final prediction that we want to have... or create even more refined features. And those... we do... and when using... building a neural network... we usually do not observe those intermediate features. We do not... they just get passed to the next layer of neurons. We only observe that part here. We look at the output that we are actually interested in. We do not look at those intermediate features. And... because we do not look at those... we also actually do not care... if what they actually represent. So we do not... what we will do in reality is... we will... let the algorithm... figure out those intermediate features on its own. So... it might turn out that one of those intermediate neurons... will do something like predicting the possible family size... because it is a useful intermediate feature. But we do not force the algorithm to do exactly that. We will let the algorithm figure out... on its own... what might be a useful intermediate feature... to make a better prediction for the price. And then it will kind of train those neurons... to predict that intermediate feature... so that this neuron can... has an easier job doing the price prediction over here. So the job of this neuron is basically... figure out some intermediate property... of the data that you have here... so that this neuron has an easier job... to predict the price. And if you stack on more layers... each layer has basically the job of... make the job of the following layer easier... and predict some... figure out some property... that might make the job for the next layer somehow easier. And our training algorithm will... later on figure out... what the useful intermediate features will be. So also... in this case we said... okay this neuron... predicts the possible family size... from the size of the house... and the number of bedrooms. Because we do not know... what the final features will be... we usually do not put any limits... on what kind of input... which a neuron can use. But say okay you can use any of those inputs... to make your prediction... and you figure out which input is important... and how important which input is. So it might figure out... that it doesn't need those... and puts a weight of zero on this edge... and doesn't use the zip code... but it's up to the neuron to figure out... what input features it wants to use... and which not. We call this architecture... being fully connected. So every input from the last layer... will be connected to each of the inputs... of the next layer. So each output from this layer... is connected to each of the inputs... of this layer and so on. What the neural network does is... each layer outputs a new... more abstract features. Which will make the job... for the next layer easier. During training... the algorithm decides... what features are most useful. The algorithm will decide... what it wants to learn... to make the final prediction... as good as it can. What kind of applications... can we build from this kind of... abstract general concept? In our case we had... several input features... like house size... zip code... distance to the next public transport... and so on. And we had one output variable... which we want to predict. For example in our... small real estate application. But we can have... other applications. So what we can for example do is... we have an input... like an advertisement... and a user's cookie history. And as an output we want to have... did the user click on the ad... or not. Which is kind of one of the earliest... use cases for big data... where internet marketing companies... started to use those... massive amounts of... cookie history data from users... to create more targeted ads. Which are haunting the internet... ever since. Other things can be... our input can be some kind of image... and the output... can be what object... or objects are in the image. Like you want to take... a photo or we want... to find out if there is... a cat on the image or not. Our input can be some audio data. And our output could be... something like the text transcript... of that audio. And we want to kind of do... speech recognition on some audio data. Could do something like machine translation... where we have an English sentence... as an input and one... Chinese sentence as an output. We could have a lot of... very very different inputs. Like image data... from different cameras. Some radar or lidar information. And we want to output the position... of other cars and our position relative to them. For like an autonomous... driving applications. So... when thinking about... how to use deep learning for something... we need to think about... in this kind of... abstract way. What is kind of the input data... that we have? At any point in time... what data does the... algorithm have access to? And what should be the prediction... that the algorithm has to do? So what is it that the... algorithm should produce... when it sees something? And in some cases... that is kind of pretty obvious. But for example if you think... in some cases... it's a little bit surprising. For example with... ChatGPT... the input is... I have a text up to... a certain point and the prediction... target is... what is the next character in the sentence. So... which is... which is a pretty surprising... thing because... when you start to build something... like a chatbot... you think about... what I want is an answer... a certain amount... a kind of question or something like that. I have a question and I want to generate an answer. But... answers are something where we don't have any... training data for. But the next character is something... where we have a lot of training data for. And... if... and surprisingly... predicting the next character is... doesn't give you the entire answer but... if you do it often enough... you will get the entire answer. So... using this kind of surprising... target that we have here... in this case... yields an incredibly powerful system... at the end. Thinking about... what will our algorithm... get and... what should it output and... do I have enough data for... exactly this kind of combination. So I can think of... a lot of things where I would want... to have a prediction... but where I don't have enough data for. And... so we kind of have to think... do we have enough... audios with... text transcriptions at the end... so that we can create our speech... recognition system and... it's kind of... the important thing to think about... when starting some... project. So what exactly... will be our input... what exactly will be the output... that our algorithm has to predict at the end... and... how do we get enough data of those... input-output pairs so that the algorithm... can train on that. So... if you... when we go through those... examples... the first two of them are very very... simple ones. So it's kind of very structured... data... so the cookie history will be like... probably a list of different websites... that the user visited and... we have like a binary output... and this is just a number as an output... so we have very structured data... and do some predictions on this. And this will be... kind of a use case for... classical fully connected... neural networks... that one can use for this or... it doesn't even have to be a neural network... it could be also a use case for very very classical... machine learning algorithms like just... some kind of regression problem. When it comes... to look at other of those examples... it gets more difficult... to think about what exactly... those inputs and outputs... are. So for an image... an image is not just... it's a way more complicated structure... so you have like... basically a matrix... of pixel values for each pixel... you get the... value of... a red value, a green value... a blue value... which... form the entire... image and... images can be different sizes... some images are larger, some are smaller... so they are not constant in size... so it's much much harder to handle those... and for those... kind of applications... we will learn about different... architectures of neural networks... that can handle those kind of... more complicated inputs. Same way... if you think about audio data... you get kind of a stream of... audio signals... and if you want to have a text transcription... that is also not like a binary output... or like... just one number... it's kind of again a stream of characters... that you want to translate audio into... and... again we will look at... see about how several tricks... that can be used to... to deal with like those... more complicated... structures of inputs... so these... this for example is kind of... the classical use case for... convolutional neural networks... those will be the classical use cases... for... recurrent neural networks... that can deal with sequences of... information and something like this might... even need something very very custom... where you have like a very very diverse... amount of inputs... so if you do autonomous driving... you have kind of image input... over time because like the... the images... it doesn't just matter... where what you see on those images... and on your sensors right now... but also what you saw like... within the last 10 minutes... because even if you... don't see a certain car in any image... at the moment... it might still be around you and you might... have to make an estimation of where it might... possibly be and so something like this might... require a very custom... and very specialized... architecture... so if you... if we think about for example... image classification... so... what... we want to... what we get as an input is... some kind of image... and what we want to produce is... some kind of output... is this a cat or is this not a cat... and... ... so we basically have a binary output... one or zero... and as an input we... have kind of... a lot of pixel values... more of them later... but... as before we have like two... inputs x and outputs y... and the... input notation that we want... we will use is... we will say that... our inputs are called x... and they are vectors... in an n-dimensional space... like... an input vector... ... so... and this one would be a one, two, three... four, five, six... dimensional... input vector... the label... or what we want to predict... is... a variable that we call y... and in this case... it's a binary one... so it's either zero or one... and we will call the tuple x and y... a single training example... so... if we have like one... piece of input data... and the corresponding label... that is one piece of data that we can... train our algorithm on... ... so and... we don't need just one example to train... on... we need a lot of those... so we will need an entire set of training examples... and... we call this D-train... so our training dataset... will be... a set of m... examples... or m-train examples... each being a tuple... of one input feature vector... and... the training label... ... so and... this number m is the number of training data that we... ... that we can use... something else... that we will later use is... the number of... test examples... so we will use a sec... later we will use a second dataset... which we call D-test... which also... contains a number of examples... that we will use... for testing our algorithm... and that... we will talk about this later again... but this will be an important thing... we should never... build some kind of... machine learning system... and then... just use it without ever checking... how good it does on data... that it has not seen before... so the test data... the idea of the test data is basically... that we use some data that is not... part of the training data... that we can use to evaluate... how well does our algorithm do... if it sees new information... that it has not seen beforehand... and... which is why we kind of need to withhold... this information... there are some exceptions to this... if you for example train something like... a language model like... ChatGPT on... 20 terabytes of data... and the... amount of data that you have is so... enormous that your algorithm... during training will see each... input example at most once... and probably you will not need... to test the data set anymore... because the amount of data... that you... of data is... humongous anyway and you can just... use the data that you trained on also for testing... because it doesn't matter anymore... but on the other hand... taking something out of such a big... data set also doesn't matter anymore... so if you have that much... data then kind of the... rules change a little bit... when... doing any kind of calculations... here... we try to use... to do as... much as we possibly can... using vector calculations... so if you have ever... worked with Matlab or... with NumPy and Python... for example... if you... calculate anything in some kind of... loop if you write a for loop... for the first value... and I multiply it with... the first value in the other vector... and do that again and again and again... you will get incredibly slow code... because the Python code part of this code... is incredibly slow and... Matlab is also a pretty slow scripting... language on its own so... you get pretty slow code if you... write anything in loops... if you... want to have fast code in Python... you need to vectorize things and use... some kind of... library like NumPy that... does vectorized operations... and... kind of... turn your loops into vector operations... so and to do that... we need to kind of... to make sure that we don't... do too many mistakes this way... so... one thing we will do is... if we have like all this training data... that we have here... we can basically... each of those vectors here... is one column... is each of our training examples... is one input vector... of information... and what we can do is... we can write a matrix... containing all... those input vectors here... so we get like the first input vector... the second input vector... the last input vector... and... write them into one large matrix... which will be then an... N... times M matrix... so N being the number of... input features... how many inputs did we have here... so where... one input feature might be something like... house size... zip code... distance to public transport... number of bedrooms and so on... and this is the first house... the second house and the last house that we have... and... this way we get like one big matrix... with all the input data... and we can do the same thing for our output... in this case... so we can say okay... I have like the first output... the second output and so on... and put that into one big... this is one by M matrix... so we kind of just have one entry... in this direction... but otherwise it's kind of... it's stacked in the very same way than... this vector was... and this way... and... now when we do some calculations... we can do them for... not just for one of the examples... but we can do them for all the examples... that we have at once... because we can kind of just multiply things... with this matrix and this way we can... we avoid doing a loop... over all the training examples... but we can kind of multiply... we later multiply this... this vector... with something else and this way... we have the benefit of... avoiding some kind of loop... over all the training examples... and this will be... doing this consistently... as often as we can... will turn into a lot of performance benefits... and make the difference... between something that... actually works on... actually works... and something that is so slow... that you will never see the results of it... at least... till the term ends... so... if we think... if we say... we want to turn... we say we have some input features here... the question is how do we turn things into input features... so this is kind of... we say we want to have one long vector... of information here... and we want... which we take... as our input... if we start with... the image of our cat here... how can we turn that... into one long feature vector... so if we start with this image here... so I have turned this... into grayscale now... to simplify things... we get 545... 54 by 564... 7 pixels... so that is kind of the dimensions... of this image... so it is... 500... 500... 45 pixels wide... and... 567 pixels high... and each pixel... is a value between 0... and 255... so that is kind of the... you usually reserve one byte... for each pixel... for each color channel... so having... which means we get this number between... 0 and 255 for each pixel... and... so... we can say... we can turn this image... into a vector... which has kind of the pixel values... at each position... so this is the number... so one always has to be careful... with images and matrices... because matrices usually take the row... as the first index... and the column as the next one... and if you talk about images... then most image libraries take the... width as the first index... and the height as the second index... and that is... common source for a lot of bugs... so it is easy to mix those things up... and it is pretty annoying... that we have different kind of... conventions there... but yeah... this way we can turn our image... into a matrix... this way... and making sure that we don't mess up... height and width... otherwise it wouldn't matter too much... in this case because we just get a catch... which is flipped over... ... ... ... and this way we can... we have turned our input into one large matrix of information... ... if we have a color image... we usually have three input channels... so we have like a blue channel... a green channel, a red channel... and each of them has its own matrix... basically... so we get not one matrix... but three matrices... so we can... suddenly our information is kind of... ... what were the numbers here... 5, 6, 7... 5, 6, 7... by... 5, 5, 4... 5, 5, 4... by 3... so we have like a... 3-dimensional object here where we have... a list of matrices... ... and each of them... has one... each entry contains one pixel information... for one of the color channels... ... sometimes we even have... a fourth channel which contains... so called alpha information... which is kind of how... transparent is the image at that pixel... which is for example... I think... GIFs have this information and PNGs... also where you can have like a... transparent image as well... so you get like a transparency channel as well... so you can also have four channels over here... ... ... and the way to turn all this... into like a feature vector is by just... stacking those matrices... so we can just say okay I'll stack all this information... so I'll just say okay I'll take like the first pixel... up here and it ends up here in a long... in a very very very long vector... and I'll just write... this way I write all the values from all the pixels... down into one... very very long vector... and in this case I get... a resulting vector which has... 900... almost a million dimensions... ... but we... which is a lot but we... suddenly we have turned our entire image... into one... into a one dimensional... into a one dimensional object... so into a one long vector... with... with a lot of entries... so every image... a problem here is... every image has different dimensions... so kind of this... by the number of color channels might be... the same for all our input images... the height and the width might be... different for each image... so the resulting vector... might be different as well... as well... so the simplest way to kind of solve this... is use some image modification software... and rescale every image to have the same... width and height... so... and make sure that all of them are... are the same... question is how large should we make this... and the answer to this is usually... just large enough that... you as a human could classify it... so if you can identify what is on the image... then we can assume the algorithm... should be able to do that as well... so if we... turn our cat image... into this size... then that's probably still enough... for you to identify the cat there... so probably for our cat classification... that might be still enough... so... with 64 by 64... and color... we can get 12,288 features... and if we can get away without colors... we can turn it into grayscale... and this way turn it into 4096 features... and... this... this kind of pre-processing... will also be... a big part of what needs to be done... to make... to get actual deep learning systems to work... because figuring out... what is the minimum amount of data... that we can get away with... means if we... scale everything down... everything else will work faster... and probably even better... than if we leave everything... at the highest resolution... but this would also make it more difficult... for the recognition to work, right? like for me as a human... it is harder to... so... the algorithm has... one advantage... it kind of... it sees every pixel in the same size... than otherwise... it is... but it is... it is true that... if you scale it down too much... then it will get harder for the algorithm... and the performance will drop... if you make it too large... the performance will also drop... because for other reasons... we will cover those later... but if you get too many input dimensions... then the algorithm gets also a harder job... at making a proper prediction... and... we will get to see other ways... to work around those problems again... but it is often... trying to find a sweet spot... so you usually... have several constraints... the quality... that you want to achieve... at the end is one of those... but also the compute power that you can invest in there... and the time you have... and the number of input images that you can use... for training... and all those kind of determine... what kind of size you can get away with here... so if you have too little images... you also need to scale it down... because otherwise you will run into problems... that are called overfitting... and then your algorithm won't work well... and it is usually... a very fine trade off... and there is no... silver bullet there... so you will need to do experiments... and see if I scale it down... even further... does it improve or does it get worse... or if I scale it up a little bit... does it get better or does it get worse... and... it highly depends on the application... of what is the right approach here... but yeah... making it smaller will make it more difficult... at some point... but it will kind of resolve other issues... that you could have... and so making it smaller will help you up to a certain point... and then it gets worse again... I think I will stop here for today... so we can... cover quite a bit of basics... how to... turn data into vectors... and everything we will do... will be... we will process vectors here... so any kind of deep learning algorithm... sees ever... is a vector of inputs... or maybe several vectors of inputs... but everything will be numbers... so one of the things... we will always need to do is... figure out how to turn things into numbers... and... which... has a lot of... interesting... facets as well... for images it is almost easy... but for texts for example... and words... there is also pretty interesting answers... of how we can turn words into numbers... so that an algorithm... can work well with those... do you have any more questions... till now? yeah? will we have a written exam at the end? it will be a written exam... at the end... so... I also upload the... first exercise sheet... today into OLAT... so there will be a very... first exercise sheet which is... only introduction into NumPy... and Python and so on... so nothing real deep learning... so far... so we will start with those... with the next exercise sheet next week... and will the exam be... done on the computer? no it will be written... the exam will not have... any parts where you need to write code... there might be parts... where there is some code... and you need to identify what is wrong with it... or something like that... but you don't... will not be required to write code... in the exam... in some way... I realize that... it is not the ideal form... of examination for a course like this... because what I want to teach you... are practical skills... I am hoping you go away from this... and are able to program... your own neural networks... and create your own deep learning systems... but... a written exam can only cover... so much of that skill... I am trying to... make those things match... but the exam is... only a poor representation of... what I want you to learn here... so... I am going to... ask you to... to bring your laptop... for the exercises... for example... something you can do is... try to follow along... when we do... the exercises... for the Friday... part of the course... it is a good idea to bring your laptop... for the Wednesday part... it does not really matter... because that will be... more me... showing you something... but for the exercises... it is probably a good idea... if you have the ability to follow along... even just typing something... even if you are... just typing something off... and trying it on your own... it is sometimes helpful... to figure out how things work... ok... any more questions? then... see you next Friday... Okay, last week we talked about machine learning systems and that one of the first things we need to do is we need to make sure that we can feed those with data in form of simple vectors. There are several ideas how we can turn stuff into vectors. For example, if we say we have some kind of color image, we can interpret that as having three matrices, each having the same size and dimensions, and one matrix is for each of the color channels. We have three channels. We have three colors. Each of them is an M by N matrix. So in total, we have some object that is M by N by three. And this object can be turned into one long vector by just stacking those matrices. So we can kind of, in NumPy, this would be taking this M by N by three object and just call reshape on that and just turn everything into one long vector. And this way we kind of get one vector with all the input data for this one image. So later when we talk more about images, we will see other ways how to properly deal with picture data. But for now, this is kind of the way how we can do that. So I think that's it. Thank you. So now we can turn those images into vectors to work with. So in this case, this would mean we get a vector of almost a million dimensions because we have like three by 567 by 554, in this case, and one thing to reduce this number is to make sure that it's not too much. We can also do some more with the vector. We can do some more with the vector. make sure that we rescale the image. So one thing is that each of the images has like different dimensions. So these numbers change from image to image. So this is kind of a problem. And another problem is that we might want to make sure that we don't have to deal with a million dimensions over here. So one thing we want to do with images is often to rescale them. And like a rule of thumb is to make sure that we rescale it so that it's just large enough that we can classify it. So in this case, for example, a 64 by 64 image might do the trick. So it's big enough that we can still see the cat inside. So classifying a cat should still work. So for the final algorithm, something like 64 by 64 might be the right number. And if we start experimenting at this point, we can then start experimenting and see, okay, maybe it's, maybe it should be 128 by 128, or maybe it should be 32 by 32. And we can kind of start experimenting from here and see if the performance of the algorithm will improve in any of those directions. On this, again, later. So probably we can also remove the color channel and just make everything grayscale. So that also kind of reduces the dimension we have over here. So this is kind of, make, make, turning, turning, pre-processing our input. So we don't kind of process the input just for the sake of turning everything into a vector. We want, what we wanted to have in the end is make some kind of prediction on the input data. So, and like one thing we were to start with, we want to start with a classification task. So, um, um, um, that was unfortunate. All the, so, um, if we want to do classification, um, what does that mean? So, what it means is we want, we have like a binary output. In our case, the picture is a cat or it's not a cat. And if we have a binary output, um, one thing that we want to predict is, um, the chance that the image is actually a cat or not. So, what we want to predict is, so, so ultimately we want to predict it is a cat, yes or no. So we have a binary output, but the, we have an intermediate goal here. We want to predict the probability that the image is the target class. So in this case, a cat or it's not a cat given the input that we have. So this is kind of the probability notation. So, it's, uh, uh, so, uh, uh, probably, uh, uh, you've seen that in, in some kind of statistics class beforehand. So we say, okay, probability of some random variable given another random variable. So it's, uh, it tells us, given, we already observed something of the world. We want to give, uh, give, give the probability of this random variable. If we would remove this one, we would get the probability of this random variable. So, uh, so, uh, so, uh, so, uh, uh, so, uh, uh, uh, uh, uh, uh, uh, give the probability of this random variable. If we would remove this one, it would basically say, what is the probability that any kind of image is an image of a cat? So that would be kind of, we take how many images are there in the world, and how many of those are images of cats. So this would be kind of the probability that some random image is an image of a cat. That is not that interesting. So in our case, we always deal with those conditional probabilities that we say, we have some information, about the world. In this case, the features that we observe, and we want to know, given the features that we have seen, what do we think is the probability of this random variable, which is the class that we want to predict at the end. So, and what we want to build is a machine that will give us or approximate this probability over here. We want to know what is the probability of, uh, uh, what, what is this probability over here? And this probability interpretation has some huge advantages in a lot of cases. So in the case with the cat image, you might say, okay, this is not something that has something to do with probabilities, because it's either a cat or it's not a cat. So there's no, no probability in here. But in other cases, there is actual probability. So if you, if you think about, for example, I want to classify, um, I have a patient, and I know the patient's blood pressure, age, um, uh, uh, some, uh, some, some, uh, cholesterol level, and so on. And what I want to predict is, does the, will this person have, uh, bad COVID-19, uh, outcome or not? So, and in this case, even given the very same input features, two patients with the same blood pressure, same age, same cholesterol level, and so on, one might have a good outcome, the other might have a bad outcome. So you can, so even, even if all the input features are exactly the same, one patient might, might, might be good. The other might, uh, uh, uh, the, uh, the other might have a bad outcome. And, um, this, this means there's a lot of tasks where only having those input features might not be enough to completely distinguish between the, those classes. And in this case, we, you are actually, you, you actually have a probabilistic outcome. So you might say something like, given this blood pressure, this age, and so on, you have a 90% chance that you might, that, that, that you might, you will be fine, uh, given, given your COVID-19 infection. So you kind of have a probab, uh, so in 90% of the patients with exactly this, uh, uh, uh, who look exactly the same will have this, this outcome over here. So in a lot, in several cases, the probabilistic interpretation here is the only, uh, the only thing that makes sense because you can, uh, you might not have, uh, uh, enough information to really know if somebody will have a good or a bad outcome. And the information that you have might only give you like a statistical information. And that's why we, when dealing with machine learning systems, we always do, uh, work with this, uh, statistical information and always say, okay, what we want to predict is a probability. It's a probability that there is a cat in the image given those are the pixel values. And a good classifier should have a very high confidence over here. So given some, uh, some, something here, the, uh, a pretty good classifier should give you a very, very high probability over here. If it, because it's kind of, uh, the, the task is something where if there is a cat in there, it should give you a pretty high probability, but it will, uh, what we will get is always something that, uh, the confidence of the classifier. If it gives you like a 50, uh, like 50% chance, it means the classifier is pretty unsure and it doesn't really know it. If it has seen a cat, it might be something. There's a, uh, a very famous pictures of where it's hard to distinguish if there's a muffin or a dog in the image. So it might be really hard to, to, to, to, to, to distinguish it. And this probability that we want to predict over here will be, will be, will be, will be, will be, will be, will be, will be, will be, will be, will be, will be, will be. We'll kind of give us the, the, the, the, the level of confidence that our algorithm has in the prediction it makes. So that's what we want to have. We want to predict this probability that we, that there is a certain class given those input features. So, and, so that's what we want to have. How do we make, how, how can we make sure that this is what the, you know, what the, the, the, the, the, the, the, the, the, the, the, the, the, the algorithm will predict. We do that by saying, so at least for as long as we start with logistic regression and the logistic regression formula for this prediction is, we take a linear translation of our input features. So we multiply each of the features with some weight, add some bias. So this is kind of, this is like one value. If we have 4,000 input features, this would be 4,000 values. So it's a vector that has exactly the same length as our input features. And we take some kind of function of this. And this function is called the sigmoid. So the sigmoid function is defined as 1 over 1 plus e to the power of minus whatever we put in here. And so this is hard to, to, this is kind of the formula. And the way this, we'll see in a bit how this looks like. So we have this sigmoid function of w, these are the learned parameters. And as I said, the learned parameters consist of a bias term b, so which is just one number, and a weight vector, which has the same dimensionality, and a weight vector, and a weight vector, which has the same dimension as our input features. So, and all this together, so this thing together, we call hypothesis. Hypothesis. So this thing is the hypothesis that we want to learn. So it's kind of, we want to learn a certain function. This function is parameterized by w and b. So there's like two free parameters, a vector, and this bias term. And depending on, if we change those values, our prediction will also change. So we can modify those values and get different predictions and every kind of choice that we can take for this vector and this bias term over here will give us a different hypothesis. So that's the building blocks that we need for logistic regression. How does this sigmoid function work? So this is a function over here. So what is this function? This function is basically something that maps every input to a number between zero and one. So if we put in a very small number over here, so that means this number gets very big. If this number gets very big, this number gets very big. So everything in the denominator down here gets very large. And if that gets very large, we get in total a very small number. So it approaches zero. It approaches zero over here. And the other way around, if we get an incredibly large number over here, then this number gets close to zero. If it gets close to zero, the denominator gets close to one. And if it's close to one, we get one over one. So in this direction, the whole thing approaches one. So the function maps kind of any value over here to something between zero and one. And that is kind of a nice property. Because that is exactly what we want to have if we want to predict a probability. If we want to say the output should be some probability, then that should be a number between zero and one. So the probability over here, that should be a number between zero and one. Having like a probability of more than 100% doesn't make sense. And the probability of less than zero doesn't make sense either. So making sure that what this hypothesis predicts is always something between zero and one. It's kind of, you know, it's a good thing. It's kind of nice. So that means no matter what values we choose over here, the output will always be a valid probability. So that's already kind of nice. So given this, we need, the next thing is that we need to define is how good is the hypothesis. So if we can choose any kind of value over here, we can choose any W, any B, and depending on the choice we make over here, we get different probabilities over here. And if we get different probabilities over here, we get different predictions. For if it's a cat or if it's not a cat. And we have to kind of evaluate how well the prediction is that we make. So given any kind of data that we have, we need to define if we make a good prediction or not. So if we say we have one data point, so one example, one input image, and the target class, so the information if it's a cat or not, what we want to have is that the prediction that we have should be close to the actual class. So the actual class will either be a one or a zero. What we output here would be, can be any number between one and zero. So it could be either a very large number, so very large probability, or a very small probability. And what we want to have is that this prediction should be pretty close to the actual value. So that's what we want to have. So for logistic regression, there is a very concrete loss function that we always use. And this loss function, this function is defined like this, and this is called the logistic loss. And so let's look at what this does. So our target class, y, is either zero or it is one. If it's either zero or one, if it's zero, this means this part vanishes over here. If it's one, it means, one. If it's zero, this means this part vanishes over here. Because this part thing becomes zero. So it means either we have this part or we have this part of the loss function, depending on the value of y. So if we say y is equal to one, and this part over here vanishes, and it means we take our loss function will be the logarithm of our prediction. So what is if we get a large prediction? So how does the log of any function look like? So I haven't made a plot of this. Would have been nice if I had some internet connection, right, but I don't have internet connection. So I don't have internet connection. So I don't have internet connection. So I don't have internet connection. So I don't have internet connection. Why not get herramients, right, instead of only doing it using fried things? Ohh, really? Don't need the internet connection, right? So this isn't Text2000. This is the node you need to stand. Yes. It's the quiz that I actually wanted to ask you for. So cause here was a game, let's pass myAsian code as blank, or default, and I'm gonna use, and then let me writeEu  Brand now, Because num is like that, front, Now is the last one. So, some plot of the logarithm. So, the logarithm is some function that getting closer to zero, it approaches minus infinity, and then levels off the larger the input gets. So, if I take this one and I have one over here, and if my y hat, so what I predict, is very close to one, that means if it's close to one, then the logarithm of my prediction will also be very, very close to zero. So, it approaches zero. So, my loss. If my target class is one, and my prediction is very close to one, then the loss over here, this number, will be very close to zero. So, the loss function that I have for this example will be close to zero. If, on the other hand, my prediction is off, and I have some number that is very close to zero, that is very, very small, and that means my logarithm over here is very, very, very close to minus infinity. So, it gets closer to minus infinity to the smaller this number over here gets. And that means, so it gets, the logarithm over here will be some very small, very negative number. I have a minus over here, so this entire thing will be a large positive number. So, the more off I'm here, the larger my loss. So, the more the loss function over here gets. And I have the same thing the other way around. If I have a very small, if my target class is zero, then this thing gets cancelled out, because it's zero over here. And I have one over here, so what I'm looking at is the logarithm of one minus my prediction. And if my prediction is also close to zero, then the logarithm over here will be zero. So, the output will be close to one, and that means it gets, the output will be close to zero again. And if my prediction is far off, then the logarithm over here gets more and more negative, and my loss function increases again. So, that's kind of makes, it makes intuitive sense that this thing penalizes whenever our prediction is on the other side than what, what, what we're looking at. So, if we have some number, some target, and our prediction is kind of on the other side, then this loss function over here increases. So, and that's exactly what we want. We want to have a function that is larger the more wrong we are. So, the more wrong our predictor is, the larger this number should be. So, for linear regression, so there's another form of machine learning task where we don't want to learn a binary target. So, if we want to learn some kind of real valued number, then, so if we have like our target is some kind of number that is just any kind of number, and our prediction is not supposed to be some probability, but we want to predict exactly that value, then what we often take as a loss function is the loss function over here. So, what we often take as a loss function is the square of the difference of those. So, we take like the difference between our prediction and the actual value and see how far off we are and square that. So, that means the further away we are, the more it gets penalized. And that would be the loss function for linear regression. We won't, we will not. We will not be doing much linear regression tasks in this vector because most things we want to do are more or less binary. So, we usually want to predict something that is some kind of classification task where we have like a binary output that we want to predict. But just so you have heard it, depending on the loss function, on what we want to predict, we might want to choose a different loss function. And for classification, this logistic loss over here, is kind of the standard thing to do. And using this loss function has another advantage. And that is, this loss over here, which is also called the cross entropy loss, . Using this loss function over here makes sure that what we have here, the numbers we predict here, can actually be interpreted as probabilities over here. So, it makes sure that the numbers we generate are calibrated as proper probabilities. Again, I won't go into the statistical details, but it's not like this number is... So, if you look at this formula, it turns out that it's something where being wrong is penalized and being right is not penalized. So, it looks like it does the right thing. But the formula doesn't just fall from the heavens. But it's a number that makes sure that in the end, we will predict numbers for which this property will be valid. And this property over here holds, so that we can actually interpret those numbers as probabilities later on. So, long story short, this is the loss function for a single training example. So, we have one data point. If we have a lot of data points, we look at this cost function. So, we look at the cost function across our entire training data set. So, we take all the training data that we have, sum up the loss function for each of them, and we divide by the average over how many data points we have. So, m is equal to the size of the train. And this gives us the so-called training loss, the loss over the entire training set. This number over here is a little bit arbitrary. It's just a constant factor. It makes sure that if we take twice the amount of data, then the training loss will still stay the same. But it's a number that later on we can also drop, and it doesn't affect anything. But the main point is we want to take the average of the losses of all the training examples over here. So, having defined all those things, the real question is how do we get these parameters w and b? So, that's when we defined our hypothesis at the beginning. We said that the hypothesis kind of depends on those two parameters over here. And we said, okay, the hypothesis is sigmoid of w, the dot product between those two vectors, those two vectors plus b. So, having those, what we want to get in the end is we want to know these parameters. Because if we know them, then we know the hypothesis, and then we can make predictions about new data points. So, as soon as we have them, those we are happy. And the way to get them is we want to find w and b such that our training loss gets minimal. So, we want to minimize this function over here. So, we want to minimize the training loss, the loss over all the training data, and we want to choose those parameters such that this loss over here gets minimal. And when we do logistic regression, there's several ways how to calculate those values. So, there's several ways how one could decide on those. For linear regression, there's even analytical ways how to just calculate them, given the training data that we have. In our case, we want to use some technique that will always work, even for something that is not logistic regression, and that will later keep working when we do larger neural networks. And that is gradient descent. So, if we think about this function here, j, the loss function is a function that depends on the parameters we put in here. So, if we change those parameters, if we change w and b, then we change our hypothesis, and if we change the hypothesis, we change the training loss. So, the training loss is mainly a function that depends on the parameters w and b. And if we want to minimize it, we look at the point where this function gets minimal, and if we think about, for example, if this would be w and this would be b, so in just two dimensions, then this would be our training loss, and what we kind of look for is, we look at the surface of our training loss, and whenever, if we start with some values w, for w and b, we want to say, okay, let's see if there is a direction in which the training loss decreases, and then we want to make a step into the direction in which the training loss decreases, and see, okay, we get new values for w and b, and then we want to look again in which direction does the training loss decrease now, and then we do another small step into that direction, until we find a point where the training loss does not decrease any further. And this technique will keep working even for more complicated hypotheses. So in this case, our hypothesis is pretty simple, but the gradient descent approach, where we say, okay, always do a very small step into the direction in which the training loss decreases, will hold for many, many other hypotheses later on. So, what is the direction in which the hypothesis, the training loss decreases the most? The direction in which something decreases the most is the gradient with respect to the parameters. So, what we want, gradient descent means we will repeatedly do an update step where we take our parameters and subtract from those parameters the gradient of our training loss at the current point in the direction of the parameters, and we multiply this with a learning rate. The learning rate is some small number, so that we don't overshoot our target. So, if we choose the learning rate too large, then we will just, for example, in this point, we might make a step that is too large into this direction, and then we get off at a point where we are worse than we were before. So, kind of the learning rate is something that makes the algorithm more stable. More on this again later. Let's look at this thing first. So, what is the gradient? The gradient is the generalization of the derivative. So, if you remember your analysis classes from back in the day, then probably you still remember the gradient in some way. So, if you have the derivative of some function, then, so, if you have f of x, is equal to x to the square, then you might remember that the derivative of it might, will be 2x. And this works all nicely as long as you have only one input variable. If you have multiple input variables, and in our case, we have like potentially a few thousand input variables, you need to generalize this derivative. And what we want, we'll do is we take the partial derivative into the direction of each of those input variables. And for each of the input variables, we get like one entry in the gradient. That gives us the, how much the function changes in that input direction. To make this more concrete, I'll take an example. So, I have a function that has two inputs and one output, and the input and the function itself will be x1 times x2 squared. So, the gradient will be, I first take the derivative in the direction of x1. If I take the derivative of, in the direction of x1, this is a constant term. So, what I get is this part vanishes. So, it's, it will be x2 squared. If I take the direction in the, the derivative in the direction of x2, this is a constant term in this case. So, the derivative will be 2x2 times this term over here. So, it will be 2x2. So, it will be 2x1x2. So, the derivative is kind of this vector over here that for in each dimension tells us how much the function changes if we make a small step in the corresponding input dimension. So, for example, if I'm at the point 3, 2, it tells me if I do a small step in the first dimension. So, if I, for example, look at f3.1, 2, then the output will change roughly by 0.4. That's kind of what the, what the derivative tells me. So, if I make a step 0. of in size of 0.1 into this direction of the first dimension, then my function will increase by, 0.1 times 4 roughly. So, and the same way, if I look at f3, 2.1, then I know that my, my function will change roughly by 1.2. So, that's, that's kind of what the derivative tells us. So, putting all this together, together, so for each dimension, the gradient tells us the slope in that direction is the same as the slope in the first dimension. So, if I look at f3, 2.1, then I know that my function will change roughly by 1.2. So, that tells us the slope in that direction. So, what we could also say, okay, I have like a multidimensional, a multidimensional function. So, it looks something could be several directions. So, I'm very bad at drawing some, something multidimensional. And the gradient kind of tells us if I take like a cutout of this, this multidimensional function, in one direction, then it tells us the slope and, on this plane that we, that we are focusing on at the moment. So, for each dimension, it tells us how much the function changes if we do a little step into that direction. The gradient in total is the direction in which the function increases the most. So, if I'm at this point, then this is a vector in the direction in which the function increases the most. So, if I want to increase it, so, if I'm, my point is 3, 3, 2, then I want to make a step into the direction 4, 12. So, it's probably a small step, so, times 0.01 to increase the function value. So, if, that it increases the most is kind of the reason why when back here, we have this minus sign over here, so this is the direction in which the function, our loss function increases the most. So, we do a small step into the, in the opposite direction, so that we have like the, this is the direction in which it increases the most, and minus the gradient is conversely the direction in which it decreases the most, and as we want to decrease the, the loss function at the end, we do a small step in the opposite direction of the gradient. So, this is kind of, that's the motivation why we do, at each iteration, we do a small step into the, opposite direction of the gradient, because that is the direction in which the function decreases the most. Another example, so, if we, we, the entire gradient is, the gradient into, in each input dimension, so we look at each of the inputs of our function and, for each input here, we get an entry in the gradient. We can also take the gradient, just for certain inputs. So, we can say, okay, I have like a function with a lot of parameters, and I only take the gradient for a few of them, and for each of the, the directions that I take over here, I get a parameter over here that will come in handy later, that we can take the gradient for just the subset of the parameters, and this is kind of a selection of the entire gradient, so we just select a few parameters, a few parts of those, kind of like, like in Python, that we take slices of lists. So, let's make the example a little more concrete, so let's assume we have like this loss function over here, so which would be squared loss, like in linear regression, so we take, have one parameter, and the parameter, minus two squared, will be the loss at the end, so if we say, so if we start, so, what is the partial derivative, so we, in this case, we only have one direct, one dimension, so, like we only have to look at, this derivative over here, so the derivative of the whole thing will be two, times W minus two, so it's like inner derivative times outer derivative, the inner is just one, so it's, we are left with this part of the derivative, and let's start at point W equals zero, so let's, if we start over here, the function value that we have will be four, so zero minus two squared is four, and the derivative is minus four, so if we put in zero over here, we have two times minus two will be minus four, and minus four means, the slope of the function of four, of the function of our loss function over here, is minus four, so the slope over here, of the tangent at this point is minus four, the derivatives, points in this direction, minus four, it points into this direction, it tells us to, if we want to increase the loss function, we have to go this way, so what we will do is, we go the other way around, and say we could make a small step, so learning rate, small step, into the other direction, going this way, and this way we will turn out, get a new point over here, get a new slope over here, make a new update, and this way slowly get closer to, the point where, our derivative gets zero, and we have like the smallest value, for our loss function over here, so in our case, the full update step that we have, is we do the same thing, for all the parameters that we have, so we have like, in our case, we have like two variables, one is a vector, the other is just a single value, and what we basically do is, for each of the parameters, we make a small step into the direction of the, into the opposite direction of the derivative, with respect to those parameters, so that's the, that's where the notation comes in handy, that we can just select a few, a subset of the parameters over here, if we get more parameters, we will kind of do the same thing, for other parameters, so it will be always the same thing, that we do for each of the parameters, we will always keep doing this, that we do, look at the derivative in the direction, of those parameters, and do a small step into the opposite direction, so one thing we haven't talked about a lot yet, this learning rate eta over here, the learning rate, this determines how stable, our, the convergence of this algorithm is, so if we, for example, take a very large learning rate, so if this number is very high, we might make a long, a big step into this direction, and might end up, at a point, where we are further away, from the optimum over here, than we were before, so having a large learning rate, means we might make, we will do larger steps, and we might reach the optimum faster, but we might also have points, where we run away from the optimum, and making everything less stable, if we use a very small, learning rate, convergence will be more stable, so we usually, we will usually not overshoot the optimum, but it might make everything slower, so we will need more iterations, and it's sometimes, there is no golden bullet, for this number over here, so there's not, the dependence of, whenever you train some kind of, machine learning algorithm, the learning rate will usually be something, that you need to calibrate, for your use case, so it's usually something, that you start with 0.01, or 0.001, some number like this, and then you will see, okay, it doesn't converge at all, so you need to make this number smaller, or that you realize, okay, it's going to small, so you start to increase the number over here, and you'll have to do a little bit, of fine tuning and calibration, to get a number here, that, that gives you a good, good training progress, without your algorithm getting unstable, all in all I would say, it's better to have a learning rate, which is slightly too small, so you shouldn't do it, shouldn't make it incredibly small, so because then you, it will take ages, for your training algorithm to converge, but if you, but it's usually better to be, a little bit on the safe side, and use a smaller number, over here, so that, you don't waste a lot of time, waiting, and, just to see that, at the end, everything works out, but at the very end, stuff starts to diverge, so it's better to pay with, a little bit more waiting time, and having a smaller, smaller learning rate over here, so that you have a little bit more stability, in training, so now, now that we have those points, we can start to, to, to, now that we have those parts, so we, have kind of the basic, setup for the algorithm, we need to determine this formula over here, so the, this gradient over here, we know that, it's, it's defined as being the partial derivatives, in each of the directions, that we have as inputs, but we now need to, determine how we can compute this derivative, over here, so we need to, in, in this case, was easy, we kind of have the analytical formula, for the derivative, and can say, okay, this is, depending on, where we are at the moment, we can determine which, what the derivative is, at this point, and this way, determine the direction that we want to go, but in general, this might be, a pretty complicated function over here, so in our case, that we, in the logistic regression case, we already have that, we take, this, cross entropy loss, so we have, like for one single training example, we have, y times, logarithm of, so and, here we have the sigma, sigma of, so the sigmoid function of, w transpose x, plus b, and so this would be like, what we predict, and then again, plus one minus y, times, the logarithm of, one minus, the sigmoid of, w, w transpose x, plus b, and so on, so, and this, and this is just for logistic regression, where this function is still a very simple one, so, having, putting all this together, for this, for this thing, we can still calculate the derivative, but we can also, we can also, we can also, we can also, we can also, we can also, we can also, calculate the derivative by hand, and get kind of a good, a nice formula for this, but if, if we start making this, number in here, so our, if we start making the prediction over here, more complicated, we need some automatic way, to do the different, differentiation over here, and, the way to do this is, we define all the computations, that we, do here, in the form of a compute graph, what is the compute graph? A compute graph is a directed, acyclic graph, where every node, represents a mathematical operation, so, for example, a node could be something like, addition, where we say, okay, we have two inputs, A and B, one output, C, and, for each of the incoming, parts, we know the derivative, derivative in this direction, so we know the derivative, in the direction of A, which for addition, is just one, and the derivative, in the direction of B, which is also just one, so, the derivative, in those directions, just as a reminder, it's kind of the rate of change, in that direction, and one can kind of, imagine it this way, so if I have like, A plus B, and I add a little epsilon, in this direction, in the direction of A, then, C will also be, one epsilon, and C will be, one epsilon, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and E, so on, I willrotate A, and so on, and so, I'm going to, mistake the derivative, for the other Won, I'm going to exchange Ah, okay, the time as, for any point. No! So, we have we've got this space, and imagine, as I say, that it's C, that this plan is C, that this this is this Right, but if I see this as Y, that this S, this  There we go, tantamounts, direction of A is B and the derivative in the direction of B is A. So I have like my operation is A times B and if I change A a little bit then the output will change by a little b and if I change b by some small number then the output will change by A times the small number. So that's kind of the derivatives into those directions and multiplication is kind of one of the main operations for which we will need the derivative. So it's kind of nice to repeat again that kind of in the direction of A the derivative is just B and in the direction of B the derivative is just A. So it's always like the value on the other side. That determines the derivative on the other side. So we can now do that for all the mathematical building blocks that we might need. Subtraction, so if we like to do A minus B we have like minus one for the derivative of B. If we square things up we can define the derivative of that function. So it's kind of it's a single input function so we have just one input and it's like two inputs. So two A will be the derivative if we square this for A squared. If we have some function like rectified linear which is a function that is linear as long as the input is positive and after that it's just zero. So if we have a function like this the derivative will be one if the input is bigger than zero and zero otherwise. So this thing here is notation for an indicated one so if it will be one as long as this condition is true and otherwise it will be zero. So it's one if A is greater than zero and otherwise the derivative over here will just be zero. We can take the if we have for example a maximum function. So it takes the derivative of A and then it's just zero. So we can take the if we have for example a maximum function. So it takes the derivative over here. So it's one if A is greater than zero and otherwise it will be zero. It's something that takes the maximum of A and B. We again can take the derivatives over here. So we have like in the direction of A if A is bigger than B then the derivative is one in the direction of A. So increasing A makes the maximum larger as long as A already is the maximum. If it's not the maximum then the derivative here will be zero and nothing changes if I change A. And the same goes for the input B. If B is larger than A then B is already the maximum and if I change B then also the maximum changes. But if B is smaller than A then the derivative here will be zero because changing B doesn't affect the maximum. And we can do that for all kinds of primitive mathematical operations and from those we can create complex functions. And we can do that for all kinds of primitive mathematical operations and from those we can create complex functions. So I can like create a compute graph where I say for example I have a function that is A times B and the result will be squared. And if I do that I can say okay this will be this compute graph over here. I multiply B and A get an intermediate results that I call C. Then I will take C squared and get a result which is D. And how do I calculate the derivative over here? And the way to do that is, so I want to calculate the derivative of the final output. So I want to calculate the derivative of my entire function of D into the direction of A. And the chain rule basically says, the derivative of D into the direction of A is equal to the derivative of D into the direction of C, times the derivative of C into the direction of A. So it kind of, so as in, to make it easy to remember, so if you could like just cancel those DCs out, then you would get D D, divided by D A, so it kind of, it looks as if you can just cancel those out, so it's kind of, helps to remember here. So what we basically do is, we have a product over here. We have a product of two smaller derivatives. We have like the large derivative over here that we want to have, and we divide it into a product of two smaller derivatives, where we make a smaller, where we divide it in two smaller stems. So this is like a big step from here, till here. And we divide it into two parts where we have the derivative of here, till here. And then we do a step from here till here. So we divide it into two small steps. And for each of those smaller steps, we basically have the derivative written onto this art over here, so we have like this art where the, in the compute graph, and for each of those smaller steps, we heard this arc over here, so we have instead this arc over here. So we have like this arc in the compute graph, where the, into the coming up ocks in der policies entHIme, till hier ainda eigkeitigeni each of those mathematical operations, we kind of know, we know the derivative. So the derivative of from here to here, is we wrote over here, so it will be two C. So if we have C squared, the derivative from D to C will be two C. So let's write this over here. And so we know this one. And if we look at this one, we can say, okay, the derivative from C to A is just what we wrote over here. So it will be just B over here. So now we have kind of, the chain rule tells us that this will be the derivative from D into the direction of A. And C is basically just the forward computation from going forward over here so it's basically we can replace C by A times B. So we get this formula over here. And if we kind of multiply this out, then we get that the whole derivative will be two times A times B squared. Which again, if you would like, just go from the calculus way of deriving this derivative, you would take the inner derivative times the outer derivative and put derive A times B. Two times A B squared as the derivative in the direction of A for what we have here. So what we did over here kind of works. And the nice thing about this is, the algorithm that we used over here, that will work for arbitrarily large compute graphs. So no matter how many nodes you have over here, you can just put the derivative over here. You can kind of always do this calculation over here that you split up the entire gradient into all the small steps that you have in between. At each step, you know the derivative of this small step over here. And at the end, you need to calculate all those small derivatives and just multiply all of them. And this way you get the entire derivative all the way up. So kind of putting all this together, you can do this. So putting all this together gives us the back propagation algorithm. And the algorithm is that for each of the compute nodes, we define two values, the forward value and the backward value. The forward value is just what you would do if, yeah, you calculate the result of the compute graph. You put in some value here, some value here, and just do all the computations through the graph up to the last of, to the final result. So that's, those are the forward values. And the backward values will be the gradients coming from the final value and going downwards to, towards the input values again. So you kind of start multiplying up those gradients from the intermediate nodes. So we kind of start calculating. So we kind of start calculating. So we kind of start calculating. At the least, to get the forward values. And then we start at the root node of the compute graph and calculate those gradients backwards till we reach the leaf nodes again. So let's do an example for this. If we have a logistic regression, then we say our loss function is this one over here for a single example. So if we have like one single example, we say the loss, this will be the loss we have. Y hat is defined as the sigmoid of some value z, where this is the sigmoid function. And z is defined as some linear operation where we have like a weight for the first input variable, a weight for the second input variable, and some bias term b. So these three are the values that we have. And these three are the input parameters. So if we have those building blocks, we need kind of, it's a nice thing to say, to define one node so we can, could say okay these are like several mathematical nodes for several operations, but as this is kind of used a lot, it makes sense to kind of define the derivative of this function, of the entire sigmoid function. And so we can use that as a single node in our compute graph and say okay, if we have some value z, that's the input of our sigmoid function. And the derivative of the sigmoid function is the sigmoid of z times one minus the sigmoid of z. So if you want, if you feel like it, you can try to validate that by calculating it by hand, but for now I'll just give you this, this, this, this, this, result so if i have take the derivative of this uh kind of get get i i i get get uh like this this value for the for the derivative and the the the this value is just that number here so it's kind of the sigmoid of that times one minus the sigma of that which is kind of it would be this number so if i if you if you remember that y hat so our prediction is just the sigmoid of z so the derivative will be the prediction that we made times one minus the prediction that we make so that's that that's that's the derivative term over here if we look at our loss function so the we uh the loss function was this number over here and again we will create one compute node for this for this function over here so again as this is kind of used a lot it makes sense to kind of uh do those calculations instead of like do it splitting it up into smaller smaller nodes and the derivative of the cross entropy loss so the cross entropy loss which is this number it turns out to be this number over here and then the derivative of the cross entropy loss which is this number it turns out to be this number over here which is minus what the number that we wanted to predict divided by what we have predicted plus one minus the number that we the the actual class divided by one minus our predicted class so um which is the derivative into the direction of y hat this is our n time constant okay im this is ourNusiM , we will take that why don't i write the derivative into the direction of y over here that is because we never take the derivative into this direction why do don't we take the derivative into this direction because we don't care we don't we cannot change this value over here that state that's just a data point we can change what we predict over here we can make a change in what we do predict we can never change what the data was so we can do the change this value instead of giving a variable what we see over here so that will be same equation two as i you've got the time. change that obviously if we change the data points, but we cannot, our algorithm takes those values over here as its ground truth and it only observes them and uses those to kind of calculate the loss function over here. But we don't need to take the derivative into this direction because we cannot change those values. So we only can change the parameters that we have downstream this way. So we don't need the derivative into this direction. We only need to go down this way when calculating derivatives because over here there's no parameter that we can change. So let's go further in our example. Let's assume that we have a data point where x is minus two and three. So that is our input features x. y is equal to one and that is, this is kind of the class that we would have liked to predict for this input example. And let's assume the weights that we currently have are two and one. And there should be some value for b as well because b is kind of also something that we need later on. So I really have to work on the formatting of those images. So I'm going to do this here. I'll make this full screen. No. So this is our compute graph. We multiply w1 with x1, multiply w2 with x2, add those values, then add the parameter b. The result will be b. And then we put into the sigmoid function. And the result of that will be compared with the target class for our loss function. So in this case, we only have like one data point. In general, we would have, again, a sum of those loss functions. So we would sum up a lot of loss functions for a lot of data points. But for now, we just skip that part. So it would just mean one more addition up here. So this is the compute graph that we have. And these are the derivatives at each of the branch into each of those directions through the graph. So let's put in values over here. So if we have those input values over here, and b is 0.5 over here. So if we... put in w1 as 2, x1 as minus 2. So w1 as 2, x1 as minus 2. And again, w2 is 1, x2 is 3, b is 0.5, and the target class that we want to predict is 1. So these are kind of the given values that we have at this moment that we want to calculate the gradient into this direction, this direction, and this direction. So these are the values that we want to change at some point, at the end. So we want to adjust them, so we need the gradient into those directions. What do we do? We start with the forward step. So we multiply 2 by minus 2 and get minus 4. 1 by 3 is 3. Minus 4 plus 3 is minus 1. Minus 1 plus 0.5 is minus 0.5. And the sigmoid of this turns out to be 0.378. So at this point we stop having round numbers, but that's just what... If we have a number that is slightly smaller than 0, we also will get a probability that is slightly smaller than 0.5, if you remember the sigmoid function. So if we have... Like 0 was here, and this would be 0.5. So if we go slightly this direction, we will have a probability that will be slightly below 50%. So this is kind of the probability that we predicted. And our loss function will now be the cross entropy between this and this one. So again, this will not be a round number, but it will be something that is quite a bit larger than 0, because we are kind of off from this. So we predicted something smaller than 50%, even though the actual class is 1. So this should be a sufficiently large number to reflect this. And the smaller we get over here, the larger this number should be. And what we need to do now is calculate the derivatives backward. And how do we do that? We kind of just fill in those formulas over here. So we go back through each of those arcs over here. So we know that for this formula, we need the values of y and y hat. So we have all them. We computed them in the forward step. So we kind of fill in those numbers over here and get this value over here as the derivative of the loss function into the direction of y hat. So this is the derivative into the direction of our prediction. And that basically says us, tells us that if we decrease the prediction, our loss will increase. So it makes sense. If we make our prediction even smaller, then our loss will be even larger. So it also tells us we should increase the value that we predicted to get a smaller loss at the end. So if we could directly control the prediction, it would tell us, please decrease the prediction to get a smaller loss at the end. So we can use this predicted value to decrease the loss that we had at the end. Makes sense so far. Next step, we calculate the derivative into this direction. So to get the derivative of our predicted value into the direction of the value that we put into the sigmoid function, which we call z over here. This will be, so we put in 0.378 into this formula over here. So we can get the loss function over here. And this tells us that if we want to, we should increase z in order to increase y. And which again makes sense and tells us kind of also the magnitude of how much we should increase that to increase y hat by one point. And we kind of keep doing that now. So at each node, we write the derivative as the compute graph tells us. For the additions, it's incredibly easy because for additions, it's always just one. And finally, we have like the multiplications where we can say, okay, for this one, it's minus two. And for this one, it's three. And for the b, we also already got the derivative into this direction. And now comes the last step that we need to do. And that formatting here is incredibly off. This, what we need to do now is, we need to, if we want to do the derivative into the direction of w1, we multiply each of those partial derivatives on the entire path from the root node to w1. So it will be this times this times this times this times this. And the same way in the true direction of w2, and in the direction of b. So we get the derivative of, there's no way to get this right at this moment. But yeah, if I have like the derivative into the direction of w, I have like this formula over here, which tells us I should, it should decrease w1 to decrease the loss function. And I should increase w2 to decrease the loss function. And in the same way, I should decrease the, increase the parameter b in order to get a better loss function. So if we think about this, we wanted to have a smaller prediction over here. And, so we wanted to have, in order to get a better prediction over here, we want to have a larger value over here. So increasing b would make this value larger. So it would increase this value over here. So it would increase everything up the chain up here. So increasing b would increase this value over here. Increasing w would again, this is a positive value over here. So everything should increase this path. And decreasing w1 would again increase everything up the chain over here. So it kind of makes sense that what the derivative tells us, the gradient tells us, is some way to increase the entire value y hat over here, which should decrease the loss function. So, and the derivative kind of, is something that tells us exactly in which direction we should change each of the parameters in order to get close, to get a better loss function at the end. And so it looks, it kind of always, it looks scary, the entire calculations, but it's nice that each step consists of a very simple calculation. So each step along the compute graph is kind of plugging it in something into a formula that you know. And then just multiplying up everything at the end along the entire chain of calculations that we have here. Turns out things will get a lot complicated later on when we do a lot of matrix multiplications, because it kind of, you need to make sure that you remember which dimension you have to multiply with which to get the right result at the end. But it's always, you always need to remember that the, what you are doing under the hood is just what we did here. So one step after the other in a larger compute graph to multiply up the partial derivatives. One thing that one can do quite often is combining those two derivatives, because the derivative of the loss, the cross entropy loss and the sigmoid function, they kind of cancel out nicely. So the derivative of the loss function was this one, and the derivative of the sigmoid was this one. And if you multiply those, you get this times this. And as you can kind of see that you have like this thing over here and this thing, those match. And so you can kind of like multiply everything, everything here with this number over here. And in total, get something that is simpler than all the parts over here. So what one kind of often does is kind of combining the sigmoid function and this loss function into like one single node to make the calculations of the derivative a little bit easier because if one just combines them into one node, you get an easier derivative. It's not strictly necessary, but it's kind of sometimes makes calculations a little easier because things, if you take this one, you can kind of multiply out everything and it turns out that putting everything together just yields like y hat minus y. So it kind of tells you if my prediction was too large, if my prediction was too small, then I need to increase the value and if it was too large, I need to decrease it. And it's just like linear into that in the direction of what, of my prediction. So, and because we kind of all, very, very often have like cross entropy and sigmoid as like some things that we need to use together. When we want to do neural networks, we need to vectorize things. And that's why we need to do this. It's useful to use kind of the derivatives of vectorized operations. So if I'd want to take the gradient into the right direction of w of this dot product between w and x, it's just the vector x. So it's kind of the same way if I have like a single value of w times x, then the derivative, the gradient of this function, into the direction of w would be just x. So it's kind of, if we vectorize things, it kind of stays the same. And when doing any kind of calculations, any kind of like gradient computations, we always want to try to, to put everything into a vectorized form again. So something like this. And if we have like more dimensions, then it's kind of, we also still want to keep everything in the vector form. Do you have questions so far? So probably that was a lot to take in. So the first exercises, that go into this direction, will be a little bit tough for you because it's kind of, you need to go through those, those kind of, it's the, my hint for you is, try to make, try to put, do everything in little steps. So it's kind of like we did over, like we did over here. As long as you do very, very small steps, everything is kind of simple because like if you have like multiple steps, you multiply two things, the derivative into one direction is kind of the value on the opposite side and like additions, the derivative is one. So kind of break, if you do something, try to calculate derivatives for some small neural network, break things up in this way. Draw the compute graph, try to calculate, okay, which is multiplied by which, what is the entire, entire compute graph that I deal with over here? So that you kind of get comfortable with the, with what's happening over here. Once this clicks, you start to realize that what we do here is actually pretty simple because it's just those, basically what I told you, the derivative always tells you, should I increase or decrease the value that I have at the moment over here? So it kind of, this number here tells me if I want to increase the, the loss function, I should decrease the value that I predicted over here. So, and as, so, and obviously I want to go into the opposite direction to decrease the loss function. So that's what this number over here tells me. And for going this way, this way up here, the sigmoid kind of tells me if I want to increase the prediction that I made, I should increase, increase whatever this number is that that I put into the sigmoid function. And so if I want to decrease the prediction, I multiply this by this number and thereby get kind of that. If I want to, if I want to increase the entire loss function, what should I do with my value that I should put this one multiplied by this one. So I should decrease the value set over here. So that my loss function over here changes again. And that goes for the sum over here. It should also be, I multiply those things. So it should also be increased. And the product over here should also be in a decreased. If I want to increase the, the loss function and so on. So I, I, I, while multiplying up, I always know that at this point, I want to increase or decrease the value. And I also know by how much. So the direction is kind of the, just the sign over here. So it's kind of the easiest thing to argue about, but it also tells me how much compared to all the other values. So how much should I increase or decrease a certain value over the, at a certain node compared to all the other nodes that I have. In order to increase the loss function. And at the very end, I just flipped the sign because I want to actually decrease the, the loss function. So that was a lot to take in. And I, I, I, I hope to be. So at the beginning, we will start trying to put neural networks completely from scratch, using those things that we had here. So we will go through all those parts, building up the, the, the gradients of the single layers that we have to get kind of come completely built a small, small neural networks completely from scratch so that you can build this intuition. What those gradients do and how those competitive, what happens when we do train a machine learning system. And later on, we will start to push out those, this work to, to the frameworks that we want to use and where the framework does basically manages this compute graph and does those calculations for us. But like the first exercises, we will try to build a complete neural network completely from scratch so that we kind of see what happens under the hood and what the framework later on, does for us when it, when it comes, when, when we do do kind of auto differentiation using those. And if there's no more questions, then thank you and see you on Friday.\n"
     ]
    }
   ],
   "source": [
    "result = \"\"\n",
    "for item in filtered_segments:\n",
    "    result += item[\"text\"]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "\n",
    "# Split the documents into chunks\n",
    "chunks = text_splitter.split_text(result)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file has been updated at chunks_updated.json\n"
     ]
    }
   ],
   "source": [
    "def save_json_to_file(data, filename):\n",
    "    file_path = \"chunks_updated.json\"\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"file already exists\")\n",
    "        # Load existing JSON data from the file\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            existing_data = json.load(json_file)\n",
    "    else:\n",
    "        # If the file doesn't exist, initialize an empty list\n",
    "        existing_data = []\n",
    "\n",
    "    # Append the new array to the existing data\n",
    "    existing_data.extend(chunks)\n",
    "\n",
    "    # Convert the combined data to JSON format\n",
    "    json_data = json.dumps(existing_data, indent=4)\n",
    "\n",
    "    # Write the JSON data back to the file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "    print(f\"JSON file has been updated at {file_path}\")\n",
    "\n",
    "# Example usage:\n",
    "data = chunks\n",
    "filename = \"chunks_updated.json\"\n",
    "save_json_to_file(data, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('chunks_updated.json', 'r') as file:\n",
    "    chunks = json.load(file)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = FAISS.from_texts(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are Karan, a student at TH Bingen University. Act as Karan and reply all questions on his behalf. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "      <|system|>\n",
    "\n",
    "      {system_prompt}\n",
    "\n",
    "      <|user|>\n",
    "\n",
    "      {prompt}\n",
    "\n",
    "      <|assistant|>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "# You are Karan, a student at TH Bingen University. Act as Karan and reply all questions on his basis. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "# \"\"\".strip()\n",
    "\n",
    "\n",
    "# def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "#     return f\"\"\"\n",
    "# [INST] <>\n",
    "# {system_prompt}\n",
    "# <>\n",
    "\n",
    "# {prompt} [/INST]\n",
    "# \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOllama(model=\"llama2:latest\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db2.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStartTimeFromSegments(sourceString):\n",
    "    i = 0\n",
    "    searchForMatch = True\n",
    "    source_strings = sourceString.split(\".\")\n",
    "    json_data = filtered_segments  # Move JSON data retrieval outside the loop\n",
    "\n",
    "    while i < len(source_strings):\n",
    "        words = source_strings[i].split()\n",
    "\n",
    "        # Start with 3 words and increase by 1 if no match is found\n",
    "        num_words = 1\n",
    "        found_objects = []\n",
    "\n",
    "        max_num_words = len(words)  # Maximum number of words in the string\n",
    "\n",
    "        while num_words <= max_num_words:\n",
    "            # Take the first num_words from the words list and convert them to lowercase\n",
    "            search_phrase = ' '.join(words[:num_words])\n",
    "\n",
    "            # Search for matches in the JSON data (case-insensitive)\n",
    "            found_objects = [obj for obj in json_data if search_phrase in obj['text']]\n",
    "\n",
    "            # If no matches found or more than 1 found, and num_words doesn't exceed the max number of words, increase the number of words\n",
    "            if not found_objects or len(found_objects) > 1:\n",
    "                num_words += 1\n",
    "            else:\n",
    "                # Return the first found object and exit the loop\n",
    "                searchForMatch = False\n",
    "                return found_objects[0]\n",
    "\n",
    "        # If num_words exceeds the total number of words in the string, print the first object and break\n",
    "        if num_words > max_num_words:\n",
    "            i += 1\n",
    "\n",
    "    if searchForMatch:\n",
    "        return json_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:91\n",
      " * Running on http://172.20.10.4:91\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [27/Mar/2024 09:03:52] \"GET / HTTP/1.1\" 200 -\n",
      "[2024-03-27 09:03:58,317] ERROR in app: Exception on / [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py\", line 793, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py\", line 496, in _make_request\n",
      "    conn.request(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py\", line 400, in request\n",
      "    self.endheaders()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py\", line 1252, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py\", line 1012, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py\", line 952, in send\n",
      "    self.connect()\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py\", line 238, in connect\n",
      "    self.sock = self._new_conn()\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x162136040>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py\", line 847, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/urllib3/util/retry.py\", line 515, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x162136040>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/flask/app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/flask/app.py\", line 872, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/flask/app.py\", line 870, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/flask/app.py\", line 855, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "  File \"/var/folders/_k/q5t59kcn5xvgtfkqscc_msm00000gn/T/ipykernel_90237/3723155087.py\", line 168, in index\n",
      "    output_text = qa_chain({\"input_documents\": inputAfterSimilaritySearch[0].page_content, \"query\": input_text})\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py\", line 378, in __call__\n",
      "    return self.invoke(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py\", line 163, in invoke\n",
      "    raise e\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py\", line 153, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/retrieval_qa/base.py\", line 144, in _call\n",
      "    answer = self.combine_documents_chain.run(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py\", line 550, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py\", line 378, in __call__\n",
      "    return self.invoke(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py\", line 163, in invoke\n",
      "    raise e\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py\", line 153, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/combine_documents/base.py\", line 137, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/llm.py\", line 293, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py\", line 378, in __call__\n",
      "    return self.invoke(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py\", line 163, in invoke\n",
      "    raise e\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py\", line 153, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/llm.py\", line 103, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain/chains/llm.py\", line 115, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py\", line 544, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py\", line 408, in generate\n",
      "    raise e\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py\", line 398, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py\", line 577, in _generate_with_cache\n",
      "    return self._generate(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_community/chat_models/ollama.py\", line 257, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_community/chat_models/ollama.py\", line 188, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_community/chat_models/ollama.py\", line 161, in _create_chat_stream\n",
      "    yield from self._create_stream(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/langchain_community/llms/ollama.py\", line 220, in _create_stream\n",
      "    response = requests.post(\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/karanghai/Library/Python/3.9/lib/python/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x162136040>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "127.0.0.1 - - [27/Mar/2024 09:03:58] \"\u001b[35m\u001b[1mPOST / HTTP/1.1\u001b[0m\" 500 -\n",
      "127.0.0.1 - - [27/Mar/2024 09:04:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Mar/2024 09:04:18] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK SPLITTED words: ['neural', 'networks']\n",
      "search_phrase: neural\n",
      "found_objects:  [{'id': 75, 'seek': 26640, 'start': 271.17999999999995, 'end': 279.4, 'text': ' So, and nothing changes if we make our neural network bigger.', 'video_name': 'Kint3'}, {'id': 77, 'seek': 26640, 'start': 281.26, 'end': 285.64, 'text': ' is a one-layer neural network, works like this.', 'video_name': 'Kint3'}, {'id': 157, 'seek': 55946, 'start': 576.7800000000001, 'end': 580.9000000000001, 'text': ' when we get to like doing proper neural networks.', 'video_name': 'Kint3'}, {'id': 185, 'seek': 64810, 'start': 661.62, 'end': 667.16, 'text': ' So if you think of a larger neural network', 'video_name': 'Kint3'}, {'id': 250, 'seek': 79786, 'start': 823.4, 'end': 825.7, 'text': ' Logistic regression is a one-layer neural network.', 'video_name': 'Kint3'}, {'id': 281, 'seek': 88728, 'start': 907.24, 'end': 909.9399999999999, 'text': ' of another neural network layer,', 'video_name': 'Kint3'}, {'id': 282, 'seek': 88728, 'start': 910.9399999999999, 'end': 912.9599999999999, 'text': ' we would call it a neural network.', 'video_name': 'Kint3'}, {'id': 290, 'seek': 91728, 'start': 927.24, 'end': 929.0799999999999, 'text': ' If we build a neural network,', 'video_name': 'Kint3'}, {'id': 447, 'seek': 141810, 'start': 1446.4599999999998, 'end': 1447.9399999999998, 'text': ' But when building neural networks,', 'video_name': 'Kint3'}, {'id': 453, 'seek': 144794, 'start': 1462.6000000000001, 'end': 1463.88, 'text': ' for a neural network.', 'video_name': 'Kint3'}, {'id': 622, 'seek': 206294, 'start': 2075.94, 'end': 2079.94, 'text': ' So if we have our formula for our two layer neural network,', 'video_name': 'Kint3'}, {'id': 912, 'seek': 258694, 'start': 2595.94, 'end': 2596.94, 'text': ' in the neural network,', 'video_name': 'Kint3'}, {'id': 1022, 'seek': 273294, 'start': 2732.94, 'end': 2736.94, 'text': ' that the entire neural network will learn something useful,', 'video_name': 'Kint3'}, {'id': 1048, 'seek': 276194, 'start': 2779.54, 'end': 2782.2000000000003, 'text': ' our neural network basically can make something', 'video_name': 'Kint3'}, {'id': 1112, 'seek': 297160, 'start': 2985.08, 'end': 2989.02, 'text': ' or the proper choice for the last layer of a neural network.', 'video_name': 'Kint3'}, {'id': 1203, 'seek': 320864, 'start': 3225.3399999999997, 'end': 3227.8799999999997, 'text': ' to add another layer in the neural network.', 'video_name': 'Kint3'}, {'id': 1214, 'seek': 323840, 'start': 3251.56, 'end': 3253.7400000000002, 'text': ' for one of our neural network neurons.', 'video_name': 'Kint3'}, {'id': 1232, 'seek': 329816, 'start': 3301.3799999999997, 'end': 3304.58, 'text': ' that a neural network with some depth makes sense.', 'video_name': 'Kint3'}, {'id': 1233, 'seek': 329816, 'start': 3304.68, 'end': 3306.8999999999996, 'text': ' So multilayer neural networks make only sense', 'video_name': 'Kint3'}, {'id': 1246, 'seek': 332816, 'start': 3328.7, 'end': 3332.22, 'text': ' Be meaningful for having a multilayer neural network.', 'video_name': 'Kint3'}, {'id': 1247, 'seek': 332816, 'start': 3334.22, 'end': 3337.8799999999997, 'text': ' So there is no talking about multilayer neural networks', 'video_name': 'Kint3'}, {'id': 1301, 'seek': 347672, 'start': 3476.72, 'end': 3481.72, 'text': ' So the trick that this neural network does is', 'video_name': 'Kint3'}, {'id': 1379, 'seek': 368572, 'start': 3696.72, 'end': 3699.72, 'text': ' for the relevant calculations for a neural network.', 'video_name': 'Kint3'}, {'id': 1411, 'seek': 374472, 'start': 3764.72, 'end': 3767.72, 'text': ' all the parameters that our neural network has.', 'video_name': 'Kint3'}, {'id': 1442, 'seek': 383272, 'start': 3834.72, 'end': 3836.72, 'text': ' more layers in the neural network,', 'video_name': 'Kint3'}, {'id': 1468, 'seek': 389272, 'start': 3911.72, 'end': 3913.72, 'text': ' for this two-layer neural network.', 'video_name': 'Kint3'}, {'id': 1776, 'seek': 462872, 'start': 4638.72, 'end': 4642.72, 'text': ' to train your own numpy-based neural network', 'video_name': 'Kint3'}, {'id': 1795, 'seek': 465772, 'start': 4681.72, 'end': 4684.72, 'text': ' that we are dealing with a neural network?', 'video_name': 'Kint3'}, {'id': 1976, 'seek': 500872, 'start': 5030.72, 'end': 5032.72, 'text': ' whatever your neural network predicts,', 'video_name': 'Kint3'}, {'id': 2010, 'seek': 506672, 'start': 5094.72, 'end': 5095.72, 'text': ' When training your neural networks,', 'video_name': 'Kint3'}, {'id': 2017, 'seek': 509572, 'start': 5106.72, 'end': 5108.72, 'text': ' when training neural networks.', 'video_name': 'Kint3'}, {'id': 2032, 'seek': 512472, 'start': 5130.72, 'end': 5132.72, 'text': ' when it comes to the final neural network', 'video_name': 'Kint3'}, {'id': 110, 'seek': 49960, 'start': 512.2, 'end': 520.28, 'text': ' But this course will be all about deep learning and how to build neural networks,', 'video_name': 'Kint'}, {'id': 140, 'seek': 61882, 'start': 641.7600000000001, 'end': 646.5200000000001, 'text': ' Back then, they discovered a few fundamental limits for neural networks.', 'video_name': 'Kint'}, {'id': 147, 'seek': 67652, 'start': 680.76, 'end': 688.12, 'text': \" It's not like they said a neural network cannot compute a lot of...\", 'video_name': 'Kint'}, {'id': 149, 'seek': 67652, 'start': 691.22, 'end': 699.1999999999999, 'text': ' It basically said that, for example, one layer neural network can never solve the XOR function.', 'video_name': 'Kint'}, {'id': 150, 'seek': 67652, 'start': 700.0, 'end': 703.5, 'text': \" And yeah, which doesn't say anything about two layer neural networks.\", 'video_name': 'Kint'}, {'id': 155, 'seek': 70652, 'start': 715.3, 'end': 722.6, 'text': ' And back then, we got the first proper industrial uses of neural networks.', 'video_name': 'Kint'}, {'id': 156, 'seek': 70652, 'start': 722.76, 'end': 730.1999999999999, 'text': ' They had the first convolutional neural networks for identifying digits on letters.', 'video_name': 'Kint'}, {'id': 164, 'seek': 76580, 'start': 766.9399999999999, 'end': 771.42, 'text': ' So kind of reinforcement learning, support vector machines, recurrent neural networks,', 'video_name': 'Kint'}, {'id': 165, 'seek': 76580, 'start': 771.8199999999999, 'end': 773.7199999999999, 'text': ' convolutional neural networks, as I just said.', 'video_name': 'Kint'}, {'id': 168, 'seek': 76580, 'start': 786.0799999999999, 'end': 789.5999999999999, 'text': ' And the compute power to really run big neural networks.', 'video_name': 'Kint'}, {'id': 208, 'seek': 91552, 'start': 917.5, 'end': 921.92, 'text': ' A neural network called AlexNet.', 'video_name': 'Kint'}, {'id': 361, 'seek': 138532, 'start': 1391.32, 'end': 1394.32, 'text': ' which covers much more than just neural networks,', 'video_name': 'Kint'}, {'id': 410, 'seek': 156032, 'start': 1560.32, 'end': 1562.32, 'text': ' a very, very large neural network,', 'video_name': 'Kint'}, {'id': 414, 'seek': 156032, 'start': 1572.32, 'end': 1575.32, 'text': ' or a smaller neural network.', 'video_name': 'Kint'}, {'id': 421, 'seek': 159032, 'start': 1595.32, 'end': 1598.32, 'text': ' or training big neural networks.', 'video_name': 'Kint'}, {'id': 444, 'seek': 164932, 'start': 1671.32, 'end': 1675.32, 'text': ' Our plan is not to use neural networks', 'video_name': 'Kint'}, {'id': 445, 'seek': 164932, 'start': 1675.32, 'end': 1677.32, 'text': ' because neural networks are incredibly cool.', 'video_name': 'Kint'}, {'id': 696, 'seek': 220332, 'start': 2219.32, 'end': 2221.32, 'text': ' I want to have a neural network with...', 'video_name': 'Kint'}, {'id': 708, 'seek': 223232, 'start': 2239.32, 'end': 2241.32, 'text': ' define our neural network.', 'video_name': 'Kint'}, {'id': 711, 'seek': 223232, 'start': 2246.32, 'end': 2248.32, 'text': ' How the neural network would update in each step.', 'video_name': 'Kint'}, {'id': 731, 'seek': 226132, 'start': 2275.32, 'end': 2277.32, 'text': ' Because kind of neural networks kind of tend...', 'video_name': 'Kint'}, {'id': 899, 'seek': 261432, 'start': 2620.32, 'end': 2623.32, 'text': ' what actually is a neural network?', 'video_name': 'Kint'}, {'id': 988, 'seek': 288012, 'start': 2897.12, 'end': 2903.12, 'text': ' And what we basically did here was we created a very, very small neural network.', 'video_name': 'Kint'}, {'id': 990, 'seek': 288012, 'start': 2907.12, 'end': 2908.12, 'text': ' a very small neural network.', 'video_name': 'Kint'}, {'id': 1005, 'seek': 294012, 'start': 2944.12, 'end': 2950.12, 'text': ' So that is mainly what one neuron in an artificial neural network is.', 'video_name': 'Kint'}, {'id': 1010, 'seek': 294012, 'start': 2961.12, 'end': 2964.12, 'text': ' When you hear people talking about neural networks,', 'video_name': 'Kint'}, {'id': 1011, 'seek': 294012, 'start': 2964.12, 'end': 2968.12, 'text': ' then the neural networks are often compared to the brain.', 'video_name': 'Kint'}, {'id': 1057, 'seek': 305612, 'start': 3079.12, 'end': 3083.12, 'text': \" But it's kind of one of the most used activation functions for neural networks.\", 'video_name': 'Kint'}, {'id': 1117, 'seek': 320412, 'start': 3213.12, 'end': 3216.12, 'text': ' a real neural network is.', 'video_name': 'Kint'}, {'id': 1131, 'seek': 323312, 'start': 3243.12, 'end': 3245.12, 'text': ' building a neural network...', 'video_name': 'Kint'}, {'id': 1204, 'seek': 341212, 'start': 3417.12, 'end': 3419.12, 'text': ' What the neural network does is...', 'video_name': 'Kint'}, {'id': 1376, 'seek': 376512, 'start': 3781.12, 'end': 3783.12, 'text': ' neural networks...', 'video_name': 'Kint'}, {'id': 1378, 'seek': 376512, 'start': 3785.12, 'end': 3787.12, 'text': \" it doesn't even have to be a neural network...\", 'video_name': 'Kint'}, {'id': 1408, 'seek': 382312, 'start': 3845.12, 'end': 3847.12, 'text': ' architectures of neural networks...', 'video_name': 'Kint'}, {'id': 1431, 'seek': 388312, 'start': 3891.12, 'end': 3893.12, 'text': ' convolutional neural networks...', 'video_name': 'Kint'}, {'id': 1434, 'seek': 388312, 'start': 3897.12, 'end': 3899.12, 'text': ' recurrent neural networks...', 'video_name': 'Kint'}, {'id': 2034, 'seek': 511312, 'start': 5123.12, 'end': 5125.12, 'text': ' your own neural networks...', 'video_name': 'Kint'}, {'id': 645, 'seek': 172422, 'start': 1745.22, 'end': 1753.22, 'text': ' and that will later keep working when we do larger neural networks.', 'video_name': 'Kint2'}, {'id': 1693, 'seek': 462696, 'start': 4645.96, 'end': 4649.96, 'text': ' When we want to do neural networks,', 'video_name': 'Kint2'}, {'id': 1746, 'seek': 477396, 'start': 4790.96, 'end': 4793.96, 'text': ' for some small neural network,', 'video_name': 'Kint2'}, {'id': 1821, 'seek': 494996, 'start': 4957.96, 'end': 4961.96, 'text': ' we will start trying to put neural networks completely from scratch,', 'video_name': 'Kint2'}, {'id': 1828, 'seek': 494996, 'start': 4972.96, 'end': 4977.96, 'text': ' small neural networks completely from scratch so that you can build this', 'video_name': 'Kint2'}, {'id': 1838, 'seek': 497896, 'start': 5001.96, 'end': 5004.96, 'text': ' we will try to build a complete neural network completely from scratch so that', 'video_name': 'Kint2'}]\n",
      "search_phrase: neural networks\n",
      "found_objects:  [{'id': 157, 'seek': 55946, 'start': 576.7800000000001, 'end': 580.9000000000001, 'text': ' when we get to like doing proper neural networks.', 'video_name': 'Kint3'}, {'id': 447, 'seek': 141810, 'start': 1446.4599999999998, 'end': 1447.9399999999998, 'text': ' But when building neural networks,', 'video_name': 'Kint3'}, {'id': 1233, 'seek': 329816, 'start': 3304.68, 'end': 3306.8999999999996, 'text': ' So multilayer neural networks make only sense', 'video_name': 'Kint3'}, {'id': 1247, 'seek': 332816, 'start': 3334.22, 'end': 3337.8799999999997, 'text': ' So there is no talking about multilayer neural networks', 'video_name': 'Kint3'}, {'id': 2010, 'seek': 506672, 'start': 5094.72, 'end': 5095.72, 'text': ' When training your neural networks,', 'video_name': 'Kint3'}, {'id': 2017, 'seek': 509572, 'start': 5106.72, 'end': 5108.72, 'text': ' when training neural networks.', 'video_name': 'Kint3'}, {'id': 110, 'seek': 49960, 'start': 512.2, 'end': 520.28, 'text': ' But this course will be all about deep learning and how to build neural networks,', 'video_name': 'Kint'}, {'id': 140, 'seek': 61882, 'start': 641.7600000000001, 'end': 646.5200000000001, 'text': ' Back then, they discovered a few fundamental limits for neural networks.', 'video_name': 'Kint'}, {'id': 150, 'seek': 67652, 'start': 700.0, 'end': 703.5, 'text': \" And yeah, which doesn't say anything about two layer neural networks.\", 'video_name': 'Kint'}, {'id': 155, 'seek': 70652, 'start': 715.3, 'end': 722.6, 'text': ' And back then, we got the first proper industrial uses of neural networks.', 'video_name': 'Kint'}, {'id': 156, 'seek': 70652, 'start': 722.76, 'end': 730.1999999999999, 'text': ' They had the first convolutional neural networks for identifying digits on letters.', 'video_name': 'Kint'}, {'id': 164, 'seek': 76580, 'start': 766.9399999999999, 'end': 771.42, 'text': ' So kind of reinforcement learning, support vector machines, recurrent neural networks,', 'video_name': 'Kint'}, {'id': 165, 'seek': 76580, 'start': 771.8199999999999, 'end': 773.7199999999999, 'text': ' convolutional neural networks, as I just said.', 'video_name': 'Kint'}, {'id': 168, 'seek': 76580, 'start': 786.0799999999999, 'end': 789.5999999999999, 'text': ' And the compute power to really run big neural networks.', 'video_name': 'Kint'}, {'id': 361, 'seek': 138532, 'start': 1391.32, 'end': 1394.32, 'text': ' which covers much more than just neural networks,', 'video_name': 'Kint'}, {'id': 421, 'seek': 159032, 'start': 1595.32, 'end': 1598.32, 'text': ' or training big neural networks.', 'video_name': 'Kint'}, {'id': 444, 'seek': 164932, 'start': 1671.32, 'end': 1675.32, 'text': ' Our plan is not to use neural networks', 'video_name': 'Kint'}, {'id': 445, 'seek': 164932, 'start': 1675.32, 'end': 1677.32, 'text': ' because neural networks are incredibly cool.', 'video_name': 'Kint'}, {'id': 731, 'seek': 226132, 'start': 2275.32, 'end': 2277.32, 'text': ' Because kind of neural networks kind of tend...', 'video_name': 'Kint'}, {'id': 1010, 'seek': 294012, 'start': 2961.12, 'end': 2964.12, 'text': ' When you hear people talking about neural networks,', 'video_name': 'Kint'}, {'id': 1011, 'seek': 294012, 'start': 2964.12, 'end': 2968.12, 'text': ' then the neural networks are often compared to the brain.', 'video_name': 'Kint'}, {'id': 1057, 'seek': 305612, 'start': 3079.12, 'end': 3083.12, 'text': \" But it's kind of one of the most used activation functions for neural networks.\", 'video_name': 'Kint'}, {'id': 1376, 'seek': 376512, 'start': 3781.12, 'end': 3783.12, 'text': ' neural networks...', 'video_name': 'Kint'}, {'id': 1408, 'seek': 382312, 'start': 3845.12, 'end': 3847.12, 'text': ' architectures of neural networks...', 'video_name': 'Kint'}, {'id': 1431, 'seek': 388312, 'start': 3891.12, 'end': 3893.12, 'text': ' convolutional neural networks...', 'video_name': 'Kint'}, {'id': 1434, 'seek': 388312, 'start': 3897.12, 'end': 3899.12, 'text': ' recurrent neural networks...', 'video_name': 'Kint'}, {'id': 2034, 'seek': 511312, 'start': 5123.12, 'end': 5125.12, 'text': ' your own neural networks...', 'video_name': 'Kint'}, {'id': 645, 'seek': 172422, 'start': 1745.22, 'end': 1753.22, 'text': ' and that will later keep working when we do larger neural networks.', 'video_name': 'Kint2'}, {'id': 1693, 'seek': 462696, 'start': 4645.96, 'end': 4649.96, 'text': ' When we want to do neural networks,', 'video_name': 'Kint2'}, {'id': 1821, 'seek': 494996, 'start': 4957.96, 'end': 4961.96, 'text': ' we will start trying to put neural networks completely from scratch,', 'video_name': 'Kint2'}, {'id': 1828, 'seek': 494996, 'start': 4972.96, 'end': 4977.96, 'text': ' small neural networks completely from scratch so that you can build this', 'video_name': 'Kint2'}]\n",
      "Exceeded total number of words, trying with next line\n",
      "CHECK SPLITTED words: ['And', 'that', 'is', 'gradient', 'descent']\n",
      "search_phrase: And\n",
      "found_objects:  [{'id': 4, 'seek': 0, 'start': 16.240000000000002, 'end': 20.3, 'text': ' And we saw that the thing that we need', 'video_name': 'Kint3'}, {'id': 13, 'seek': 2740, 'start': 51.019999999999996, 'end': 57.379999999999995, 'text': ' And if, for example, we use a', 'video_name': 'Kint3'}, {'id': 21, 'seek': 8732, 'start': 87.32, 'end': 89.08, 'text': ' And then we can use the sigma function of all of this', 'video_name': 'Kint3'}, {'id': 23, 'seek': 8732, 'start': 94.75999999999999, 'end': 99.75999999999999, 'text': ' And for each of those steps, we know the gradients', 'video_name': 'Kint3'}, {'id': 29, 'seek': 8732, 'start': 111.24, 'end': 113.53999999999999, 'text': \" And after we've looked it up, we can just\", 'video_name': 'Kint3'}, {'id': 31, 'seek': 11686, 'start': 116.86, 'end': 119.8, 'text': ' And if we say, OK, we put in our current values,', 'video_name': 'Kint3'}, {'id': 41, 'seek': 14640, 'start': 164.64000000000001, 'end': 166.44, 'text': ' And actually, it is a positive example.', 'video_name': 'Kint3'}, {'id': 44, 'seek': 14640, 'start': 170.96, 'end': 175.72, 'text': ' And if we used cross entropy formula, we get a loss of close', 'video_name': 'Kint3'}, {'id': 46, 'seek': 17640, 'start': 176.4, 'end': 182.64000000000001, 'text': ' And if we do the backward calculation,', 'video_name': 'Kint3'}, {'id': 54, 'seek': 20640, 'start': 206.4, 'end': 209.14000000000001, 'text': ' And we can plug in the y hat in the formula over here.', 'video_name': 'Kint3'}, {'id': 57, 'seek': 20640, 'start': 221.56, 'end': 224.98000000000002, 'text': ' And we can basically do that for all the other steps.', 'video_name': 'Kint3'}, {'id': 60, 'seek': 20640, 'start': 231.42000000000002, 'end': 236.4, 'text': \" And for the last step over here, with multiplication, it's,\", 'video_name': 'Kint3'}, {'id': 64, 'seek': 23640, 'start': 243.74, 'end': 249.72, 'text': ' And now using the chain rule, we can calculate,', 'video_name': 'Kint3'}, {'id': 71, 'seek': 23640, 'start': 262.84000000000003, 'end': 264.4, 'text': ' And if we multiply everything up,', 'video_name': 'Kint3'}, {'id': 73, 'seek': 23640, 'start': 265.52, 'end': 266.4, 'text': ' And then we can calculate the gradient.', 'video_name': 'Kint3'}, {'id': 85, 'seek': 29640, 'start': 308.03999999999996, 'end': 315.88, 'text': ' And to help us there, using back propagation and the compute graph', 'video_name': 'Kint3'}, {'id': 89, 'seek': 29640, 'start': 325.23999999999995, 'end': 326.15999999999997, 'text': ' And then we can just write down the formulas.', 'video_name': 'Kint3'}, {'id': 90, 'seek': 29640, 'start': 326.15999999999997, 'end': 326.35999999999996, 'text': ' And then we can just write down the formulas.', 'video_name': 'Kint3'}, {'id': 91, 'seek': 29640, 'start': 326.35999999999996, 'end': 326.38, 'text': ' And then we can just write down the formulas.', 'video_name': 'Kint3'}, {'id': 92, 'seek': 32640, 'start': 326.4, 'end': 326.64, 'text': ' And then we can just write down the formulas.', 'video_name': 'Kint3'}, {'id': 101, 'seek': 35636, 'start': 356.36, 'end': 365.46000000000004, 'text': ' And sigmoid, for combining cross entropy loss and the sigmoid is just y hat minus y.', 'video_name': 'Kint3'}, {'id': 121, 'seek': 43990, 'start': 443.79999999999995, 'end': 452.59999999999997, 'text': ' And if we...', 'video_name': 'Kint3'}, {'id': 123, 'seek': 43990, 'start': 459.47999999999996, 'end': 461.73999999999995, 'text': ' And what is the...', 'video_name': 'Kint3'}, {'id': 127, 'seek': 46966, 'start': 474.82000000000005, 'end': 477.82000000000005, 'text': ' And if we take the gradient in the direction of w1,', 'video_name': 'Kint3'}, {'id': 129, 'seek': 46966, 'start': 481.28000000000003, 'end': 482.72, 'text': ' And we are left with x1.', 'video_name': 'Kint3'}, {'id': 139, 'seek': 49966, 'start': 514.96, 'end': 516.98, 'text': ' And so if I have a times b', 'video_name': 'Kint3'}, {'id': 144, 'seek': 52946, 'start': 529.46, 'end': 532.4000000000001, 'text': ' And for a dot product, it works exactly the same way.', 'video_name': 'Kint3'}, {'id': 145, 'seek': 52946, 'start': 532.98, 'end': 536.94, 'text': ' And somehow this also kind of carries over', 'video_name': 'Kint3'}, {'id': 153, 'seek': 55946, 'start': 564.86, 'end': 566.2800000000001, 'text': ' And for each of those, again,', 'video_name': 'Kint3'}, {'id': 156, 'seek': 55946, 'start': 574.14, 'end': 576.7800000000001, 'text': \" And we'll do more about this later\", 'video_name': 'Kint3'}, {'id': 167, 'seek': 58822, 'start': 600.36, 'end': 605.1600000000001, 'text': ' And the complicated part is always', 'video_name': 'Kint3'}, {'id': 175, 'seek': 61810, 'start': 625.9, 'end': 628.98, 'text': ' And backpropagation is kind of the basic', 'video_name': 'Kint3'}, {'id': 190, 'seek': 67810, 'start': 678.1, 'end': 681.22, 'text': \" And it's kind of an engineering fine-tuning thing\", 'video_name': 'Kint3'}, {'id': 216, 'seek': 73792, 'start': 744.88, 'end': 747.3399999999999, 'text': \" And yeah, that's basically it.\", 'video_name': 'Kint3'}, {'id': 241, 'seek': 79786, 'start': 802.74, 'end': 804.62, 'text': ' And the general advice is', 'video_name': 'Kint3'}, {'id': 251, 'seek': 79786, 'start': 825.86, 'end': 827.28, 'text': ' And here we use the sigmoid', 'video_name': 'Kint3'}, {'id': 254, 'seek': 82728, 'start': 831.24, 'end': 837.3, 'text': ' And the logistic regression', 'video_name': 'Kint3'}, {'id': 263, 'seek': 85728, 'start': 865.4599999999999, 'end': 869.28, 'text': ' And...', 'video_name': 'Kint3'}, {'id': 274, 'seek': 88728, 'start': 889.5, 'end': 890.02, 'text': ' And...', 'video_name': 'Kint3'}, {'id': 297, 'seek': 94728, 'start': 947.28, 'end': 950.56, 'text': ' And produce some more refined output', 'video_name': 'Kint3'}, {'id': 301, 'seek': 94728, 'start': 958.24, 'end': 959.06, 'text': ' And...', 'video_name': 'Kint3'}, {'id': 313, 'seek': 97728, 'start': 982.6999999999999, 'end': 984.86, 'text': ' And everything else we call a hidden layer.', 'video_name': 'Kint3'}, {'id': 314, 'seek': 97728, 'start': 985.5, 'end': 986.9599999999999, 'text': ' And we can have a lot of those.', 'video_name': 'Kint3'}, {'id': 324, 'seek': 100728, 'start': 1011.24, 'end': 1014.56, 'text': ' And we have a bias term for this neuron over here.', 'video_name': 'Kint3'}, {'id': 328, 'seek': 100728, 'start': 1023.16, 'end': 1026.26, 'text': ' And we call this output z1.', 'video_name': 'Kint3'}, {'id': 329, 'seek': 100728, 'start': 1027.36, 'end': 1030.02, 'text': ' And we apply some activation function', 'video_name': 'Kint3'}, {'id': 332, 'seek': 100728, 'start': 1033.8, 'end': 1036.3799999999999, 'text': ' And get something that we call the activations.', 'video_name': 'Kint3'}, {'id': 333, 'seek': 103728, 'start': 1037.28, 'end': 1039.56, 'text': ' And we have a number of the first layer.', 'video_name': 'Kint3'}, {'id': 335, 'seek': 103728, 'start': 1042.6, 'end': 1046.62, 'text': ' And this number would be a1 superscript 1.', 'video_name': 'Kint3'}, {'id': 336, 'seek': 103728, 'start': 1046.62, 'end': 1050.72, 'text': ' And we do the same thing for each of those neurons.', 'video_name': 'Kint3'}, {'id': 342, 'seek': 103728, 'start': 1059.0, 'end': 1063.34, 'text': ' And because we will have like a lot of layers', 'video_name': 'Kint3'}, {'id': 348, 'seek': 106668, 'start': 1077.8600000000001, 'end': 1081.68, 'text': \" And probably I'll also have still some errors in the slides\", 'video_name': 'Kint3'}, {'id': 356, 'seek': 109668, 'start': 1099.94, 'end': 1104.8400000000001, 'text': ' And I use the subscript over here to determine the individual neuron', 'video_name': 'Kint3'}, {'id': 361, 'seek': 109668, 'start': 1114.44, 'end': 1117.24, 'text': ' And the output layer has just one neuron.', 'video_name': 'Kint3'}, {'id': 365, 'seek': 112668, 'start': 1126.68, 'end': 1130.14, 'text': \" And we'll have to add more indices later on\", 'video_name': 'Kint3'}, {'id': 370, 'seek': 112668, 'start': 1143.38, 'end': 1148.88, 'text': ' And the subscript indicates kind of the neuron which we are using.', 'video_name': 'Kint3'}, {'id': 378, 'seek': 115652, 'start': 1179.44, 'end': 1183.62, 'text': ' And those get mapped using the sigmoid or some activation', 'video_name': 'Kint3'}, {'id': 380, 'seek': 118652, 'start': 1186.52, 'end': 1192.76, 'text': ' And those are the input for the next layer.', 'video_name': 'Kint3'}, {'id': 381, 'seek': 118652, 'start': 1192.76, 'end': 1198.0, 'text': ' And what we want to do is batch together all those operations', 'video_name': 'Kint3'}, {'id': 389, 'seek': 121184, 'start': 1227.3799999999999, 'end': 1231.12, 'text': ' And each of those neurons has three different inputs.', 'video_name': 'Kint3'}, {'id': 398, 'seek': 124176, 'start': 1254.94, 'end': 1258.3799999999999, 'text': ' And then as an output, we get a vector with all those z values.', 'video_name': 'Kint3'}, {'id': 406, 'seek': 127176, 'start': 1294.4, 'end': 1299.32, 'text': ' And the sigmoid function, again, in the way that NumPy does it,', 'video_name': 'Kint3'}, {'id': 413, 'seek': 130176, 'start': 1331.12, 'end': 1331.68, 'text': ' And then we have the z vector over here.', 'video_name': 'Kint3'}, {'id': 417, 'seek': 133176, 'start': 1341.92, 'end': 1347.9, 'text': ' And, again, this whole thing will be added with bias term b.', 'video_name': 'Kint3'}, {'id': 420, 'seek': 133176, 'start': 1355.44, 'end': 1361.68, 'text': ' And what is inside here, we call the logit,', 'video_name': 'Kint3'}, {'id': 426, 'seek': 136168, 'start': 1374.16, 'end': 1376.14, 'text': ' And the sigmoid over here gives us', 'video_name': 'Kint3'}, {'id': 428, 'seek': 136168, 'start': 1378.42, 'end': 1379.92, 'text': ' And because that was the last layer,', 'video_name': 'Kint3'}, {'id': 439, 'seek': 138810, 'start': 1411.56, 'end': 1414.6, 'text': ' And of course, if we would add the loss function over here,', 'video_name': 'Kint3'}, {'id': 454, 'seek': 144794, 'start': 1464.42, 'end': 1465.94, 'text': ' And the layer is...', 'video_name': 'Kint3'}, {'id': 460, 'seek': 147794, 'start': 1480.44, 'end': 1485.94, 'text': ' And getting the activations for each of those neurons in here.', 'video_name': 'Kint3'}, {'id': 461, 'seek': 147794, 'start': 1485.94, 'end': 1493.94, 'text': ' And we usually never really think in terms of individual neurons,', 'video_name': 'Kint3'}, {'id': 475, 'seek': 153694, 'start': 1536.94, 'end': 1541.94, 'text': ' And if we have those sizes over here,', 'video_name': 'Kint3'}, {'id': 484, 'seek': 156294, 'start': 1574.94, 'end': 1577.94, 'text': \" And because that's the size of the output,\", 'video_name': 'Kint3'}, {'id': 494, 'seek': 159094, 'start': 1614.94, 'end': 1617.94, 'text': ' And the thing to remember is,', 'video_name': 'Kint3'}, {'id': 499, 'seek': 162094, 'start': 1632.94, 'end': 1636.94, 'text': ' And if we have like, if B turns out to be a vector,', 'video_name': 'Kint3'}, {'id': 503, 'seek': 162094, 'start': 1644.94, 'end': 1648.94, 'text': \" And that's kind of the image that you need to keep in mind\", 'video_name': 'Kint3'}, {'id': 505, 'seek': 165094, 'start': 1650.94, 'end': 1652.94, 'text': ' And what your inputs and outputs have to be,', 'video_name': 'Kint3'}, {'id': 517, 'seek': 168094, 'start': 1691.94, 'end': 1700.94, 'text': \" And so I'll just give you kind of a brief example over here\", 'video_name': 'Kint3'}, {'id': 536, 'seek': 176794, 'start': 1782.94, 'end': 1786.94, 'text': ' And so,', 'video_name': 'Kint3'}, {'id': 576, 'seek': 191594, 'start': 1928.94, 'end': 1936.94, 'text': ' And to make sure that I now add the bias term individually to each of those data points,', 'video_name': 'Kint3'}, {'id': 593, 'seek': 197394, 'start': 1988.94, 'end': 1995.94, 'text': \" And that's kind of the motivation over here to,\", 'video_name': 'Kint3'}, {'id': 614, 'seek': 203394, 'start': 2057.94, 'end': 2060.94, 'text': \" And there's probably one question,\", 'video_name': 'Kint3'}, {'id': 626, 'seek': 206294, 'start': 2088.94, 'end': 2090.94, 'text': ' And in the original version,', 'video_name': 'Kint3'}, {'id': 642, 'seek': 212094, 'start': 2130.94, 'end': 2131.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 654, 'seek': 214994, 'start': 2155.94, 'end': 2159.94, 'text': ' And if I multiply this matrix with this one and at this vector over here,', 'video_name': 'Kint3'}, {'id': 656, 'seek': 214994, 'start': 2162.94, 'end': 2163.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 666, 'seek': 217894, 'start': 2185.94, 'end': 2189.94, 'text': ' And to make lots and lots of calculations.', 'video_name': 'Kint3'}, {'id': 675, 'seek': 220794, 'start': 2209.94, 'end': 2212.94, 'text': ' And that is the reason why,', 'video_name': 'Kint3'}, {'id': 700, 'seek': 226694, 'start': 2282.94, 'end': 2284.94, 'text': ' And down here,', 'video_name': 'Kint3'}, {'id': 705, 'seek': 226694, 'start': 2294.94, 'end': 2295.94, 'text': ' And would it,', 'video_name': 'Kint3'}, {'id': 717, 'seek': 229594, 'start': 2316.94, 'end': 2318.94, 'text': ' And that means,', 'video_name': 'Kint3'}, {'id': 722, 'seek': 232594, 'start': 2328.94, 'end': 2329.94, 'text': ' And that,', 'video_name': 'Kint3'}, {'id': 733, 'seek': 232594, 'start': 2345.94, 'end': 2346.94, 'text': ' And we,', 'video_name': 'Kint3'}, {'id': 760, 'seek': 238294, 'start': 2389.94, 'end': 2390.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 763, 'seek': 238294, 'start': 2395.94, 'end': 2396.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 771, 'seek': 238294, 'start': 2407.94, 'end': 2408.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 782, 'seek': 241194, 'start': 2421.94, 'end': 2422.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 804, 'seek': 244094, 'start': 2450.94, 'end': 2451.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 807, 'seek': 244094, 'start': 2453.94, 'end': 2454.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 833, 'seek': 246994, 'start': 2484.94, 'end': 2485.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 850, 'seek': 249894, 'start': 2502.94, 'end': 2503.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 862, 'seek': 249894, 'start': 2524.94, 'end': 2525.94, 'text': ' And,', 'video_name': 'Kint3'}, {'id': 1045, 'seek': 276194, 'start': 2769.14, 'end': 2773.42, 'text': ' And a linear classifier can only predict', 'video_name': 'Kint3'}, {'id': 1061, 'seek': 282164, 'start': 2826.72, 'end': 2834.18, 'text': ' And no matter how large the value is that we have over here,', 'video_name': 'Kint3'}, {'id': 1071, 'seek': 285164, 'start': 2855.6, 'end': 2857.2599999999998, 'text': ' And there is a fix for this, and this', 'video_name': 'Kint3'}, {'id': 1078, 'seek': 285164, 'start': 2878.2599999999998, 'end': 2881.44, 'text': \" And if it's small, then I'll just map it to a very small,\", 'video_name': 'Kint3'}, {'id': 1103, 'seek': 294162, 'start': 2958.4, 'end': 2961.2599999999998, 'text': ' And obviously, this is a bad choice for all hidden layers', 'video_name': 'Kint3'}, {'id': 1187, 'seek': 317882, 'start': 3201.0800000000004, 'end': 3202.3, 'text': ' And do I make...', 'video_name': 'Kint3'}, {'id': 1192, 'seek': 317882, 'start': 3208.48, 'end': 3208.5800000000004, 'text': \" And if it's a very complicated function,\", 'video_name': 'Kint3'}, {'id': 1194, 'seek': 320864, 'start': 3208.64, 'end': 3208.68, 'text': \" And if it's a very complicated function,\", 'video_name': 'Kint3'}, {'id': 1204, 'seek': 320864, 'start': 3228.2, 'end': 3230.66, 'text': ' And then probably ReLU with another layer', 'video_name': 'Kint3'}, {'id': 1206, 'seek': 320864, 'start': 3233.94, 'end': 3236.16, 'text': ' And then I could have also just used ReLU', 'video_name': 'Kint3'}, {'id': 1215, 'seek': 323840, 'start': 3254.02, 'end': 3258.56, 'text': ' And each of the neurons has basically two jobs', 'video_name': 'Kint3'}, {'id': 1218, 'seek': 323840, 'start': 3263.78, 'end': 3264.6, 'text': ' And that is part...', 'video_name': 'Kint3'}, {'id': 1226, 'seek': 326816, 'start': 3283.3199999999997, 'end': 3286.1, 'text': ' And then I introduce some non-linearity', 'video_name': 'Kint3'}, {'id': 1229, 'seek': 326816, 'start': 3292.1, 'end': 3292.54, 'text': ' And...', 'video_name': 'Kint3'}, {'id': 1230, 'seek': 326816, 'start': 3292.54, 'end': 3298.14, 'text': ' And basically these are also the...', 'video_name': 'Kint3'}, {'id': 1235, 'seek': 329816, 'start': 3309.68, 'end': 3310.12, 'text': ' And...', 'video_name': 'Kint3'}, {'id': 1258, 'seek': 335804, 'start': 3360.74, 'end': 3363.08, 'text': ' And so if both are 0 or both are 1,', 'video_name': 'Kint3'}, {'id': 1275, 'seek': 338792, 'start': 3409.42, 'end': 3411.62, 'text': ' And then I...', 'video_name': 'Kint3'}, {'id': 1277, 'seek': 338792, 'start': 3414.98, 'end': 3417.2400000000002, 'text': ' And so if I go through the calculations,', 'video_name': 'Kint3'}, {'id': 1285, 'seek': 341792, 'start': 3433.02, 'end': 3435.92, 'text': ' And if I do the same thing for A2,', 'video_name': 'Kint3'}, {'id': 1288, 'seek': 341792, 'start': 3440.2200000000003, 'end': 3443.62, 'text': ' And 0 and 1 is 1.', 'video_name': 'Kint3'}, {'id': 1289, 'seek': 341792, 'start': 3444.12, 'end': 3447.7200000000003, 'text': ' And minus 1 plus 0 is minus 1.', 'video_name': 'Kint3'}, {'id': 1291, 'seek': 344772, 'start': 3449.4199999999996, 'end': 3452.4199999999996, 'text': ' And minus 1 plus 1 is 0 again.', 'video_name': 'Kint3'}, {'id': 1293, 'seek': 344772, 'start': 3454.12, 'end': 3455.4199999999996, 'text': ' And if I just add them up,', 'video_name': 'Kint3'}, {'id': 1295, 'seek': 344772, 'start': 3458.72, 'end': 3461.62, 'text': ' And this is kind of also shows you', 'video_name': 'Kint3'}, {'id': 1299, 'seek': 344772, 'start': 3473.22, 'end': 3475.22, 'text': ' And this 0 over here would be a minus 1.', 'video_name': 'Kint3'}, {'id': 1300, 'seek': 344772, 'start': 3475.22, 'end': 3476.72, 'text': ' And if I add them up over here, I get 0s over here.', 'video_name': 'Kint3'}, {'id': 1304, 'seek': 347672, 'start': 3488.72, 'end': 3494.72, 'text': ' And this way, I can get this kind of kink', 'video_name': 'Kint3'}, {'id': 1306, 'seek': 347672, 'start': 3499.22, 'end': 3501.72, 'text': \" And that's kind of what...\", 'video_name': 'Kint3'}, {'id': 1307, 'seek': 347672, 'start': 3501.72, 'end': 3505.72, 'text': ' And it also shows that even the simple ReLU function', 'video_name': 'Kint3'}, {'id': 1315, 'seek': 350672, 'start': 3524.72, 'end': 3526.72, 'text': ' And I have like two circles over here', 'video_name': 'Kint3'}, {'id': 1318, 'seek': 350672, 'start': 3532.72, 'end': 3533.72, 'text': ' And now the second layer can just make', 'video_name': 'Kint3'}, {'id': 1321, 'seek': 353672, 'start': 3540.72, 'end': 3542.72, 'text': \" And if I don't have a ReLU function,\", 'video_name': 'Kint3'}, {'id': 1323, 'seek': 353672, 'start': 3546.72, 'end': 3548.72, 'text': ' And again, I cannot draw...', 'video_name': 'Kint3'}, {'id': 1354, 'seek': 362672, 'start': 3640.72, 'end': 3643.72, 'text': ' And this is kind of pretty neat', 'video_name': 'Kint3'}, {'id': 1359, 'seek': 362672, 'start': 3650.72, 'end': 3654.72, 'text': ' And so the number of calculations we need to do', 'video_name': 'Kint3'}, {'id': 1363, 'seek': 365572, 'start': 3660.72, 'end': 3661.72, 'text': ' And like...', 'video_name': 'Kint3'}, {'id': 1385, 'seek': 368572, 'start': 3711.72, 'end': 3712.72, 'text': ' And for each of the parameters,', 'video_name': 'Kint3'}, {'id': 1392, 'seek': 371572, 'start': 3726.72, 'end': 3728.72, 'text': ' And we have the bias terms', 'video_name': 'Kint3'}, {'id': 1394, 'seek': 371572, 'start': 3730.72, 'end': 3732.72, 'text': ' And like the dimensions here,', 'video_name': 'Kint3'}, {'id': 1402, 'seek': 374472, 'start': 3744.72, 'end': 3746.72, 'text': ' And if we think about it,', 'video_name': 'Kint3'}, {'id': 1406, 'seek': 374472, 'start': 3751.72, 'end': 3753.72, 'text': ' And we usually call that the function J,', 'video_name': 'Kint3'}, {'id': 1410, 'seek': 374472, 'start': 3760.72, 'end': 3763.72, 'text': ' And the cost function is dependent on', 'video_name': 'Kint3'}, {'id': 1414, 'seek': 374472, 'start': 3771.72, 'end': 3773.72, 'text': ' And we can also use the cost function', 'video_name': 'Kint3'}, {'id': 1420, 'seek': 377372, 'start': 3783.72, 'end': 3786.72, 'text': ' And our cost function then is', 'video_name': 'Kint3'}, {'id': 1423, 'seek': 377372, 'start': 3791.72, 'end': 3793.72, 'text': ' And probably the...', 'video_name': 'Kint3'}, {'id': 1424, 'seek': 377372, 'start': 3793.72, 'end': 3795.72, 'text': ' And if we do classification,', 'video_name': 'Kint3'}, {'id': 1427, 'seek': 380372, 'start': 3803.72, 'end': 3805.72, 'text': ' And if we do gradient descent,', 'video_name': 'Kint3'}, {'id': 1434, 'seek': 380372, 'start': 3817.72, 'end': 3819.72, 'text': \" And that's the same thing for each parameter.\", 'video_name': 'Kint3'}, {'id': 1439, 'seek': 380372, 'start': 3829.72, 'end': 3831.72, 'text': ' And having more layers and more parameters', 'video_name': 'Kint3'}, {'id': 1466, 'seek': 389272, 'start': 3906.72, 'end': 3909.72, 'text': ' And getting the second activations,', 'video_name': 'Kint3'}, {'id': 1469, 'seek': 389272, 'start': 3913.72, 'end': 3915.72, 'text': ' And the activation function over here', 'video_name': 'Kint3'}, {'id': 1482, 'seek': 395272, 'start': 3952.72, 'end': 3955.72, 'text': ' And we probably want to do everything', 'video_name': 'Kint3'}, {'id': 1486, 'seek': 395272, 'start': 3962.72, 'end': 3965.72, 'text': ' And then we also get like a matrix,', 'video_name': 'Kint3'}, {'id': 1502, 'seek': 398172, 'start': 4007.72, 'end': 4009.72, 'text': ' And as an output,', 'video_name': 'Kint3'}, {'id': 1519, 'seek': 404072, 'start': 4053.72, 'end': 4055.72, 'text': \" And so I'll write it down\", 'video_name': 'Kint3'}, {'id': 1524, 'seek': 404072, 'start': 4064.72, 'end': 4066.72, 'text': \" And it's even...\", 'video_name': 'Kint3'}, {'id': 1553, 'seek': 412872, 'start': 4150.72, 'end': 4152.72, 'text': ' And...', 'video_name': 'Kint3'}, {'id': 1558, 'seek': 415872, 'start': 4162.72, 'end': 4164.72, 'text': ' And we get the gradient that we got from over here.', 'video_name': 'Kint3'}, {'id': 1581, 'seek': 421672, 'start': 4216.72, 'end': 4218.72, 'text': ' And...', 'video_name': 'Kint3'}, {'id': 1592, 'seek': 421672, 'start': 4241.72, 'end': 4244.72, 'text': ' And each of those need to be multiplied', 'video_name': 'Kint3'}, {'id': 1594, 'seek': 424672, 'start': 4246.72, 'end': 4248.72, 'text': ' And we will need to add them up', 'video_name': 'Kint3'}, {'id': 1597, 'seek': 424672, 'start': 4251.72, 'end': 4252.72, 'text': ' And this is kind of what happens', 'video_name': 'Kint3'}, {'id': 1631, 'seek': 430572, 'start': 4324.72, 'end': 4326.72, 'text': \" And now it's the gradient\", 'video_name': 'Kint3'}, {'id': 1660, 'seek': 436472, 'start': 4381.72, 'end': 4382.72, 'text': ' And in this case,', 'video_name': 'Kint3'}, {'id': 1796, 'seek': 465772, 'start': 4685.72, 'end': 4686.72, 'text': ' And...', 'video_name': 'Kint3'}, {'id': 1800, 'seek': 468772, 'start': 4690.72, 'end': 4691.72, 'text': ' And...', 'video_name': 'Kint3'}, {'id': 1897, 'seek': 486172, 'start': 4875.72, 'end': 4877.72, 'text': ' And the easiest way to do that', 'video_name': 'Kint3'}, {'id': 1965, 'seek': 500872, 'start': 5011.72, 'end': 5014.72, 'text': ' And have some direction to start with.', 'video_name': 'Kint3'}, {'id': 1967, 'seek': 500872, 'start': 5016.72, 'end': 5018.72, 'text': ' And one of the reasons for this', 'video_name': 'Kint3'}, {'id': 1971, 'seek': 500872, 'start': 5023.72, 'end': 5024.72, 'text': ' And if you think about', 'video_name': 'Kint3'}, {'id': 1990, 'seek': 503772, 'start': 5055.72, 'end': 5057.72, 'text': ' And...', 'video_name': 'Kint3'}, {'id': 2014, 'seek': 509572, 'start': 5101.72, 'end': 5102.72, 'text': ' And there is...', 'video_name': 'Kint3'}, {'id': 2016, 'seek': 509572, 'start': 5104.72, 'end': 5106.72, 'text': ' And there can be a lot of issues', 'video_name': 'Kint3'}, {'id': 2018, 'seek': 509572, 'start': 5108.72, 'end': 5110.72, 'text': ' And if you, like, start again...', 'video_name': 'Kint3'}, {'id': 2096, 'seek': 521272, 'start': 5228.72, 'end': 5229.72, 'text': ' And see you on Friday.', 'video_name': 'Kint3'}, {'id': 3, 'seek': 2488, 'start': 24.88, 'end': 30.24, 'text': ' And I will upload exercises here and the solutions for exercises.', 'video_name': 'Kint'}, {'id': 69, 'seek': 35108, 'start': 353.08, 'end': 357.08, 'text': ' And', 'video_name': 'Kint'}, {'id': 82, 'seek': 38108, 'start': 387.58, 'end': 391.21999999999997, 'text': ' And that can be a nice thing to have those.', 'video_name': 'Kint'}, {'id': 94, 'seek': 43966, 'start': 442.54, 'end': 447.6, 'text': ' And Andrew Eng has put up a lot of good learning resources as well and learning videos.', 'video_name': 'Kint'}, {'id': 95, 'seek': 43966, 'start': 447.88000000000005, 'end': 451.20000000000005, 'text': \" And I'm following along a lot of his course material.\", 'video_name': 'Kint'}, {'id': 99, 'seek': 43966, 'start': 466.22, 'end': 469.52000000000004, 'text': ' And the main focus of this course will be...', 'video_name': 'Kint'}, {'id': 105, 'seek': 46966, 'start': 488.70000000000005, 'end': 494.68, 'text': \" And there's a lot of gold rush fever around deep learning topics.\", 'video_name': 'Kint'}, {'id': 108, 'seek': 49960, 'start': 499.6, 'end': 506.92, 'text': \" And there's a lot of other techniques and algorithms that are also incredibly useful\", 'video_name': 'Kint'}, {'id': 119, 'seek': 52960, 'start': 547.4200000000001, 'end': 552.4200000000001, 'text': \" And it's first mentioned in this book...\", 'video_name': 'Kint'}, {'id': 121, 'seek': 52960, 'start': 558.94, 'end': 559.52, 'text': ' And...', 'video_name': 'Kint'}, {'id': 122, 'seek': 52960, 'start': 559.52, 'end': 559.58, 'text': ' And...', 'video_name': 'Kint'}, {'id': 127, 'seek': 55958, 'start': 570.5600000000001, 'end': 581.22, 'text': ' And the perceptron was a machine that could do classification tasks by learning from data.', 'video_name': 'Kint'}, {'id': 128, 'seek': 55958, 'start': 581.74, 'end': 589.5600000000001, 'text': ' And the first instance of this was basically a machine for this thing.', 'video_name': 'Kint'}, {'id': 129, 'seek': 58958, 'start': 589.58, 'end': 592.1600000000001, 'text': \" And now that machines can write number of41 doesn't have this specific purpose.\", 'video_name': 'Kint'}, {'id': 143, 'seek': 64652, 'start': 662.84, 'end': 674.02, 'text': ' And this report led to a lot of funding for AI research being frozen and a lot of research being discontinued.', 'video_name': 'Kint'}, {'id': 150, 'seek': 67652, 'start': 700.0, 'end': 703.5, 'text': \" And yeah, which doesn't say anything about two layer neural networks.\", 'video_name': 'Kint'}, {'id': 151, 'seek': 67652, 'start': 703.5, 'end': 705.62, 'text': ' And so kind of...', 'video_name': 'Kint'}, {'id': 155, 'seek': 70652, 'start': 715.3, 'end': 722.6, 'text': ' And back then, we got the first proper industrial uses of neural networks.', 'video_name': 'Kint'}, {'id': 161, 'seek': 73652, 'start': 752.1, 'end': 755.86, 'text': ' And thereby kind of read in the zip code automatically.', 'video_name': 'Kint'}, {'id': 167, 'seek': 76580, 'start': 781.14, 'end': 785.5799999999999, 'text': ' And that was sufficient data for training all those algorithms.', 'video_name': 'Kint'}, {'id': 168, 'seek': 76580, 'start': 786.0799999999999, 'end': 789.5999999999999, 'text': ' And the compute power to really run big neural networks.', 'video_name': 'Kint'}, {'id': 169, 'seek': 76580, 'start': 789.5999999999999, 'end': 795.18, 'text': ' And so interest all died down again.', 'video_name': 'Kint'}, {'id': 170, 'seek': 76580, 'start': 795.4799999999999, 'end': 795.5999999999999, 'text': ' And it...', 'video_name': 'Kint'}, {'id': 178, 'seek': 82580, 'start': 826.8399999999999, 'end': 828.38, 'text': ' And it turned out that...', 'video_name': 'Kint'}, {'id': 179, 'seek': 82580, 'start': 828.38, 'end': 838.54, 'text': ' And they put out their movie recommendation data set for everybody to use and to fine tune their algorithms.', 'video_name': 'Kint'}, {'id': 180, 'seek': 82580, 'start': 838.8, 'end': 840.1999999999999, 'text': ' And that was a pretty big thing.', 'video_name': 'Kint'}, {'id': 182, 'seek': 82580, 'start': 849.3199999999999, 'end': 850.4399999999999, 'text': ' And to do research on it.', 'video_name': 'Kint'}, {'id': 183, 'seek': 82580, 'start': 850.56, 'end': 852.9599999999999, 'text': ' And probably the price money was net.', 'video_name': 'Kint'}, {'id': 184, 'seek': 82580, 'start': 853.04, 'end': 855.14, 'text': ' And the price itself was also a nice thing.', 'video_name': 'Kint'}, {'id': 189, 'seek': 85568, 'start': 871.5999999999999, 'end': 872.16, 'text': ' And...', 'video_name': 'Kint'}, {'id': 193, 'seek': 88568, 'start': 885.68, 'end': 887.06, 'text': ' And...', 'video_name': 'Kint'}, {'id': 198, 'seek': 88568, 'start': 899.4399999999999, 'end': 899.92, 'text': ' And...', 'video_name': 'Kint'}, {'id': 204, 'seek': 88568, 'start': 913.4399999999999, 'end': 914.68, 'text': ' And in the 2010s...', 'video_name': 'Kint'}, {'id': 206, 'seek': 91552, 'start': 915.52, 'end': 916.86, 'text': ' And...', 'video_name': 'Kint'}, {'id': 215, 'seek': 91552, 'start': 938.9399999999999, 'end': 944.12, 'text': ' And that basically led to the deep learning boom that lasts till today.', 'video_name': 'Kint'}, {'id': 297, 'seek': 112090, 'start': 1142.92, 'end': 1147.4, 'text': ' And this shape determines the properties that the protein has.', 'video_name': 'Kint'}, {'id': 298, 'seek': 114740, 'start': 1147.4, 'end': 1152.3200000000002, 'text': \" And it's not trivial to know, if you just have the string of molecules,\", 'video_name': 'Kint'}, {'id': 303, 'seek': 114740, 'start': 1174.2800000000002, 'end': 1176.48, 'text': ' And a team from...', 'video_name': 'Kint'}, {'id': 348, 'seek': 132632, 'start': 1345.32, 'end': 1348.32, 'text': \" And if we don't have the data to learn from\", 'video_name': 'Kint'}, {'id': 391, 'seek': 147532, 'start': 1490.32, 'end': 1493.32, 'text': \" And it's all already available in digital form\", 'video_name': 'Kint'}, {'id': 403, 'seek': 153032, 'start': 1530.32, 'end': 1538.32, 'text': ' And so as long as you have only little data available,', 'video_name': 'Kint'}, {'id': 417, 'seek': 156032, 'start': 1584.32, 'end': 1586.32, 'text': ' And that is basically...', 'video_name': 'Kint'}, {'id': 435, 'seek': 162032, 'start': 1639.32, 'end': 1643.32, 'text': ' And two terabytes is an enormous amount of text data.', 'video_name': 'Kint'}, {'id': 451, 'seek': 167832, 'start': 1690.32, 'end': 1694.32, 'text': ' And to do that, we need kind of deep learning...', 'video_name': 'Kint'}, {'id': 473, 'seek': 173732, 'start': 1746.32, 'end': 1748.32, 'text': ' And if you have no way to acquire that,', 'video_name': 'Kint'}, {'id': 488, 'seek': 176732, 'start': 1788.32, 'end': 1790.32, 'text': ' And basically,', 'video_name': 'Kint'}, {'id': 495, 'seek': 179732, 'start': 1803.32, 'end': 1806.32, 'text': ' And if you think about managing', 'video_name': 'Kint'}, {'id': 507, 'seek': 182532, 'start': 1839.32, 'end': 1842.32, 'text': ' And an important part is that you kind of...', 'video_name': 'Kint'}, {'id': 514, 'seek': 185332, 'start': 1860.32, 'end': 1862.32, 'text': ' And if you think about it,', 'video_name': 'Kint'}, {'id': 535, 'seek': 188232, 'start': 1905.32, 'end': 1909.32, 'text': ' And they also basically started at this point.', 'video_name': 'Kint'}, {'id': 549, 'seek': 191132, 'start': 1939.32, 'end': 1940.32, 'text': \" And they didn't even...\", 'video_name': 'Kint'}, {'id': 553, 'seek': 194032, 'start': 1944.32, 'end': 1945.32, 'text': ' And you have the same', 'video_name': 'Kint'}, {'id': 563, 'seek': 194032, 'start': 1966.32, 'end': 1968.32, 'text': ' And a lot of the big industry companies nowadays,', 'video_name': 'Kint'}, {'id': 667, 'seek': 214432, 'start': 2169.32, 'end': 2170.32, 'text': ' And...', 'video_name': 'Kint'}, {'id': 676, 'seek': 217332, 'start': 2183.32, 'end': 2185.32, 'text': ' And...', 'video_name': 'Kint'}, {'id': 712, 'seek': 223232, 'start': 2248.32, 'end': 2249.32, 'text': ' And...', 'video_name': 'Kint'}, {'id': 729, 'seek': 226132, 'start': 2273.32, 'end': 2274.32, 'text': ' And so...', 'video_name': 'Kint'}, {'id': 738, 'seek': 226132, 'start': 2285.32, 'end': 2286.32, 'text': ' And...', 'video_name': 'Kint'}, {'id': 744, 'seek': 229032, 'start': 2293.32, 'end': 2294.32, 'text': ' And so...', 'video_name': 'Kint'}, {'id': 847, 'seek': 252632, 'start': 2526.32, 'end': 2533.32, 'text': ' And...', 'video_name': 'Kint'}, {'id': 941, 'seek': 267232, 'start': 2679.28, 'end': 2687.6400000000003, 'text': ' And one of the easiest ways to do that is do linear regression.', 'video_name': 'Kint'}, {'id': 944, 'seek': 270232, 'start': 2702.32, 'end': 2707.7400000000002, 'text': ' And which has two free parameters, W0 and W1.', 'video_name': 'Kint'}, {'id': 945, 'seek': 270232, 'start': 2708.6400000000003, 'end': 2713.84, 'text': ' And once I know those two parameters,', 'video_name': 'Kint'}, {'id': 951, 'seek': 273232, 'start': 2740.02, 'end': 2742.1800000000003, 'text': ' And those are the two parameters that I want to learn.', 'video_name': 'Kint'}, {'id': 952, 'seek': 273232, 'start': 2742.1800000000003, 'end': 2745.38, 'text': ' And given I have them, I have my entire model H.', 'video_name': 'Kint'}, {'id': 953, 'seek': 273232, 'start': 2745.9, 'end': 2748.4, 'text': ' And that model will give me a price for the house.', 'video_name': 'Kint'}, {'id': 954, 'seek': 273232, 'start': 2756.56, 'end': 2761.04, 'text': ' And what we want to do is,', 'video_name': 'Kint'}, {'id': 956, 'seek': 276232, 'start': 2764.32, 'end': 2769.82, 'text': ' And this kind of linear model is kind of one of the earliest things', 'video_name': 'Kint'}, {'id': 960, 'seek': 276232, 'start': 2785.5, 'end': 2792.1000000000004, 'text': ' And what we would want to do is calculate those parameters,', 'video_name': 'Kint'}, {'id': 971, 'seek': 282232, 'start': 2837.32, 'end': 2840.32, 'text': ' And that is kind of very, very obviously wrong.', 'video_name': 'Kint'}, {'id': 974, 'seek': 282232, 'start': 2844.92, 'end': 2846.32, 'text': ' And that model is,', 'video_name': 'Kint'}, {'id': 980, 'seek': 285112, 'start': 2863.12, 'end': 2865.12, 'text': \" And if it's below zero, we just take zero.\", 'video_name': 'Kint'}, {'id': 982, 'seek': 285112, 'start': 2871.12, 'end': 2876.12, 'text': ' And this is probably a better predictor than the one we had before.', 'video_name': 'Kint'}, {'id': 985, 'seek': 288012, 'start': 2882.12, 'end': 2887.12, 'text': ' And that makes things at least a little better than it was before.', 'video_name': 'Kint'}, {'id': 988, 'seek': 288012, 'start': 2897.12, 'end': 2903.12, 'text': ' And what we basically did here was we created a very, very small neural network.', 'video_name': 'Kint'}, {'id': 1012, 'seek': 294012, 'start': 2968.12, 'end': 2969.12, 'text': ' And this is...', 'video_name': 'Kint'}, {'id': 1030, 'seek': 299812, 'start': 3023.12, 'end': 3025.12, 'text': ' And this something on top of...', 'video_name': 'Kint'}, {'id': 1035, 'seek': 302712, 'start': 3035.12, 'end': 3039.12, 'text': ' And in this case we use the maximum of...', 'video_name': 'Kint'}, {'id': 1037, 'seek': 302712, 'start': 3042.12, 'end': 3043.12, 'text': ' And say,', 'video_name': 'Kint'}, {'id': 1041, 'seek': 302712, 'start': 3049.12, 'end': 3050.12, 'text': ' And...', 'video_name': 'Kint'}, {'id': 1050, 'seek': 305612, 'start': 3068.12, 'end': 3069.12, 'text': ' And...', 'video_name': 'Kint'}, {'id': 1054, 'seek': 305612, 'start': 3074.12, 'end': 3076.12, 'text': ' And this...', 'video_name': 'Kint'}, {'id': 1063, 'seek': 308612, 'start': 3100.12, 'end': 3101.12, 'text': ' And to do better...', 'video_name': 'Kint'}, {'id': 1069, 'seek': 308612, 'start': 3113.12, 'end': 3114.12, 'text': ' And...', 'video_name': 'Kint'}, {'id': 1081, 'seek': 311612, 'start': 3135.12, 'end': 3137.12, 'text': ' And...', 'video_name': 'Kint'}, {'id': 1096, 'seek': 314612, 'start': 3158.12, 'end': 3161.12, 'text': ' And it could predict that from the size and the number of bedrooms.', 'video_name': 'Kint'}, {'id': 1097, 'seek': 314612, 'start': 3161.12, 'end': 3162.12, 'text': ' And this neuron...', 'video_name': 'Kint'}, {'id': 1101, 'seek': 314612, 'start': 3172.12, 'end': 3173.12, 'text': ' And it might be...', 'video_name': 'Kint'}, {'id': 1113, 'seek': 317512, 'start': 3202.12, 'end': 3203.12, 'text': ' And the next neuron...', 'video_name': 'Kint'}, {'id': 1116, 'seek': 320412, 'start': 3211.12, 'end': 3213.12, 'text': ' And that is basically what...', 'video_name': 'Kint'}, {'id': 1122, 'seek': 320412, 'start': 3226.12, 'end': 3228.12, 'text': ' And gives those...', 'video_name': 'Kint'}, {'id': 1128, 'seek': 323312, 'start': 3240.12, 'end': 3241.12, 'text': ' And those...', 'video_name': 'Kint'}, {'id': 1138, 'seek': 323312, 'start': 3261.12, 'end': 3262.12, 'text': ' And...', 'video_name': 'Kint'}, {'id': 1156, 'seek': 329212, 'start': 3303.12, 'end': 3307.12, 'text': ' And then it will kind of train those neurons...', 'video_name': 'Kint'}, {'id': 1165, 'seek': 332212, 'start': 3327.12, 'end': 3329.12, 'text': ' And if you stack on more layers...', 'video_name': 'Kint'}, {'id': 1171, 'seek': 332212, 'start': 3341.12, 'end': 3344.12, 'text': ' And our training algorithm will...', 'video_name': 'Kint'}, {'id': 1225, 'seek': 344112, 'start': 3466.12, 'end': 3468.12, 'text': ' And we had one output variable...', 'video_name': 'Kint'}, {'id': 1235, 'seek': 347012, 'start': 3486.12, 'end': 3488.12, 'text': ' And as an output we want to have...', 'video_name': 'Kint'}, {'id': 1257, 'seek': 352912, 'start': 3537.12, 'end': 3539.12, 'text': ' And our output could be...', 'video_name': 'Kint'}, {'id': 1260, 'seek': 352912, 'start': 3543.12, 'end': 3545.12, 'text': ' And we want to kind of do...', 'video_name': 'Kint'}, {'id': 1271, 'seek': 355512, 'start': 3567.12, 'end': 3569.12, 'text': ' And we want to output the position...', 'video_name': 'Kint'}, {'id': 1285, 'seek': 358512, 'start': 3595.12, 'end': 3597.12, 'text': ' And what should be the prediction...', 'video_name': 'Kint'}, {'id': 1290, 'seek': 358512, 'start': 3605.12, 'end': 3607.12, 'text': ' And in some cases...', 'video_name': 'Kint'}, {'id': 1318, 'seek': 364512, 'start': 3661.12, 'end': 3663.12, 'text': ' And...', 'video_name': 'Kint'}, {'id': 1341, 'seek': 370512, 'start': 3709.12, 'end': 3711.12, 'text': ' And...', 'video_name': 'Kint'}, {'id': 1373, 'seek': 376512, 'start': 3775.12, 'end': 3777.12, 'text': ' And this will be...', 'video_name': 'Kint'}, {'id': 11, 'seek': 5258, 'start': 67.32, 'end': 76.3, 'text': ' And this object can be turned into one long vector by just stacking those matrices.', 'video_name': 'Kint2'}, {'id': 14, 'seek': 8212, 'start': 89.4, 'end': 98.52000000000001, 'text': ' And this way we kind of get one vector with all the input data for this one image.', 'video_name': 'Kint2'}, {'id': 31, 'seek': 14140, 'start': 151.36, 'end': 155.44, 'text': ' And another problem is that we might want to make sure', 'video_name': 'Kint2'}, {'id': 35, 'seek': 14140, 'start': 163.76, 'end': 167.92000000000002, 'text': ' And like a rule of thumb is to make sure that we rescale it', 'video_name': 'Kint2'}, {'id': 44, 'seek': 17012, 'start': 192.74, 'end': 196.06, 'text': ' And if we start experimenting at this point,', 'video_name': 'Kint2'}, {'id': 49, 'seek': 19966, 'start': 207.34, 'end': 209.5, 'text': ' And we can kind of start experimenting from here', 'video_name': 'Kint2'}, {'id': 88, 'seek': 28966, 'start': 303.12, 'end': 307.0, 'text': ' And if we have a binary output,', 'video_name': 'Kint2'}, {'id': 174, 'seek': 43464, 'start': 434.64, 'end': 441.64, 'text': ' And this probability interpretation has some huge advantages in a lot of cases.', 'video_name': 'Kint2'}, {'id': 202, 'seek': 46364, 'start': 486.64, 'end': 489.64, 'text': ' And what I want to predict is,', 'video_name': 'Kint2'}, {'id': 231, 'seek': 52164, 'start': 529.64, 'end': 530.64, 'text': ' And,', 'video_name': 'Kint2'}, {'id': 237, 'seek': 52164, 'start': 542.64, 'end': 543.64, 'text': ' And in this case,', 'video_name': 'Kint2'}, {'id': 272, 'seek': 57964, 'start': 591.64, 'end': 596.64, 'text': ' And the information that you have might only give you like a statistical information.', 'video_name': 'Kint2'}, {'id': 273, 'seek': 57964, 'start': 596.64, 'end': 597.64, 'text': \" And that's why we,\", 'video_name': 'Kint2'}, {'id': 283, 'seek': 60864, 'start': 615.64, 'end': 619.64, 'text': ' And a good classifier should have a very high confidence over here.', 'video_name': 'Kint2'}, {'id': 318, 'seek': 63764, 'start': 654.64, 'end': 655.64, 'text': ' And this probability that we want to predict over here will be,', 'video_name': 'Kint2'}, {'id': 376, 'seek': 72506, 'start': 737.14, 'end': 739.6999999999999, 'text': ' And we take some kind of function of this.', 'video_name': 'Kint2'}, {'id': 377, 'seek': 72506, 'start': 739.9, 'end': 742.3399999999999, 'text': ' And this function is called the sigmoid.', 'video_name': 'Kint2'}, {'id': 381, 'seek': 72506, 'start': 753.2199999999999, 'end': 754.66, 'text': ' And so this is hard to,', 'video_name': 'Kint2'}, {'id': 384, 'seek': 75506, 'start': 757.92, 'end': 759.68, 'text': ' And the way this,', 'video_name': 'Kint2'}, {'id': 388, 'seek': 75506, 'start': 773.06, 'end': 773.78, 'text': ' And as I said,', 'video_name': 'Kint2'}, {'id': 409, 'seek': 81416, 'start': 816.76, 'end': 819.9399999999999, 'text': ' And depending on,', 'video_name': 'Kint2'}, {'id': 429, 'seek': 84400, 'start': 868.88, 'end': 870.52, 'text': ' And if that gets very large,', 'video_name': 'Kint2'}, {'id': 433, 'seek': 87364, 'start': 876.24, 'end': 877.66, 'text': ' And the other way around,', 'video_name': 'Kint2'}, {'id': 438, 'seek': 87364, 'start': 888.76, 'end': 889.78, 'text': \" And if it's close to one,\", 'video_name': 'Kint2'}, {'id': 444, 'seek': 87364, 'start': 902.24, 'end': 903.64, 'text': ' And that is kind of a nice property.', 'video_name': 'Kint2'}, {'id': 453, 'seek': 90364, 'start': 920.74, 'end': 922.28, 'text': ' And the probability of less than zero', 'video_name': 'Kint2'}, {'id': 469, 'seek': 96250, 'start': 968.04, 'end': 970.88, 'text': ' And if we get different probabilities over here,', 'video_name': 'Kint2'}, {'id': 472, 'seek': 96250, 'start': 978.38, 'end': 980.64, 'text': ' And we have to kind of evaluate', 'video_name': 'Kint2'}, {'id': 488, 'seek': 102250, 'start': 1029.82, 'end': 1033.26, 'text': ' And what we want to have is that this prediction', 'video_name': 'Kint2'}, {'id': 493, 'seek': 102250, 'start': 1050.18, 'end': 1052.2, 'text': ' And this loss function,', 'video_name': 'Kint2'}, {'id': 496, 'seek': 105220, 'start': 1061.22, 'end': 1065.56, 'text': \" And so let's look at what this does.\", 'video_name': 'Kint2'}, {'id': 547, 'seek': 125422, 'start': 1268.9, 'end': 1274.72, 'text': ' And that means, so it gets, the logarithm over here will be some very small, very negative number.', 'video_name': 'Kint2'}, {'id': 551, 'seek': 128422, 'start': 1286.22, 'end': 1288.22, 'text': ' And I have the same thing the other way around.', 'video_name': 'Kint2'}, {'id': 553, 'seek': 128422, 'start': 1298.22, 'end': 1306.22, 'text': \" And I have one over here, so what I'm looking at is the logarithm of one minus my prediction.\", 'video_name': 'Kint2'}, {'id': 554, 'seek': 128422, 'start': 1308.22, 'end': 1314.22, 'text': ' And if my prediction is also close to zero, then the logarithm over here will be zero.', 'video_name': 'Kint2'}, {'id': 556, 'seek': 131422, 'start': 1321.22, 'end': 1331.02, 'text': ' And if my prediction is far off, then the logarithm over here gets more and more negative, and my loss function increases again.', 'video_name': 'Kint2'}, {'id': 573, 'seek': 140322, 'start': 1427.22, 'end': 1431.22, 'text': ' And that would be the loss function for linear regression.', 'video_name': 'Kint2'}, {'id': 581, 'seek': 143222, 'start': 1457.22, 'end': 1461.22, 'text': ' And for classification, this logistic loss over here,', 'video_name': 'Kint2'}, {'id': 583, 'seek': 146222, 'start': 1466.22, 'end': 1473.22, 'text': ' And using this loss function has another advantage.', 'video_name': 'Kint2'}, {'id': 584, 'seek': 146222, 'start': 1473.22, 'end': 1486.22, 'text': ' And that is, this loss over here, which is also called the cross entropy loss,', 'video_name': 'Kint2'}, {'id': 597, 'seek': 154822, 'start': 1548.22, 'end': 1551.22, 'text': ' And this property over here holds,', 'video_name': 'Kint2'}, {'id': 609, 'seek': 157722, 'start': 1603.22, 'end': 1606.22, 'text': ' And this gives us the', 'video_name': 'Kint2'}, {'id': 624, 'seek': 163722, 'start': 1656.22, 'end': 1661.22, 'text': ' And we said, okay, the hypothesis is sigmoid of w,', 'video_name': 'Kint2'}, {'id': 632, 'seek': 166622, 'start': 1685.22, 'end': 1691.22, 'text': ' And the way to get them is we want to find w and b', 'video_name': 'Kint2'}, {'id': 638, 'seek': 169522, 'start': 1713.22, 'end': 1721.22, 'text': ' And when we do logistic regression,', 'video_name': 'Kint2'}, {'id': 646, 'seek': 175322, 'start': 1753.22, 'end': 1755.22, 'text': ' And that is gradient descent.', 'video_name': 'Kint2'}, {'id': 655, 'seek': 178322, 'start': 1783.22, 'end': 1786.22, 'text': ' And if we want to minimize it,', 'video_name': 'Kint2'}, {'id': 673, 'seek': 184122, 'start': 1850.22, 'end': 1857.22, 'text': ' And this technique will keep working even for more complicated hypotheses.', 'video_name': 'Kint2'}, {'id': 713, 'seek': 198922, 'start': 1997.22, 'end': 2001.22, 'text': ' And this works all nicely as long as you have only one input variable.', 'video_name': 'Kint2'}, {'id': 718, 'seek': 198922, 'start': 2010.22, 'end': 2011.22, 'text': ' And what we want,', 'video_name': 'Kint2'}, {'id': 721, 'seek': 198922, 'start': 2017.22, 'end': 2019.22, 'text': ' And for each of the input variables,', 'video_name': 'Kint2'}, {'id': 787, 'seek': 216322, 'start': 2186.22, 'end': 2188.22, 'text': ' And the gradient kind of tells us', 'video_name': 'Kint2'}, {'id': 1253, 'seek': 312660, 'start': 3153.86, 'end': 3156.6, 'text': ' And the same goes for the input B.', 'video_name': 'Kint2'}, {'id': 1256, 'seek': 315660, 'start': 3178.6, 'end': 3183.86, 'text': ' And we can do that for all kinds of primitive mathematical operations and from those we can create complex functions.', 'video_name': 'Kint2'}, {'id': 1257, 'seek': 315660, 'start': 3183.86, 'end': 3186.36, 'text': ' And we can do that for all kinds of primitive mathematical operations and from those we can create complex functions.', 'video_name': 'Kint2'}, {'id': 1260, 'seek': 318660, 'start': 3200.98, 'end': 3206.58, 'text': ' And if I do that I can say okay this will be this compute graph over here.', 'video_name': 'Kint2'}, {'id': 1264, 'seek': 321660, 'start': 3222.02, 'end': 3226.72, 'text': ' And how do I calculate the derivative over here?', 'video_name': 'Kint2'}, {'id': 1265, 'seek': 321660, 'start': 3226.72, 'end': 3228.06, 'text': ' And the way to do that is,', 'video_name': 'Kint2'}, {'id': 1269, 'seek': 321660, 'start': 3243.22, 'end': 3246.04, 'text': ' And the chain rule basically says,', 'video_name': 'Kint2'}, {'id': 1286, 'seek': 330654, 'start': 3311.36, 'end': 3313.68, 'text': ' And we divide it into two parts where we have', 'video_name': 'Kint2'}, {'id': 1288, 'seek': 330654, 'start': 3319.22, 'end': 3322.42, 'text': ' And then we do a step from here till here.', 'video_name': 'Kint2'}, {'id': 1290, 'seek': 330654, 'start': 3325.66, 'end': 3328.22, 'text': ' And for each of those smaller steps,', 'video_name': 'Kint2'}, {'id': 1302, 'seek': 333654, 'start': 3356.84, 'end': 3359.88, 'text': ' And so we know this one.', 'video_name': 'Kint2'}, {'id': 1303, 'seek': 333654, 'start': 3359.88, 'end': 3362.44, 'text': ' And if we look at this one, we can say,', 'video_name': 'Kint2'}, {'id': 1310, 'seek': 336244, 'start': 3383.6, 'end': 3388.6, 'text': ' And C is basically just the forward computation', 'video_name': 'Kint2'}, {'id': 1314, 'seek': 339244, 'start': 3400.06, 'end': 3401.82, 'text': ' And if we kind of multiply this out,', 'video_name': 'Kint2'}, {'id': 1324, 'seek': 342244, 'start': 3439.32, 'end': 3442.06, 'text': ' And the nice thing about this is,', 'video_name': 'Kint2'}, {'id': 1334, 'seek': 345224, 'start': 3469.14, 'end': 3472.08, 'text': ' And at the end, you need to calculate', 'video_name': 'Kint2'}, {'id': 1336, 'seek': 345224, 'start': 3474.8799999999997, 'end': 3479.8799999999997, 'text': ' And this way you get the entire derivative all the way up.', 'video_name': 'Kint2'}, {'id': 1341, 'seek': 348224, 'start': 3486.3599999999997, 'end': 3490.54, 'text': ' And the algorithm is that for each of the compute nodes,', 'video_name': 'Kint2'}, {'id': 1351, 'seek': 351224, 'start': 3517.14, 'end': 3521.56, 'text': ' And the backward values will be the gradients', 'video_name': 'Kint2'}, {'id': 1360, 'seek': 354212, 'start': 3546.42, 'end': 3550.96, 'text': ' And then we start at the root node of the compute graph', 'video_name': 'Kint2'}, {'id': 1371, 'seek': 357210, 'start': 3587.2999999999997, 'end': 3591.94, 'text': ' And z is defined as some linear operation', 'video_name': 'Kint2'}, {'id': 1376, 'seek': 360110, 'start': 3601.1, 'end': 3603.44, 'text': ' And these three are the input parameters.', 'video_name': 'Kint2'}, {'id': 1384, 'seek': 362972, 'start': 3629.72, 'end': 3632.62, 'text': ' And so we can use that as a single node', 'video_name': 'Kint2'}, {'id': 1388, 'seek': 362972, 'start': 3640.3599999999997, 'end': 3642.98, 'text': ' And the derivative of the sigmoid function', 'video_name': 'Kint2'}, {'id': 1439, 'seek': 386096, 'start': 3866.58, 'end': 3870.68, 'text': \" And let's assume the weights that we currently have\", 'video_name': 'Kint2'}, {'id': 1441, 'seek': 386096, 'start': 3873.82, 'end': 3877.06, 'text': ' And there should be some value for b as well', 'video_name': 'Kint2'}, {'id': 1453, 'seek': 392096, 'start': 3920.96, 'end': 3922.96, 'text': ' And then we put into the sigmoid function.', 'video_name': 'Kint2'}, {'id': 1454, 'seek': 392096, 'start': 3922.96, 'end': 3926.96, 'text': ' And the result of that will be compared', 'video_name': 'Kint2'}, {'id': 1464, 'seek': 395096, 'start': 3954.96, 'end': 3959.96, 'text': ' And these are the derivatives at each of the branch', 'video_name': 'Kint2'}, {'id': 1473, 'seek': 398096, 'start': 3992.96, 'end': 3997.96, 'text': ' And again, w2 is 1, x2 is 3,', 'video_name': 'Kint2'}, {'id': 1489, 'seek': 403996, 'start': 4039.96, 'end': 4048.96, 'text': ' And the sigmoid of this turns out to be 0.378.', 'video_name': 'Kint2'}, {'id': 1500, 'seek': 406996, 'start': 4081.96, 'end': 4085.96, 'text': ' And our loss function will now be the cross entropy', 'video_name': 'Kint2'}, {'id': 1508, 'seek': 409896, 'start': 4109.96, 'end': 4111.96, 'text': ' And the smaller we get over here,', 'video_name': 'Kint2'}, {'id': 1510, 'seek': 409896, 'start': 4113.96, 'end': 4119.96, 'text': ' And what we need to do now is calculate the derivatives backward.', 'video_name': 'Kint2'}, {'id': 1511, 'seek': 409896, 'start': 4119.96, 'end': 4121.96, 'text': ' And how do we do that?', 'video_name': 'Kint2'}, {'id': 1523, 'seek': 412896, 'start': 4156.96, 'end': 4158.96, 'text': ' And that basically says us,', 'video_name': 'Kint2'}, {'id': 1548, 'seek': 421696, 'start': 4228.96, 'end': 4233.96, 'text': ' And this tells us that if we want to,', 'video_name': 'Kint2'}, {'id': 1550, 'seek': 421696, 'start': 4240.96, 'end': 4244.96, 'text': ' And which again makes sense', 'video_name': 'Kint2'}, {'id': 1554, 'seek': 424596, 'start': 4251.96, 'end': 4254.96, 'text': ' And we kind of keep doing that now.', 'video_name': 'Kint2'}, {'id': 1562, 'seek': 424596, 'start': 4269.96, 'end': 4271.96, 'text': ' And finally,', 'video_name': 'Kint2'}, {'id': 1567, 'seek': 427596, 'start': 4278.96, 'end': 4280.96, 'text': ' And for this one,', 'video_name': 'Kint2'}, {'id': 1569, 'seek': 427596, 'start': 4282.96, 'end': 4284.96, 'text': ' And for the b,', 'video_name': 'Kint2'}, {'id': 1572, 'seek': 427596, 'start': 4288.96, 'end': 4290.96, 'text': ' And now comes the last step', 'video_name': 'Kint2'}, {'id': 1574, 'seek': 427596, 'start': 4292.96, 'end': 4296.96, 'text': ' And that formatting here is incredibly off.', 'video_name': 'Kint2'}, {'id': 1584, 'seek': 430596, 'start': 4329.96, 'end': 4334.96, 'text': ' And the same way in the true direction of w2,', 'video_name': 'Kint2'}, {'id': 1594, 'seek': 436396, 'start': 4363.96, 'end': 4368.96, 'text': ' And I should increase w2', 'video_name': 'Kint2'}, {'id': 1596, 'seek': 436396, 'start': 4370.96, 'end': 4371.96, 'text': ' And in the same way,', 'video_name': 'Kint2'}, {'id': 1602, 'seek': 436396, 'start': 4387.96, 'end': 4392.96, 'text': ' And,', 'video_name': 'Kint2'}, {'id': 1614, 'seek': 442196, 'start': 4423.96, 'end': 4425.96, 'text': ' And decreasing w1', 'video_name': 'Kint2'}, {'id': 1626, 'seek': 445096, 'start': 4459.96, 'end': 4463.96, 'text': ' And so it looks,', 'video_name': 'Kint2'}, {'id': 1632, 'seek': 447996, 'start': 4479.96, 'end': 4482.96, 'text': ' And then just multiplying up everything at the end', 'video_name': 'Kint2'}, {'id': 1655, 'seek': 453996, 'start': 4546.96, 'end': 4549.96, 'text': ' And if you multiply those,', 'video_name': 'Kint2'}, {'id': 1657, 'seek': 453996, 'start': 4554.96, 'end': 4559.96, 'text': ' And as you can kind of see that you have like this thing over here', 'video_name': 'Kint2'}, {'id': 1659, 'seek': 453996, 'start': 4561.96, 'end': 4563.96, 'text': ' And so you can kind of', 'video_name': 'Kint2'}, {'id': 1662, 'seek': 456896, 'start': 4568.96, 'end': 4570.96, 'text': ' And in total,', 'video_name': 'Kint2'}, {'id': 1686, 'seek': 459796, 'start': 4624.96, 'end': 4626.96, 'text': \" And it's just like linear into that\", 'video_name': 'Kint2'}, {'id': 1695, 'seek': 462696, 'start': 4652.96, 'end': 4655.96, 'text': \" And that's why we need to do this.\", 'video_name': 'Kint2'}, {'id': 1710, 'seek': 468596, 'start': 4696.96, 'end': 4704.96, 'text': ' And when doing any kind of calculations,', 'video_name': 'Kint2'}, {'id': 1715, 'seek': 471396, 'start': 4721.96, 'end': 4723.96, 'text': ' And if we have like more dimensions,', 'video_name': 'Kint2'}, {'id': 1774, 'seek': 483296, 'start': 4849.96, 'end': 4852.96, 'text': ' And for going this way,', 'video_name': 'Kint2'}, {'id': 1782, 'seek': 486196, 'start': 4866.96, 'end': 4871.96, 'text': ' And so if I want to decrease the prediction,', 'video_name': 'Kint2'}, {'id': 1791, 'seek': 489096, 'start': 4894.96, 'end': 4897.96, 'text': ' And that goes for the sum over here.', 'video_name': 'Kint2'}, {'id': 1795, 'seek': 489096, 'start': 4902.96, 'end': 4906.96, 'text': ' And the product over here should also be in a decreased.', 'video_name': 'Kint2'}, {'id': 1804, 'seek': 491996, 'start': 4919.96, 'end': 4921.96, 'text': ' And I also know by how much.', 'video_name': 'Kint2'}, {'id': 1812, 'seek': 491996, 'start': 4943.96, 'end': 4944.96, 'text': ' And at the very end,', 'video_name': 'Kint2'}, {'id': 1816, 'seek': 494996, 'start': 4952.96, 'end': 4953.96, 'text': ' And I,', 'video_name': 'Kint2'}, {'id': 1832, 'seek': 497896, 'start': 4984.96, 'end': 4985.96, 'text': ' And later on,', 'video_name': 'Kint2'}, {'id': 1844, 'seek': 500896, 'start': 5016.96, 'end': 5017.96, 'text': \" And if there's no more questions,\", 'video_name': 'Kint2'}]\n",
      "search_phrase: And that\n",
      "found_objects:  [{'id': 503, 'seek': 162094, 'start': 1644.94, 'end': 1648.94, 'text': \" And that's kind of the image that you need to keep in mind\", 'video_name': 'Kint3'}, {'id': 593, 'seek': 197394, 'start': 1988.94, 'end': 1995.94, 'text': \" And that's kind of the motivation over here to,\", 'video_name': 'Kint3'}, {'id': 675, 'seek': 220794, 'start': 2209.94, 'end': 2212.94, 'text': ' And that is the reason why,', 'video_name': 'Kint3'}, {'id': 717, 'seek': 229594, 'start': 2316.94, 'end': 2318.94, 'text': ' And that means,', 'video_name': 'Kint3'}, {'id': 722, 'seek': 232594, 'start': 2328.94, 'end': 2329.94, 'text': ' And that,', 'video_name': 'Kint3'}, {'id': 1218, 'seek': 323840, 'start': 3263.78, 'end': 3264.6, 'text': ' And that is part...', 'video_name': 'Kint3'}, {'id': 1306, 'seek': 347672, 'start': 3499.22, 'end': 3501.72, 'text': \" And that's kind of what...\", 'video_name': 'Kint3'}, {'id': 1434, 'seek': 380372, 'start': 3817.72, 'end': 3819.72, 'text': \" And that's the same thing for each parameter.\", 'video_name': 'Kint3'}, {'id': 82, 'seek': 38108, 'start': 387.58, 'end': 391.21999999999997, 'text': ' And that can be a nice thing to have those.', 'video_name': 'Kint'}, {'id': 167, 'seek': 76580, 'start': 781.14, 'end': 785.5799999999999, 'text': ' And that was sufficient data for training all those algorithms.', 'video_name': 'Kint'}, {'id': 180, 'seek': 82580, 'start': 838.8, 'end': 840.1999999999999, 'text': ' And that was a pretty big thing.', 'video_name': 'Kint'}, {'id': 215, 'seek': 91552, 'start': 938.9399999999999, 'end': 944.12, 'text': ' And that basically led to the deep learning boom that lasts till today.', 'video_name': 'Kint'}, {'id': 417, 'seek': 156032, 'start': 1584.32, 'end': 1586.32, 'text': ' And that is basically...', 'video_name': 'Kint'}, {'id': 953, 'seek': 273232, 'start': 2745.9, 'end': 2748.4, 'text': ' And that model will give me a price for the house.', 'video_name': 'Kint'}, {'id': 971, 'seek': 282232, 'start': 2837.32, 'end': 2840.32, 'text': ' And that is kind of very, very obviously wrong.', 'video_name': 'Kint'}, {'id': 974, 'seek': 282232, 'start': 2844.92, 'end': 2846.32, 'text': ' And that model is,', 'video_name': 'Kint'}, {'id': 985, 'seek': 288012, 'start': 2882.12, 'end': 2887.12, 'text': ' And that makes things at least a little better than it was before.', 'video_name': 'Kint'}, {'id': 1116, 'seek': 320412, 'start': 3211.12, 'end': 3213.12, 'text': ' And that is basically what...', 'video_name': 'Kint'}, {'id': 273, 'seek': 57964, 'start': 596.64, 'end': 597.64, 'text': \" And that's why we,\", 'video_name': 'Kint2'}, {'id': 444, 'seek': 87364, 'start': 902.24, 'end': 903.64, 'text': ' And that is kind of a nice property.', 'video_name': 'Kint2'}, {'id': 547, 'seek': 125422, 'start': 1268.9, 'end': 1274.72, 'text': ' And that means, so it gets, the logarithm over here will be some very small, very negative number.', 'video_name': 'Kint2'}, {'id': 573, 'seek': 140322, 'start': 1427.22, 'end': 1431.22, 'text': ' And that would be the loss function for linear regression.', 'video_name': 'Kint2'}, {'id': 584, 'seek': 146222, 'start': 1473.22, 'end': 1486.22, 'text': ' And that is, this loss over here, which is also called the cross entropy loss,', 'video_name': 'Kint2'}, {'id': 646, 'seek': 175322, 'start': 1753.22, 'end': 1755.22, 'text': ' And that is gradient descent.', 'video_name': 'Kint2'}, {'id': 1523, 'seek': 412896, 'start': 4156.96, 'end': 4158.96, 'text': ' And that basically says us,', 'video_name': 'Kint2'}, {'id': 1574, 'seek': 427596, 'start': 4292.96, 'end': 4296.96, 'text': ' And that formatting here is incredibly off.', 'video_name': 'Kint2'}, {'id': 1695, 'seek': 462696, 'start': 4652.96, 'end': 4655.96, 'text': \" And that's why we need to do this.\", 'video_name': 'Kint2'}, {'id': 1791, 'seek': 489096, 'start': 4894.96, 'end': 4897.96, 'text': ' And that goes for the sum over here.', 'video_name': 'Kint2'}]\n",
      "search_phrase: And that is\n",
      "found_objects:  [{'id': 675, 'seek': 220794, 'start': 2209.94, 'end': 2212.94, 'text': ' And that is the reason why,', 'video_name': 'Kint3'}, {'id': 1218, 'seek': 323840, 'start': 3263.78, 'end': 3264.6, 'text': ' And that is part...', 'video_name': 'Kint3'}, {'id': 417, 'seek': 156032, 'start': 1584.32, 'end': 1586.32, 'text': ' And that is basically...', 'video_name': 'Kint'}, {'id': 971, 'seek': 282232, 'start': 2837.32, 'end': 2840.32, 'text': ' And that is kind of very, very obviously wrong.', 'video_name': 'Kint'}, {'id': 1116, 'seek': 320412, 'start': 3211.12, 'end': 3213.12, 'text': ' And that is basically what...', 'video_name': 'Kint'}, {'id': 444, 'seek': 87364, 'start': 902.24, 'end': 903.64, 'text': ' And that is kind of a nice property.', 'video_name': 'Kint2'}, {'id': 584, 'seek': 146222, 'start': 1473.22, 'end': 1486.22, 'text': ' And that is, this loss over here, which is also called the cross entropy loss,', 'video_name': 'Kint2'}, {'id': 646, 'seek': 175322, 'start': 1753.22, 'end': 1755.22, 'text': ' And that is gradient descent.', 'video_name': 'Kint2'}]\n",
      "search_phrase: And that is gradient\n",
      "found_objects:  [{'id': 646, 'seek': 175322, 'start': 1753.22, 'end': 1755.22, 'text': ' And that is gradient descent.', 'video_name': 'Kint2'}]\n",
      "Found object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Mar/2024 09:05:55] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK SPLITTED words: ['one', 'of', 'the', 'color', 'channels']\n",
      "search_phrase: one\n",
      "found_objects:  [{'id': 8, 'seek': 2740, 'start': 31.7, 'end': 34.28, 'text': ' So we take one compute graph where', 'video_name': 'Kint3'}, {'id': 16, 'seek': 5738, 'start': 67.94, 'end': 74.18, 'text': ' of the input dimensions, and one bias term,', 'video_name': 'Kint3'}, {'id': 77, 'seek': 26640, 'start': 281.26, 'end': 285.64, 'text': ' is a one-layer neural network, works like this.', 'video_name': 'Kint3'}, {'id': 102, 'seek': 35636, 'start': 366.18, 'end': 376.58000000000004, 'text': ' So like this times this one over here will in the end just be y hat minus y.', 'video_name': 'Kint3'}, {'id': 110, 'seek': 41004, 'start': 420.68, 'end': 428.20000000000005, 'text': \" So the gradient of going towards... of addition, multiplication, it's always just taking one of the numbers.\", 'video_name': 'Kint3'}, {'id': 128, 'seek': 46966, 'start': 477.94, 'end': 481.16, 'text': ' everything here drops and this one goes away too.', 'video_name': 'Kint3'}, {'id': 191, 'seek': 67810, 'start': 681.22, 'end': 683.24, 'text': ' to determine which one works best.', 'video_name': 'Kint3'}, {'id': 195, 'seek': 67810, 'start': 691.22, 'end': 694.26, 'text': ' So if you, for example, want to predict one class,', 'video_name': 'Kint3'}, {'id': 198, 'seek': 67810, 'start': 700.96, 'end': 702.5600000000001, 'text': ' because that one gives you something', 'video_name': 'Kint3'}, {'id': 208, 'seek': 70808, 'start': 723.46, 'end': 726.0600000000001, 'text': ' from which you want to predict one,', 'video_name': 'Kint3'}, {'id': 220, 'seek': 73792, 'start': 754.36, 'end': 756.9399999999999, 'text': ' Like you want to have one number,', 'video_name': 'Kint3'}, {'id': 227, 'seek': 76792, 'start': 771.5, 'end': 775.0799999999999, 'text': \" because there's like one activation function\", 'video_name': 'Kint3'}, {'id': 235, 'seek': 76792, 'start': 791.76, 'end': 792.98, 'text': ' But which one works best,', 'video_name': 'Kint3'}, {'id': 250, 'seek': 79786, 'start': 823.4, 'end': 825.7, 'text': ' Logistic regression is a one-layer neural network.', 'video_name': 'Kint3'}, {'id': 256, 'seek': 82728, 'start': 839.0799999999999, 'end': 841.54, 'text': ' to one output using one linear layer.', 'video_name': 'Kint3'}, {'id': 334, 'seek': 103728, 'start': 1039.56, 'end': 1042.6, 'text': ' So we get like one number as an output over here.', 'video_name': 'Kint3'}, {'id': 338, 'seek': 103728, 'start': 1053.12, 'end': 1054.98, 'text': ' So this one would be neuron 1, 1.', 'video_name': 'Kint3'}, {'id': 355, 'seek': 109668, 'start': 1097.5600000000002, 'end': 1099.94, 'text': ' This one is in layer 2.', 'video_name': 'Kint3'}, {'id': 361, 'seek': 109668, 'start': 1114.44, 'end': 1117.24, 'text': ' And the output layer has just one neuron.', 'video_name': 'Kint3'}, {'id': 377, 'seek': 115652, 'start': 1171.02, 'end': 1176.74, 'text': ' So this one would be z1, z2, z3 each of the first layer.', 'video_name': 'Kint3'}, {'id': 385, 'seek': 118652, 'start': 1209.9, 'end': 1211.84, 'text': ' into one matrix vector operation.', 'video_name': 'Kint3'}, {'id': 386, 'seek': 121184, 'start': 1211.84, 'end': 1222.02, 'text': ' So if we write those individual vectors W as rows of one large matrix W1,', 'video_name': 'Kint3'}, {'id': 399, 'seek': 124176, 'start': 1258.3799999999999, 'end': 1269.32, 'text': ' So this would be the vector with that one, one, z2, one, z3, one.', 'video_name': 'Kint3'}, {'id': 402, 'seek': 127176, 'start': 1271.76, 'end': 1275.94, 'text': ' We kind of can get rid of one of the subscript index', 'video_name': 'Kint3'}, {'id': 405, 'seek': 127176, 'start': 1285.48, 'end': 1294.4, 'text': ' grouped together all the neurons into one large matrix with parameters.', 'video_name': 'Kint3'}, {'id': 411, 'seek': 130176, 'start': 1320.04, 'end': 1330.36, 'text': ' to finish into one formula, we get that our predicted value is the sigmoid of vector w2,', 'video_name': 'Kint3'}, {'id': 433, 'seek': 138810, 'start': 1393.78, 'end': 1396.6, 'text': ' basically we get this one over here.', 'video_name': 'Kint3'}, {'id': 445, 'seek': 141810, 'start': 1440.62, 'end': 1443.58, 'text': ' where we have one value as an output,', 'video_name': 'Kint3'}, {'id': 450, 'seek': 144794, 'start': 1453.18, 'end': 1455.94, 'text': ' of one layer,', 'video_name': 'Kint3'}, {'id': 459, 'seek': 147794, 'start': 1477.94, 'end': 1480.44, 'text': \" So we're having like one matrix operation over here.\", 'video_name': 'Kint3'}, {'id': 490, 'seek': 159094, 'start': 1597.94, 'end': 1602.94, 'text': ' we multiply this element and this one,', 'video_name': 'Kint3'}, {'id': 491, 'seek': 159094, 'start': 1602.94, 'end': 1605.94, 'text': ' add this one and this one, this one and this one,', 'video_name': 'Kint3'}, {'id': 515, 'seek': 168094, 'start': 1686.94, 'end': 1688.94, 'text': ' for one input example,', 'video_name': 'Kint3'}, {'id': 524, 'seek': 173894, 'start': 1738.94, 'end': 1741.94, 'text': ' and the same would be if I just remove this one,', 'video_name': 'Kint3'}, {'id': 533, 'seek': 173894, 'start': 1765.94, 'end': 1767.94, 'text': \" So let's leave this one out,\", 'video_name': 'Kint3'}, {'id': 538, 'seek': 179794, 'start': 1797.94, 'end': 1798.94, 'text': ' So this one,', 'video_name': 'Kint3'}, {'id': 540, 'seek': 179794, 'start': 1803.94, 'end': 1806.94, 'text': ' and this one would also be just a regular vector,', 'video_name': 'Kint3'}, {'id': 561, 'seek': 185594, 'start': 1871.94, 'end': 1879.94, 'text': ' I have a matrix x where each of those columns is one of the input,', 'video_name': 'Kint3'}, {'id': 567, 'seek': 188594, 'start': 1894.94, 'end': 1901.94, 'text': ' because that turns out now to be again a matrix with one output,', 'video_name': 'Kint3'}, {'id': 571, 'seek': 188594, 'start': 1906.94, 'end': 1911.94, 'text': ' but I have one of those vectors for each of my data points.', 'video_name': 'Kint3'}, {'id': 590, 'seek': 197394, 'start': 1977.94, 'end': 1979.94, 'text': ' of size three by one,', 'video_name': 'Kint3'}, {'id': 614, 'seek': 203394, 'start': 2057.94, 'end': 2060.94, 'text': \" And there's probably one question,\", 'video_name': 'Kint3'}, {'id': 637, 'seek': 209194, 'start': 2112.94, 'end': 2119.94, 'text': ' this thing is the same as W2 times this plus W2 times this one.', 'video_name': 'Kint3'}, {'id': 639, 'seek': 212094, 'start': 2120.94, 'end': 2122.94, 'text': ' got this one wrong.', 'video_name': 'Kint3'}, {'id': 654, 'seek': 214994, 'start': 2155.94, 'end': 2159.94, 'text': ' And if I multiply this matrix with this one and at this vector over here,', 'video_name': 'Kint3'}, {'id': 699, 'seek': 226694, 'start': 2280.94, 'end': 2282.94, 'text': \" just in the W's alone.\", 'video_name': 'Kint3'}, {'id': 817, 'seek': 244094, 'start': 2466.94, 'end': 2469.94, 'text': ' than the one where the mean is scaled to 0.5.', 'video_name': 'Kint3'}, {'id': 834, 'seek': 246994, 'start': 2485.94, 'end': 2486.94, 'text': ' one thing is,', 'video_name': 'Kint3'}, {'id': 899, 'seek': 255794, 'start': 2578.94, 'end': 2579.94, 'text': ' one,', 'video_name': 'Kint3'}, {'id': 902, 'seek': 255794, 'start': 2583.94, 'end': 2585.94, 'text': ' I multiply up a lot of numbers that are smaller than one,', 'video_name': 'Kint3'}, {'id': 920, 'seek': 258694, 'start': 2604.94, 'end': 2607.94, 'text': \" it's also one of the possible reasons why your gradients might get close to zero,\", 'video_name': 'Kint3'}, {'id': 928, 'seek': 261594, 'start': 2617.94, 'end': 2619.94, 'text': ' one usually talks about a different,', 'video_name': 'Kint3'}, {'id': 948, 'seek': 261594, 'start': 2639.94, 'end': 2641.94, 'text': \" is kind of one of the main reasons why training doesn't work well,\", 'video_name': 'Kint3'}, {'id': 1002, 'seek': 270394, 'start': 2707.94, 'end': 2708.94, 'text': ' take one point,', 'video_name': 'Kint3'}, {'id': 1009, 'seek': 270394, 'start': 2718.94, 'end': 2720.94, 'text': ' one single point,', 'video_name': 'Kint3'}, {'id': 1159, 'seek': 311882, 'start': 3123.46, 'end': 3127.1200000000003, 'text': \" because we can say it's just one point\", 'video_name': 'Kint3'}, {'id': 1164, 'seek': 311882, 'start': 3142.32, 'end': 3145.5800000000004, 'text': ' and we just choose one of those if we are at exactly 0.', 'video_name': 'Kint3'}, {'id': 1170, 'seek': 314882, 'start': 3154.6000000000004, 'end': 3156.9, 'text': ' So we just choose one of those possible derivatives', 'video_name': 'Kint3'}, {'id': 1207, 'seek': 320864, 'start': 3236.16, 'end': 3237.68, 'text': ' and done nothing.', 'video_name': 'Kint3'}, {'id': 1211, 'seek': 323840, 'start': 3241.56, 'end': 3244.94, 'text': ' than the simple ones is not that easy.', 'video_name': 'Kint3'}, {'id': 1214, 'seek': 323840, 'start': 3251.56, 'end': 3253.7400000000002, 'text': ' for one of our neural network neurons.', 'video_name': 'Kint3'}, {'id': 1271, 'seek': 338792, 'start': 3397.76, 'end': 3400.5, 'text': ' So one way would be to define my weights in this way', 'video_name': 'Kint3'}, {'id': 1334, 'seek': 356672, 'start': 3582.72, 'end': 3585.72, 'text': ' So for the sigmoid, we already had that one.', 'video_name': 'Kint3'}, {'id': 1338, 'seek': 356672, 'start': 3593.72, 'end': 3596.72, 'text': ' is the sigmoid times one minus the sigmoid.', 'video_name': 'Kint3'}, {'id': 1345, 'seek': 359672, 'start': 3619.72, 'end': 3622.72, 'text': ' is one minus the hyperbolic tangent squared.', 'video_name': 'Kint3'}, {'id': 1352, 'seek': 362672, 'start': 3635.72, 'end': 3638.72, 'text': \" So it's one for every number that is bigger than zero\", 'video_name': 'Kint3'}, {'id': 1579, 'seek': 418672, 'start': 4208.72, 'end': 4212.72, 'text': ' so this one over here times this value over here', 'video_name': 'Kint3'}, {'id': 1585, 'seek': 421672, 'start': 4224.72, 'end': 4226.72, 'text': ' times this one over here', 'video_name': 'Kint3'}, {'id': 1590, 'seek': 421672, 'start': 4234.72, 'end': 4237.72, 'text': ' which are the ones that are in here.', 'video_name': 'Kint3'}, {'id': 1608, 'seek': 424672, 'start': 4274.72, 'end': 4275.72, 'text': ' by this one', 'video_name': 'Kint3'}, {'id': 1613, 'seek': 427572, 'start': 4284.72, 'end': 4286.72, 'text': ' go one step...', 'video_name': 'Kint3'}, {'id': 1619, 'seek': 427572, 'start': 4296.72, 'end': 4298.72, 'text': ' one, because the gradient', 'video_name': 'Kint3'}, {'id': 1620, 'seek': 427572, 'start': 4298.72, 'end': 4300.72, 'text': ' from here to here is just one,', 'video_name': 'Kint3'}, {'id': 1623, 'seek': 430572, 'start': 4305.72, 'end': 4306.72, 'text': ' one step further.', 'video_name': 'Kint3'}, {'id': 1677, 'seek': 439472, 'start': 4410.72, 'end': 4411.72, 'text': ' for that one,', 'video_name': 'Kint3'}, {'id': 1723, 'seek': 448172, 'start': 4500.72, 'end': 4503.72, 'text': ' we again multiply this one by 1,', 'video_name': 'Kint3'}, {'id': 1764, 'seek': 459872, 'start': 4608.72, 'end': 4611.72, 'text': \" it's one thing that needs to be done\", 'video_name': 'Kint3'}, {'id': 1802, 'seek': 468772, 'start': 4692.72, 'end': 4693.72, 'text': ' one thing...', 'video_name': 'Kint3'}, {'id': 1803, 'seek': 468772, 'start': 4693.72, 'end': 4695.72, 'text': ' one thing is if we have multiple layers,', 'video_name': 'Kint3'}, {'id': 1851, 'seek': 477472, 'start': 4785.72, 'end': 4788.72, 'text': ' this one will say my output is five,', 'video_name': 'Kint3'}, {'id': 1852, 'seek': 477472, 'start': 4788.72, 'end': 4790.72, 'text': \" and this one will say it's five,\", 'video_name': 'Kint3'}, {'id': 1853, 'seek': 477472, 'start': 4790.72, 'end': 4792.72, 'text': \" and this one will say it's five.\", 'video_name': 'Kint3'}, {'id': 1890, 'seek': 486172, 'start': 4862.72, 'end': 4863.72, 'text': ' I can just use one', 'video_name': 'Kint3'}, {'id': 1967, 'seek': 500872, 'start': 5016.72, 'end': 5018.72, 'text': ' And one of the reasons for this', 'video_name': 'Kint3'}, {'id': 1984, 'seek': 503772, 'start': 5043.72, 'end': 5045.72, 'text': ' put bad apples on one side,', 'video_name': 'Kint3'}, {'id': 2024, 'seek': 509572, 'start': 5118.72, 'end': 5120.72, 'text': ' or worse than another one.', 'video_name': 'Kint3'}, {'id': 2, 'seek': 0, 'start': 17.28, 'end': 24.88, 'text': ' the right one. Yeah, so and the OLAT course has some kind of the links to the relevant links.', 'video_name': 'Kint'}, {'id': 13, 'seek': 5486, 'start': 67.72, 'end': 75.68, 'text': ' version is the print version. So the print version is basically also just a web page, but one which is kind of', 'video_name': 'Kint'}, {'id': 33, 'seek': 17308, 'start': 192.94, 'end': 197.82000000000002, 'text': ' using this Jupyter instance that I put on one of the THBing servers.', 'video_name': 'Kint'}, {'id': 53, 'seek': 23308, 'start': 261.08000000000004, 'end': 263.08000000000004, 'text': ' one of the main Python libraries we will be using for Python.', 'video_name': 'Kint'}, {'id': 90, 'seek': 40966, 'start': 427.32000000000005, 'end': 435.58000000000004, 'text': \" There's kind of one really good book about deep learning by Ian Goodfellow and Joshua Bengio and Aaron Colville,\", 'video_name': 'Kint'}, {'id': 93, 'seek': 43966, 'start': 439.66, 'end': 441.82000000000005, 'text': ' None of those references are necessary.', 'video_name': 'Kint'}, {'id': 119, 'seek': 52960, 'start': 547.4200000000001, 'end': 552.4200000000001, 'text': \" And it's first mentioned in this book...\", 'video_name': 'Kint'}, {'id': 133, 'seek': 58958, 'start': 601.36, 'end': 604.12, 'text': ' However, one might be confused about why this mechanism works.', 'video_name': 'Kint'}, {'id': 149, 'seek': 67652, 'start': 691.22, 'end': 699.1999999999999, 'text': ' It basically said that, for example, one layer neural network can never solve the XOR function.', 'video_name': 'Kint'}, {'id': 173, 'seek': 79580, 'start': 808.92, 'end': 811.3599999999999, 'text': ' There was one thing, the Netflix price.', 'video_name': 'Kint'}, {'id': 174, 'seek': 79580, 'start': 811.5, 'end': 823.3399999999999, 'text': ' Netflix put out a price money of $1 million for somebody who could improve their movie recommender algorithm...', 'video_name': 'Kint'}, {'id': 183, 'seek': 82580, 'start': 850.56, 'end': 852.9599999999999, 'text': ' And probably the price money was net.', 'video_name': 'Kint'}, {'id': 190, 'seek': 85568, 'start': 872.16, 'end': 878.8599999999999, 'text': ' This was one of the first times...', 'video_name': 'Kint'}, {'id': 191, 'seek': 85568, 'start': 879.4799999999999, 'end': 882.5, 'text': ' That was one of the first things that needed to be solved.', 'video_name': 'Kint'}, {'id': 308, 'seek': 117740, 'start': 1196.6000000000001, 'end': 1200.0, 'text': ' or from the long one-dimensional structure', 'video_name': 'Kint'}, {'id': 371, 'seek': 141532, 'start': 1421.32, 'end': 1423.32, 'text': ' but one where, for example,', 'video_name': 'Kint'}, {'id': 432, 'seek': 162032, 'start': 1628.32, 'end': 1633.32, 'text': ' On the Jupyter server, I have a copy of one of those dumps', 'video_name': 'Kint'}, {'id': 460, 'seek': 170832, 'start': 1717.32, 'end': 1720.32, 'text': ' So one thing is they are really, really good', 'video_name': 'Kint'}, {'id': 769, 'seek': 232032, 'start': 2334.32, 'end': 2336.32, 'text': ' one way to do that is...', 'video_name': 'Kint'}, {'id': 774, 'seek': 232032, 'start': 2346.32, 'end': 2349.32, 'text': ' and which is kind of one of the easiest ways to install', 'video_name': 'Kint'}, {'id': 829, 'seek': 243632, 'start': 2437.32, 'end': 2439.32, 'text': \" there should be one where there's a link.\", 'video_name': 'Kint'}, {'id': 844, 'seek': 246632, 'start': 2491.32, 'end': 2492.32, 'text': ' this one...', 'video_name': 'Kint'}, {'id': 929, 'seek': 264332, 'start': 2657.32, 'end': 2658.32, 'text': ' on one axis.', 'video_name': 'Kint'}, {'id': 933, 'seek': 264332, 'start': 2662.32, 'end': 2665.32, 'text': ' one two three four five six houses...', 'video_name': 'Kint'}, {'id': 940, 'seek': 267232, 'start': 2672.32, 'end': 2677.88, 'text': ' any kind of size of one house, I want to predict the price for it.', 'video_name': 'Kint'}, {'id': 941, 'seek': 267232, 'start': 2679.28, 'end': 2687.6400000000003, 'text': ' And one of the easiest ways to do that is do linear regression.', 'video_name': 'Kint'}, {'id': 956, 'seek': 276232, 'start': 2764.32, 'end': 2769.82, 'text': ' And this kind of linear model is kind of one of the earliest things', 'video_name': 'Kint'}, {'id': 966, 'seek': 279232, 'start': 2819.54, 'end': 2821.6000000000004, 'text': ' then there is one...', 'video_name': 'Kint'}, {'id': 979, 'seek': 285112, 'start': 2861.12, 'end': 2863.12, 'text': \" we'll just take that one.\", 'video_name': 'Kint'}, {'id': 982, 'seek': 285112, 'start': 2871.12, 'end': 2876.12, 'text': ' And this is probably a better predictor than the one we had before.', 'video_name': 'Kint'}, {'id': 983, 'seek': 285112, 'start': 2876.12, 'end': 2880.12, 'text': ' So because we kind of have fixed one of the small issues that we have with this model,', 'video_name': 'Kint'}, {'id': 1005, 'seek': 294012, 'start': 2944.12, 'end': 2950.12, 'text': ' So that is mainly what one neuron in an artificial neural network is.', 'video_name': 'Kint'}, {'id': 1009, 'seek': 294012, 'start': 2954.12, 'end': 2957.12, 'text': \" but it's one way a neuron could look like.\", 'video_name': 'Kint'}, {'id': 1018, 'seek': 296912, 'start': 2982.12, 'end': 2988.12, 'text': ' like the information processing that happens within one human neuron', 'video_name': 'Kint'}, {'id': 1033, 'seek': 302712, 'start': 3028.12, 'end': 3030.12, 'text': \" it's a one-to-one function.\", 'video_name': 'Kint'}, {'id': 1034, 'seek': 302712, 'start': 3030.12, 'end': 3035.12, 'text': \" So it's a function that takes one input and produces one output.\", 'video_name': 'Kint'}, {'id': 1057, 'seek': 305612, 'start': 3079.12, 'end': 3083.12, 'text': \" But it's kind of one of the most used activation functions for neural networks.\", 'video_name': 'Kint'}, {'id': 1066, 'seek': 308612, 'start': 3104.12, 'end': 3107.12, 'text': ' the size of the house is one particular...', 'video_name': 'Kint'}, {'id': 1082, 'seek': 311612, 'start': 3137.12, 'end': 3139.12, 'text': \" we don't want to just use one...\", 'video_name': 'Kint'}, {'id': 1086, 'seek': 311612, 'start': 3144.12, 'end': 3146.12, 'text': ' have one neuron...', 'video_name': 'Kint'}, {'id': 1148, 'seek': 326212, 'start': 3283.12, 'end': 3286.12, 'text': ' it might turn out that one of those intermediate neurons...', 'video_name': 'Kint'}, {'id': 1225, 'seek': 344112, 'start': 3466.12, 'end': 3468.12, 'text': ' And we had one output variable...', 'video_name': 'Kint'}, {'id': 1238, 'seek': 347012, 'start': 3491.12, 'end': 3493.12, 'text': ' Which is kind of one of the earliest...', 'video_name': 'Kint'}, {'id': 1264, 'seek': 352912, 'start': 3551.12, 'end': 3553.12, 'text': ' as an input and one...', 'video_name': 'Kint'}, {'id': 1364, 'seek': 373512, 'start': 3757.12, 'end': 3759.12, 'text': \" simple ones. So it's kind of very structured...\", 'video_name': 'Kint'}, {'id': 1377, 'seek': 376512, 'start': 3783.12, 'end': 3785.12, 'text': ' that one can use for this or...', 'video_name': 'Kint'}, {'id': 1418, 'seek': 385312, 'start': 3865.12, 'end': 3867.12, 'text': ' just one number...', 'video_name': 'Kint'}, {'id': 1472, 'seek': 397312, 'start': 3973.12, 'end': 3975.12, 'text': ' one or zero...', 'video_name': 'Kint'}, {'id': 1491, 'seek': 400312, 'start': 4011.12, 'end': 4013.12, 'text': ' and this one would be a one, two, three...', 'video_name': 'Kint'}, {'id': 1500, 'seek': 400312, 'start': 4029.12, 'end': 4031.12, 'text': \" it's a binary one...\", 'video_name': 'Kint'}, {'id': 1501, 'seek': 400312, 'start': 4031.12, 'end': 4033.12, 'text': \" so it's either zero or one...\", 'video_name': 'Kint'}, {'id': 1505, 'seek': 403312, 'start': 4039.12, 'end': 4041.12, 'text': ' if we have like one...', 'video_name': 'Kint'}, {'id': 1508, 'seek': 403312, 'start': 4045.12, 'end': 4047.12, 'text': ' that is one piece of data that we can...', 'video_name': 'Kint'}, {'id': 1512, 'seek': 403312, 'start': 4053.12, 'end': 4055.12, 'text': \" we don't need just one example to train...\", 'video_name': 'Kint'}, {'id': 1524, 'seek': 406312, 'start': 4077.12, 'end': 4079.12, 'text': ' of one input feature vector...', 'video_name': 'Kint'}, {'id': 1627, 'seek': 427312, 'start': 4297.12, 'end': 4299.12, 'text': ' one thing we will do is...', 'video_name': 'Kint'}, {'id': 1632, 'seek': 430312, 'start': 4307.12, 'end': 4309.12, 'text': ' is one column...', 'video_name': 'Kint'}, {'id': 1634, 'seek': 430312, 'start': 4311.12, 'end': 4313.12, 'text': ' is one input vector...', 'video_name': 'Kint'}, {'id': 1644, 'seek': 430312, 'start': 4331.12, 'end': 4333.12, 'text': ' write them into one large matrix...', 'video_name': 'Kint'}, {'id': 1652, 'seek': 433312, 'start': 4347.12, 'end': 4349.12, 'text': ' one input feature might be something like...', 'video_name': 'Kint'}, {'id': 1660, 'seek': 436312, 'start': 4363.12, 'end': 4365.12, 'text': ' this way we get like one big matrix...', 'video_name': 'Kint'}, {'id': 1667, 'seek': 436312, 'start': 4379.12, 'end': 4381.12, 'text': ' and put that into one big...', 'video_name': 'Kint'}, {'id': 1668, 'seek': 436312, 'start': 4381.12, 'end': 4383.12, 'text': ' this is one by M matrix...', 'video_name': 'Kint'}, {'id': 1669, 'seek': 436312, 'start': 4383.12, 'end': 4385.12, 'text': ' so we kind of just have one entry...', 'video_name': 'Kint'}, {'id': 1678, 'seek': 439312, 'start': 4401.12, 'end': 4403.12, 'text': ' not just for one of the examples...', 'video_name': 'Kint'}, {'id': 1711, 'seek': 445312, 'start': 4467.12, 'end': 4469.12, 'text': ' we say we want to have one long vector...', 'video_name': 'Kint'}, {'id': 1719, 'seek': 448312, 'start': 4483.12, 'end': 4485.12, 'text': ' into one long feature vector...', 'video_name': 'Kint'}, {'id': 1739, 'seek': 451312, 'start': 4523.12, 'end': 4525.12, 'text': ' you usually reserve one byte...', 'video_name': 'Kint'}, {'id': 1753, 'seek': 454312, 'start': 4551.12, 'end': 4553.12, 'text': ' so one always has to be careful...', 'video_name': 'Kint'}, {'id': 1757, 'seek': 454312, 'start': 4559.12, 'end': 4561.12, 'text': ' and the column as the next one...', 'video_name': 'Kint'}, {'id': 1781, 'seek': 460312, 'start': 4607.12, 'end': 4609.12, 'text': ' we have turned our input into one large matrix of information...', 'video_name': 'Kint'}, {'id': 1789, 'seek': 460312, 'start': 4623.12, 'end': 4625.12, 'text': ' so we get not one matrix...', 'video_name': 'Kint'}, {'id': 1806, 'seek': 463312, 'start': 4657.12, 'end': 4659.12, 'text': ' has one...', 'video_name': 'Kint'}, {'id': 1807, 'seek': 463312, 'start': 4659.12, 'end': 4661.12, 'text': ' each entry contains one pixel information...', 'video_name': 'Kint'}, {'id': 1808, 'seek': 463312, 'start': 4661.12, 'end': 4663.12, 'text': ' for one of the color channels...', 'video_name': 'Kint'}, {'id': 1833, 'seek': 469312, 'start': 4711.12, 'end': 4713.12, 'text': ' down into one...', 'video_name': 'Kint'}, {'id': 1843, 'seek': 472312, 'start': 4731.12, 'end': 4733.12, 'text': ' into one...', 'video_name': 'Kint'}, {'id': 1844, 'seek': 472312, 'start': 4733.12, 'end': 4735.12, 'text': ' into a one dimensional...', 'video_name': 'Kint'}, {'id': 1845, 'seek': 472312, 'start': 4735.12, 'end': 4737.12, 'text': ' into a one dimensional object...', 'video_name': 'Kint'}, {'id': 1846, 'seek': 472312, 'start': 4737.12, 'end': 4739.12, 'text': ' so into a one long vector...', 'video_name': 'Kint'}, {'id': 1892, 'seek': 481312, 'start': 4829.12, 'end': 4831.12, 'text': ' a big part of what needs to be done...', 'video_name': 'Kint'}, {'id': 1910, 'seek': 484312, 'start': 4865.12, 'end': 4867.12, 'text': ' one advantage...', 'video_name': 'Kint'}, {'id': 1936, 'seek': 490312, 'start': 4917.12, 'end': 4919.12, 'text': ' at the end is one of those...', 'video_name': 'Kint'}, {'id': 1981, 'seek': 499312, 'start': 5015.12, 'end': 5017.12, 'text': ' so one of the things...', 'video_name': 'Kint'}, {'id': 2015, 'seek': 508312, 'start': 5085.12, 'end': 5087.12, 'text': ' done on the computer?', 'video_name': 'Kint'}, {'id': 0, 'seek': 0, 'start': 0.0, 'end': 10.72, 'text': ' Okay, last week we talked about machine learning systems and that one of the first things we', 'video_name': 'Kint2'}, {'id': 5, 'seek': 2260, 'start': 40.24, 'end': 49.28, 'text': ' three matrices, each having the same size and dimensions, and one matrix is for each', 'video_name': 'Kint2'}, {'id': 11, 'seek': 5258, 'start': 67.32, 'end': 76.3, 'text': ' And this object can be turned into one long vector by just stacking those matrices.', 'video_name': 'Kint2'}, {'id': 13, 'seek': 8212, 'start': 82.12, 'end': 89.4, 'text': ' call reshape on that and just turn everything into one long vector.', 'video_name': 'Kint2'}, {'id': 14, 'seek': 8212, 'start': 89.4, 'end': 98.52000000000001, 'text': ' And this way we kind of get one vector with all the input data for this one image.', 'video_name': 'Kint2'}, {'id': 22, 'seek': 11212, 'start': 125.9, 'end': 137.26, 'text': ' we have like three by 567 by 554, in this case, and one thing to reduce this number', 'video_name': 'Kint2'}, {'id': 27, 'seek': 14140, 'start': 143.78, 'end': 145.86, 'text': ' So one thing is that each of the images', 'video_name': 'Kint2'}, {'id': 33, 'seek': 14140, 'start': 158.24, 'end': 160.24, 'text': ' So one thing we want to do with images', 'video_name': 'Kint2'}, {'id': 68, 'seek': 22966, 'start': 248.84, 'end': 253.28, 'text': ' and like one thing we were to start with,', 'video_name': 'Kint2'}, {'id': 90, 'seek': 28966, 'start': 308.34000000000003, 'end': 312.28000000000003, 'text': ' one thing that we want to predict is,', 'video_name': 'Kint2'}, {'id': 129, 'seek': 34914, 'start': 364.14, 'end': 365.14, 'text': ' If we would remove this one,', 'video_name': 'Kint2'}, {'id': 149, 'seek': 37464, 'start': 377.64, 'end': 379.64, 'text': ' If we would remove this one,', 'video_name': 'Kint2'}, {'id': 216, 'seek': 49264, 'start': 507.64, 'end': 509.64, 'text': ' one might have a good outcome,', 'video_name': 'Kint2'}, {'id': 221, 'seek': 49264, 'start': 517.64, 'end': 518.64, 'text': ' one patient might,', 'video_name': 'Kint2'}, {'id': 371, 'seek': 72506, 'start': 725.06, 'end': 727.4799999999999, 'text': ' this is like one value.', 'video_name': 'Kint2'}, {'id': 391, 'seek': 75506, 'start': 779.78, 'end': 781.38, 'text': ' so which is just one number,', 'video_name': 'Kint2'}, {'id': 423, 'seek': 84400, 'start': 849.5, 'end': 853.72, 'text': ' that maps every input to a number between zero and one.', 'video_name': 'Kint2'}, {'id': 437, 'seek': 87364, 'start': 886.66, 'end': 888.76, 'text': ' the denominator gets close to one.', 'video_name': 'Kint2'}, {'id': 438, 'seek': 87364, 'start': 888.76, 'end': 889.78, 'text': \" And if it's close to one,\", 'video_name': 'Kint2'}, {'id': 439, 'seek': 87364, 'start': 889.78, 'end': 890.86, 'text': ' we get one over one.', 'video_name': 'Kint2'}, {'id': 441, 'seek': 87364, 'start': 892.72, 'end': 894.6999999999999, 'text': ' the whole thing approaches one.', 'video_name': 'Kint2'}, {'id': 443, 'seek': 87364, 'start': 900.22, 'end': 902.24, 'text': ' to something between zero and one.', 'video_name': 'Kint2'}, {'id': 448, 'seek': 90364, 'start': 910.1999999999999, 'end': 912.72, 'text': ' then that should be a number between zero and one.', 'video_name': 'Kint2'}, {'id': 450, 'seek': 90364, 'start': 914.78, 'end': 917.22, 'text': ' that should be a number between zero and one.', 'video_name': 'Kint2'}, {'id': 456, 'seek': 90364, 'start': 930.18, 'end': 932.76, 'text': ' is always something between zero and one.', 'video_name': 'Kint2'}, {'id': 476, 'seek': 99250, 'start': 992.5, 'end': 996.42, 'text': ' So if we say we have one data point,', 'video_name': 'Kint2'}, {'id': 477, 'seek': 99250, 'start': 996.42, 'end': 999.82, 'text': ' so one example, one input image,', 'video_name': 'Kint2'}, {'id': 482, 'seek': 99250, 'start': 1013.18, 'end': 1018.18, 'text': ' So the actual class will either be a one or a zero.', 'video_name': 'Kint2'}, {'id': 484, 'seek': 99250, 'start': 1020.58, 'end': 1022.5, 'text': ' can be any number between one and zero.', 'video_name': 'Kint2'}, {'id': 498, 'seek': 105220, 'start': 1068.64, 'end': 1073.1000000000001, 'text': ' is either zero or it is one.', 'video_name': 'Kint2'}, {'id': 499, 'seek': 105220, 'start': 1073.1000000000001, 'end': 1075.3600000000001, 'text': \" If it's either zero or one,\", 'video_name': 'Kint2'}, {'id': 501, 'seek': 105220, 'start': 1080.18, 'end': 1081.14, 'text': \" If it's one, it means, one.\", 'video_name': 'Kint2'}, {'id': 507, 'seek': 108210, 'start': 1095.28, 'end': 1099.54, 'text': ' So if we say y is equal to one,', 'video_name': 'Kint2'}, {'id': 534, 'seek': 114198, 'start': 1171.14, 'end': 1171.94, 'text': ' Now is the last one.', 'video_name': 'Kint2'}, {'id': 538, 'seek': 119556, 'start': 1195.56, 'end': 1210.1799999999998, 'text': ' So, if I take this one and I have one over here, and if my y hat, so what I predict, is very close to one,', 'video_name': 'Kint2'}, {'id': 539, 'seek': 119556, 'start': 1211.86, 'end': 1221.1399999999999, 'text': \" that means if it's close to one, then the logarithm of my prediction will also be very, very close to zero.\", 'video_name': 'Kint2'}, {'id': 542, 'seek': 122422, 'start': 1224.22, 'end': 1235.64, 'text': ' If my target class is one, and my prediction is very close to one, then the loss over here, this number, will be very close to zero.', 'video_name': 'Kint2'}, {'id': 553, 'seek': 128422, 'start': 1298.22, 'end': 1306.22, 'text': \" And I have one over here, so what I'm looking at is the logarithm of one minus my prediction.\", 'video_name': 'Kint2'}, {'id': 555, 'seek': 131422, 'start': 1314.22, 'end': 1320.22, 'text': ' So, the output will be close to one, and that means it gets, the output will be close to zero again.', 'video_name': 'Kint2'}, {'id': 601, 'seek': 154822, 'start': 1565.22, 'end': 1569.22, 'text': ' So, we have one data point.', 'video_name': 'Kint2'}, {'id': 640, 'seek': 172422, 'start': 1724.22, 'end': 1730.22, 'text': \" So, there's several ways how one could decide on those.\", 'video_name': 'Kint2'}, {'id': 713, 'seek': 198922, 'start': 1997.22, 'end': 2001.22, 'text': ' And this works all nicely as long as you have only one input variable.', 'video_name': 'Kint2'}, {'id': 722, 'seek': 201922, 'start': 2019.22, 'end': 2023.22, 'text': ' we get like one entry in the gradient.', 'video_name': 'Kint2'}, {'id': 727, 'seek': 201922, 'start': 2032.22, 'end': 2035.22, 'text': ' So, I have a function that has two inputs and one output,', 'video_name': 'Kint2'}, {'id': 790, 'seek': 219322, 'start': 2193.22, 'end': 2195.22, 'text': ' in one direction,', 'video_name': 'Kint2'}, {'id': 867, 'seek': 236722, 'start': 2388.22, 'end': 2390.22, 'text': ' have one parameter,', 'video_name': 'Kint2'}, {'id': 877, 'seek': 239622, 'start': 2404.22, 'end': 2405.22, 'text': ' we only have one direct,', 'video_name': 'Kint2'}, {'id': 878, 'seek': 239622, 'start': 2405.22, 'end': 2406.22, 'text': ' one dimension,', 'video_name': 'Kint2'}, {'id': 885, 'seek': 239622, 'start': 2419.22, 'end': 2420.22, 'text': ' the inner is just one,', 'video_name': 'Kint2'}, {'id': 932, 'seek': 251322, 'start': 2524.22, 'end': 2526.22, 'text': ' one is a vector,', 'video_name': 'Kint2'}, {'id': 953, 'seek': 257222, 'start': 2572.22, 'end': 2578.22, 'text': \" so one thing we haven't talked about a lot yet,\", 'video_name': 'Kint2'}, {'id': 1087, 'seek': 280622, 'start': 2808.22, 'end': 2810.22, 'text': ' like for one single training example,', 'video_name': 'Kint2'}, {'id': 1100, 'seek': 280622, 'start': 2831.22, 'end': 2833.22, 'text': ' plus one minus y,', 'video_name': 'Kint2'}, {'id': 1103, 'seek': 283522, 'start': 2837.22, 'end': 2838.22, 'text': ' one minus,', 'video_name': 'Kint2'}, {'id': 1112, 'seek': 283522, 'start': 2849.22, 'end': 2851.22, 'text': ' where this function is still a very simple one,', 'video_name': 'Kint2'}, {'id': 1157, 'seek': 289322, 'start': 2908.22, 'end': 2909.22, 'text': ' one output,', 'video_name': 'Kint2'}, {'id': 1167, 'seek': 292222, 'start': 2922.22, 'end': 2923.22, 'text': ' is just one,', 'video_name': 'Kint2'}, {'id': 1170, 'seek': 292222, 'start': 2925.22, 'end': 2926.22, 'text': ' which is also just one,', 'video_name': 'Kint2'}, {'id': 1177, 'seek': 292222, 'start': 2932.22, 'end': 2933.22, 'text': ' and one can kind of,', 'video_name': 'Kint2'}, {'id': 1186, 'seek': 292222, 'start': 2941.22, 'end': 2942.22, 'text': ' one epsilon,', 'video_name': 'Kint2'}, {'id': 1188, 'seek': 292222, 'start': 2943.22, 'end': 2944.22, 'text': ' one epsilon,', 'video_name': 'Kint2'}, {'id': 1230, 'seek': 300674, 'start': 3012.68, 'end': 3018.8199999999997, 'text': \" kind of one of the main operations for which we will need the derivative. So it's kind\", 'video_name': 'Kint2'}, {'id': 1236, 'seek': 303664, 'start': 3053.8199999999997, 'end': 3060.22, 'text': ' minus one for the derivative of B. If we square things up we can define the derivative of', 'video_name': 'Kint2'}, {'id': 1237, 'seek': 303664, 'start': 3060.22, 'end': 3065.68, 'text': \" that function. So it's kind of it's a single input function so we have just one input and\", 'video_name': 'Kint2'}, {'id': 1242, 'seek': 309660, 'start': 3096.6, 'end': 3104.8399999999997, 'text': ' one if the input is bigger than zero and zero otherwise. So this thing here is notation for', 'video_name': 'Kint2'}, {'id': 1243, 'seek': 309660, 'start': 3104.8399999999997, 'end': 3112.18, 'text': ' an indicated one so if it will be one as long as this condition is true and otherwise it will be', 'video_name': 'Kint2'}, {'id': 1244, 'seek': 309660, 'start': 3112.18, 'end': 3120.52, 'text': \" zero. So it's one if A is greater than zero and otherwise the derivative over here will just be\", 'video_name': 'Kint2'}, {'id': 1248, 'seek': 312660, 'start': 3126.6, 'end': 3127.1, 'text': \" derivative over here. So it's one if A is greater than zero and otherwise it will be zero.\", 'video_name': 'Kint2'}, {'id': 1250, 'seek': 312660, 'start': 3133.18, 'end': 3141.06, 'text': ' So we have like in the direction of A if A is bigger than B then the derivative is one in the', 'video_name': 'Kint2'}, {'id': 1302, 'seek': 333654, 'start': 3356.84, 'end': 3359.88, 'text': ' And so we know this one.', 'video_name': 'Kint2'}, {'id': 1303, 'seek': 333654, 'start': 3359.88, 'end': 3362.44, 'text': ' And if we look at this one, we can say,', 'video_name': 'Kint2'}, {'id': 1365, 'seek': 354212, 'start': 3565.92, 'end': 3570.74, 'text': ' then we say our loss function is this one over here', 'video_name': 'Kint2'}, {'id': 1367, 'seek': 357210, 'start': 3572.1, 'end': 3574.92, 'text': ' So if we have like one single example,', 'video_name': 'Kint2'}, {'id': 1379, 'seek': 360110, 'start': 3610.96, 'end': 3614.48, 'text': ' to define one node so we can,', 'video_name': 'Kint2'}, {'id': 1389, 'seek': 362972, 'start': 3642.98, 'end': 3647.18, 'text': ' is the sigmoid of z times one minus the sigmoid of z.', 'video_name': 'Kint2'}, {'id': 1396, 'seek': 365956, 'start': 3677.96, 'end': 3684.52, 'text': \" so it's kind of the sigmoid of that times one minus the sigma of that which is kind of it\", 'video_name': 'Kint2'}, {'id': 1398, 'seek': 368452, 'start': 3691.8, 'end': 3699.48, 'text': ' the sigmoid of z so the derivative will be the prediction that we made times one minus', 'video_name': 'Kint2'}, {'id': 1401, 'seek': 371452, 'start': 3714.52, 'end': 3722.12, 'text': ' over here and again we will create one compute node for this for this function over here so', 'video_name': 'Kint2'}, {'id': 1407, 'seek': 374452, 'start': 3753.96, 'end': 3762.28, 'text': ' one minus the number that we the the actual class divided by one minus our predicted class', 'video_name': 'Kint2'}, {'id': 1436, 'seek': 383096, 'start': 3857.68, 'end': 3860.94, 'text': ' y is equal to one and that is,', 'video_name': 'Kint2'}, {'id': 1440, 'seek': 386096, 'start': 3870.68, 'end': 3871.68, 'text': ' are two and one.', 'video_name': 'Kint2'}, {'id': 1456, 'seek': 392096, 'start': 3931.96, 'end': 3935.96, 'text': ' So in this case, we only have like one data point.', 'video_name': 'Kint2'}, {'id': 1462, 'seek': 392096, 'start': 3946.96, 'end': 3949.96, 'text': ' So it would just mean one more addition up here.', 'video_name': 'Kint2'}, {'id': 1501, 'seek': 406996, 'start': 4085.96, 'end': 4087.96, 'text': ' between this and this one.', 'video_name': 'Kint2'}, {'id': 1553, 'seek': 424596, 'start': 4248.96, 'end': 4251.96, 'text': ' to increase y hat by one point.', 'video_name': 'Kint2'}, {'id': 1561, 'seek': 424596, 'start': 4266.96, 'end': 4269.96, 'text': \" it's always just one.\", 'video_name': 'Kint2'}, {'id': 1565, 'seek': 427596, 'start': 4275.96, 'end': 4276.96, 'text': ' okay, for this one,', 'video_name': 'Kint2'}, {'id': 1567, 'seek': 427596, 'start': 4278.96, 'end': 4280.96, 'text': ' And for this one,', 'video_name': 'Kint2'}, {'id': 1644, 'seek': 450996, 'start': 4511.96, 'end': 4515.96, 'text': ' So one step after the other in a larger compute graph', 'video_name': 'Kint2'}, {'id': 1646, 'seek': 450996, 'start': 4522.96, 'end': 4525.96, 'text': ' One thing that one can do quite often is', 'video_name': 'Kint2'}, {'id': 1653, 'seek': 453996, 'start': 4539.96, 'end': 4543.96, 'text': ' the loss function was this one,', 'video_name': 'Kint2'}, {'id': 1654, 'seek': 453996, 'start': 4543.96, 'end': 4546.96, 'text': ' and the derivative of the sigmoid was this one.', 'video_name': 'Kint2'}, {'id': 1665, 'seek': 456896, 'start': 4574.96, 'end': 4578.96, 'text': ' So what one kind of often does is kind of combining', 'video_name': 'Kint2'}, {'id': 1667, 'seek': 456896, 'start': 4581.96, 'end': 4583.96, 'text': ' into like one single node', 'video_name': 'Kint2'}, {'id': 1670, 'seek': 456896, 'start': 4587.96, 'end': 4591.96, 'text': ' if one just combines them into one node,', 'video_name': 'Kint2'}, {'id': 1675, 'seek': 459796, 'start': 4600.96, 'end': 4601.96, 'text': ' if you take this one,', 'video_name': 'Kint2'}, {'id': 1739, 'seek': 477396, 'start': 4774.96, 'end': 4776.96, 'text': ' the derivative into one direction', 'video_name': 'Kint2'}, {'id': 1742, 'seek': 477396, 'start': 4781.96, 'end': 4783.96, 'text': ' the derivative is one.', 'video_name': 'Kint2'}, {'id': 1788, 'seek': 486196, 'start': 4884.96, 'end': 4887.96, 'text': ' put this one multiplied by this one.', 'video_name': 'Kint2'}]\n",
      "search_phrase: one of\n",
      "found_objects:  [{'id': 110, 'seek': 41004, 'start': 420.68, 'end': 428.20000000000005, 'text': \" So the gradient of going towards... of addition, multiplication, it's always just taking one of the numbers.\", 'video_name': 'Kint3'}, {'id': 402, 'seek': 127176, 'start': 1271.76, 'end': 1275.94, 'text': ' We kind of can get rid of one of the subscript index', 'video_name': 'Kint3'}, {'id': 561, 'seek': 185594, 'start': 1871.94, 'end': 1879.94, 'text': ' I have a matrix x where each of those columns is one of the input,', 'video_name': 'Kint3'}, {'id': 571, 'seek': 188594, 'start': 1906.94, 'end': 1911.94, 'text': ' but I have one of those vectors for each of my data points.', 'video_name': 'Kint3'}, {'id': 920, 'seek': 258694, 'start': 2604.94, 'end': 2607.94, 'text': \" it's also one of the possible reasons why your gradients might get close to zero,\", 'video_name': 'Kint3'}, {'id': 948, 'seek': 261594, 'start': 2639.94, 'end': 2641.94, 'text': \" is kind of one of the main reasons why training doesn't work well,\", 'video_name': 'Kint3'}, {'id': 1164, 'seek': 311882, 'start': 3142.32, 'end': 3145.5800000000004, 'text': ' and we just choose one of those if we are at exactly 0.', 'video_name': 'Kint3'}, {'id': 1170, 'seek': 314882, 'start': 3154.6000000000004, 'end': 3156.9, 'text': ' So we just choose one of those possible derivatives', 'video_name': 'Kint3'}, {'id': 1214, 'seek': 323840, 'start': 3251.56, 'end': 3253.7400000000002, 'text': ' for one of our neural network neurons.', 'video_name': 'Kint3'}, {'id': 1967, 'seek': 500872, 'start': 5016.72, 'end': 5018.72, 'text': ' And one of the reasons for this', 'video_name': 'Kint3'}, {'id': 33, 'seek': 17308, 'start': 192.94, 'end': 197.82000000000002, 'text': ' using this Jupyter instance that I put on one of the THBing servers.', 'video_name': 'Kint'}, {'id': 53, 'seek': 23308, 'start': 261.08000000000004, 'end': 263.08000000000004, 'text': ' one of the main Python libraries we will be using for Python.', 'video_name': 'Kint'}, {'id': 93, 'seek': 43966, 'start': 439.66, 'end': 441.82000000000005, 'text': ' None of those references are necessary.', 'video_name': 'Kint'}, {'id': 190, 'seek': 85568, 'start': 872.16, 'end': 878.8599999999999, 'text': ' This was one of the first times...', 'video_name': 'Kint'}, {'id': 191, 'seek': 85568, 'start': 879.4799999999999, 'end': 882.5, 'text': ' That was one of the first things that needed to be solved.', 'video_name': 'Kint'}, {'id': 432, 'seek': 162032, 'start': 1628.32, 'end': 1633.32, 'text': ' On the Jupyter server, I have a copy of one of those dumps', 'video_name': 'Kint'}, {'id': 774, 'seek': 232032, 'start': 2346.32, 'end': 2349.32, 'text': ' and which is kind of one of the easiest ways to install', 'video_name': 'Kint'}, {'id': 941, 'seek': 267232, 'start': 2679.28, 'end': 2687.6400000000003, 'text': ' And one of the easiest ways to do that is do linear regression.', 'video_name': 'Kint'}, {'id': 956, 'seek': 276232, 'start': 2764.32, 'end': 2769.82, 'text': ' And this kind of linear model is kind of one of the earliest things', 'video_name': 'Kint'}, {'id': 983, 'seek': 285112, 'start': 2876.12, 'end': 2880.12, 'text': ' So because we kind of have fixed one of the small issues that we have with this model,', 'video_name': 'Kint'}, {'id': 1057, 'seek': 305612, 'start': 3079.12, 'end': 3083.12, 'text': \" But it's kind of one of the most used activation functions for neural networks.\", 'video_name': 'Kint'}, {'id': 1148, 'seek': 326212, 'start': 3283.12, 'end': 3286.12, 'text': ' it might turn out that one of those intermediate neurons...', 'video_name': 'Kint'}, {'id': 1238, 'seek': 347012, 'start': 3491.12, 'end': 3493.12, 'text': ' Which is kind of one of the earliest...', 'video_name': 'Kint'}, {'id': 1678, 'seek': 439312, 'start': 4401.12, 'end': 4403.12, 'text': ' not just for one of the examples...', 'video_name': 'Kint'}, {'id': 1808, 'seek': 463312, 'start': 4661.12, 'end': 4663.12, 'text': ' for one of the color channels...', 'video_name': 'Kint'}, {'id': 1936, 'seek': 490312, 'start': 4917.12, 'end': 4919.12, 'text': ' at the end is one of those...', 'video_name': 'Kint'}, {'id': 1981, 'seek': 499312, 'start': 5015.12, 'end': 5017.12, 'text': ' so one of the things...', 'video_name': 'Kint'}, {'id': 0, 'seek': 0, 'start': 0.0, 'end': 10.72, 'text': ' Okay, last week we talked about machine learning systems and that one of the first things we', 'video_name': 'Kint2'}, {'id': 1230, 'seek': 300674, 'start': 3012.68, 'end': 3018.8199999999997, 'text': \" kind of one of the main operations for which we will need the derivative. So it's kind\", 'video_name': 'Kint2'}]\n",
      "search_phrase: one of the\n",
      "found_objects:  [{'id': 110, 'seek': 41004, 'start': 420.68, 'end': 428.20000000000005, 'text': \" So the gradient of going towards... of addition, multiplication, it's always just taking one of the numbers.\", 'video_name': 'Kint3'}, {'id': 402, 'seek': 127176, 'start': 1271.76, 'end': 1275.94, 'text': ' We kind of can get rid of one of the subscript index', 'video_name': 'Kint3'}, {'id': 561, 'seek': 185594, 'start': 1871.94, 'end': 1879.94, 'text': ' I have a matrix x where each of those columns is one of the input,', 'video_name': 'Kint3'}, {'id': 920, 'seek': 258694, 'start': 2604.94, 'end': 2607.94, 'text': \" it's also one of the possible reasons why your gradients might get close to zero,\", 'video_name': 'Kint3'}, {'id': 948, 'seek': 261594, 'start': 2639.94, 'end': 2641.94, 'text': \" is kind of one of the main reasons why training doesn't work well,\", 'video_name': 'Kint3'}, {'id': 1967, 'seek': 500872, 'start': 5016.72, 'end': 5018.72, 'text': ' And one of the reasons for this', 'video_name': 'Kint3'}, {'id': 33, 'seek': 17308, 'start': 192.94, 'end': 197.82000000000002, 'text': ' using this Jupyter instance that I put on one of the THBing servers.', 'video_name': 'Kint'}, {'id': 53, 'seek': 23308, 'start': 261.08000000000004, 'end': 263.08000000000004, 'text': ' one of the main Python libraries we will be using for Python.', 'video_name': 'Kint'}, {'id': 190, 'seek': 85568, 'start': 872.16, 'end': 878.8599999999999, 'text': ' This was one of the first times...', 'video_name': 'Kint'}, {'id': 191, 'seek': 85568, 'start': 879.4799999999999, 'end': 882.5, 'text': ' That was one of the first things that needed to be solved.', 'video_name': 'Kint'}, {'id': 774, 'seek': 232032, 'start': 2346.32, 'end': 2349.32, 'text': ' and which is kind of one of the easiest ways to install', 'video_name': 'Kint'}, {'id': 941, 'seek': 267232, 'start': 2679.28, 'end': 2687.6400000000003, 'text': ' And one of the easiest ways to do that is do linear regression.', 'video_name': 'Kint'}, {'id': 956, 'seek': 276232, 'start': 2764.32, 'end': 2769.82, 'text': ' And this kind of linear model is kind of one of the earliest things', 'video_name': 'Kint'}, {'id': 983, 'seek': 285112, 'start': 2876.12, 'end': 2880.12, 'text': ' So because we kind of have fixed one of the small issues that we have with this model,', 'video_name': 'Kint'}, {'id': 1057, 'seek': 305612, 'start': 3079.12, 'end': 3083.12, 'text': \" But it's kind of one of the most used activation functions for neural networks.\", 'video_name': 'Kint'}, {'id': 1238, 'seek': 347012, 'start': 3491.12, 'end': 3493.12, 'text': ' Which is kind of one of the earliest...', 'video_name': 'Kint'}, {'id': 1678, 'seek': 439312, 'start': 4401.12, 'end': 4403.12, 'text': ' not just for one of the examples...', 'video_name': 'Kint'}, {'id': 1808, 'seek': 463312, 'start': 4661.12, 'end': 4663.12, 'text': ' for one of the color channels...', 'video_name': 'Kint'}, {'id': 1981, 'seek': 499312, 'start': 5015.12, 'end': 5017.12, 'text': ' so one of the things...', 'video_name': 'Kint'}, {'id': 0, 'seek': 0, 'start': 0.0, 'end': 10.72, 'text': ' Okay, last week we talked about machine learning systems and that one of the first things we', 'video_name': 'Kint2'}, {'id': 1230, 'seek': 300674, 'start': 3012.68, 'end': 3018.8199999999997, 'text': \" kind of one of the main operations for which we will need the derivative. So it's kind\", 'video_name': 'Kint2'}]\n",
      "search_phrase: one of the color\n",
      "found_objects:  [{'id': 1808, 'seek': 463312, 'start': 4661.12, 'end': 4663.12, 'text': ' for one of the color channels...', 'video_name': 'Kint'}]\n",
      "Found object\n"
     ]
    }
   ],
   "source": [
    "#WITHOUT BUFFER MEMORY\n",
    "from flask import Flask, request, render_template_string, send_file\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Chatbot</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            background-color: #f0f0f0;\n",
    "        }\n",
    "        .chat-container {\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            background-color: #ffffff;\n",
    "            border-radius: 10px 10px 0px 0px;\n",
    "            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        .chat-form{\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            background-color: #ffffff;\n",
    "            border-radius: 0px 0px 10px 10px;\n",
    "            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        .chat-message {\n",
    "            background-color: #f9f9f9;\n",
    "            border-radius: 10px;\n",
    "            padding: 10px;\n",
    "            margin: 10px 0;\n",
    "        }\n",
    "        .user-message {\n",
    "            text-align: right;\n",
    "        }\n",
    "        input[type=\"text\"] {\n",
    "            width: calc(100% - 100px); /* Adjust width of input */\n",
    "            padding: 8px;\n",
    "            border-radius: 5px;\n",
    "            border: 1px solid #ccc;\n",
    "            margin-right: 10px;\n",
    "        }\n",
    "        .additional-content {\n",
    "            display: none;\n",
    "        }\n",
    "        .show-button{\n",
    "            background-color: #555555;\n",
    "            border: none;\n",
    "            color: white;\n",
    "            padding: 8px 10px;\n",
    "            text-align: center;\n",
    "            text-decoration: none;\n",
    "            display: inline-block;\n",
    "            font-size: 16px;\n",
    "            margin: 4px 2px;\n",
    "            cursor: pointer;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        #input_submit{\n",
    "            background-color: #555555;\n",
    "            border: none;\n",
    "            color: white;\n",
    "            padding: 8px 10px;\n",
    "            text-align: center;\n",
    "            text-decoration: none;\n",
    "            display: inline-block;\n",
    "            font-size: 16px;\n",
    "            margin: 4px 2px;\n",
    "            cursor: pointer;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"chat-container\" id=\"chat-container\">\n",
    "    <h3>Welcome to the Artificial Intelligence QnA service.  You may ask me anything </h3>\n",
    "        {% for message in chat_history %}\n",
    "            <div class=\"chat-message {% if message['sender'] == 'user' %}user-message{% endif %}\">\n",
    "                {{ message['content'] }}\n",
    "                {% if message['sender'] == 'bot' %}\n",
    "                    <br><br>\n",
    "                    <button class=\"show-button\">Show source</button>\n",
    "                    <div class=\"additional-content\">\n",
    "                        {{ message['source'] }}\n",
    "                        <br><br>\n",
    "                        {{ message['seekTime'] }}\n",
    "                        <video id=\"myVideo\" width=\"780\" height=\"640\" controls>\n",
    "                            <source src=\"http://127.0.0.1:81/Kint2\" type=\"video/mp4\">\n",
    "                            Your browser does not support the video tag.\n",
    "                        </video>\n",
    "                    </div>\n",
    "                    <br><br>\n",
    "                {% endif %}\n",
    "            </div>\n",
    "        {% endfor %}\n",
    "    </div>\n",
    "    <div class=\"chat-form\">\n",
    "    <form action=\"/\" method=\"POST\" id=\"chat-form\">\n",
    "        <input type=\"text\" name=\"input_text\" id=\"input_text\" placeholder=\"Message Chatbot...\">\n",
    "        <input type=\"submit\" id=\"input_submit\" value=\"Send\">\n",
    "    </form>\n",
    "    </div>\n",
    "    <script>\n",
    "        document.getElementById('chat-form').addEventListener('submit', function(event) {\n",
    "            event.preventDefault(); // Prevent default form submission\n",
    "            var inputText = document.getElementById('input_text').value;\n",
    "            if (inputText.trim() !== '') {\n",
    "                var inputBox = document.getElementById('input_text');\n",
    "                var chatContainer = document.getElementById('chat-container');\n",
    "                var userMessage = document.createElement('div');\n",
    "                userMessage.className = 'chat-message user-message';\n",
    "                userMessage.textContent = inputText;\n",
    "                chatContainer.appendChild(userMessage);\n",
    "                document.getElementById('chat-form').submit(); // Submit form\n",
    "            }\n",
    "        });\n",
    "\n",
    "        //show/hide the source button\n",
    "        document.querySelectorAll('.show-button').forEach(button => {\n",
    "            button.addEventListener('click', function() {\n",
    "                const additionalContent = this.nextElementSibling;\n",
    "                additionalContent.style.display = additionalContent.style.display === 'block' ? 'none' : 'block';\n",
    "                this.textContent = additionalContent.style.display === 'block' ? 'Hide Source' : 'Show source';\n",
    "            });\n",
    "        });\n",
    "\n",
    "        //to seek the video\n",
    "        document.addEventListener('DOMContentLoaded', function () {\n",
    "        var video = document.getElementById('myVideo');\n",
    "        var chatHistory = {{ chat_history|tojson }};\n",
    "        console.log('chatHistory', chatHistory)\n",
    "        startTime = chatHistory[chatHistory.length - 1]['seekTime']['start'];\n",
    "        console.log('startTime', startTime)\n",
    "\n",
    "        // When the video metadata has loaded, set the start time\n",
    "        video.addEventListener('loadedmetadata', function () {\n",
    "            video.currentTime = startTime;\n",
    "        });\n",
    "    });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "@app.route('/Kint2')\n",
    "def video():\n",
    "    video_path = 'Kint2.mp4'\n",
    "    return send_file(video_path, mimetype='video/mp4')\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    global chat_history\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        input_text = request.form['input_text']\n",
    "        inputAfterSimilaritySearch = db2.similarity_search(query=input_text, k=2)\n",
    "        output_text = qa_chain({\"input_documents\": inputAfterSimilaritySearch[0].page_content, \"query\": input_text})\n",
    "        # output_text = qa_chain(input_text)\n",
    "        chat_history.append({'sender': 'user', 'content': input_text})\n",
    "\n",
    "        # Add logic here to generate response based on input_text\n",
    "        seekTime = getStartTimeFromSegments(inputAfterSimilaritySearch[0].page_content)\n",
    "        response_text = output_text['result']\n",
    "        chat_history.append({'sender': 'bot', 'content': response_text, 'source': inputAfterSimilaritySearch[0].page_content, 'seekTime': seekTime})\n",
    "\n",
    "    return render_template_string(html_template, chat_history=chat_history)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, request, jsonify\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# response_text = ''\n",
    "\n",
    "# @app.route('/', methods=['GET', 'POST'])\n",
    "# def index():\n",
    "#     global response_text\n",
    "\n",
    "#     if request.method == 'POST':\n",
    "#         input_text = request.form['input_text']\n",
    "#         inputAfterSimilaritySearch = db2.similarity_search(query=input_text, k=2)\n",
    "#         output_text = qa_chain({\"input_documents\": inputAfterSimilaritySearch[0].page_content, \"query\": input_text})\n",
    "#         seekTime = getStartTimeFromSegments(inputAfterSimilaritySearch[0].page_content)\n",
    "#         response_data = {\n",
    "#             'result': output_text['result'],\n",
    "#             'source': inputAfterSimilaritySearch[0].page_content,\n",
    "#             'seekTime': seekTime\n",
    "#         }\n",
    "#         return jsonify(response_data)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(host=\"0.0.0.0\", port=81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
